# Backend Automation with Guardrailed LLMs

This repository contains an open-source backend system leveraging **Large Language Models (LLMs)** within a **Model Context Protocol (MCP)** framework to transform user intents, expressed in natural language, into **validated, reproducible workflows** (â€œrecipesâ€).

Originally designed for specialized domains, the system now targets **broad backend automation** â€”  like **DevOps**, **Object storage**, **ETL pipelines** etc.

> **Our Goal: Make AI Agents Predictable for Backend Tasks ğŸ¤–**
> LLMs are powerful but often produce hallucinations, unsafe outputs, or actions that are out of scope. This project solves that problem by integrating 
> strict guardrails, precise semantic classification, and vetted workflows to ensure every output is correct, safe, and repeatable.

---
## The Problem ğŸ˜Ÿ
LLMs are like eager assistants who will confidently lead you down the wrong path. Their tendency to be inaccurate or unreliable is a major roadblock for complex tasks , especially when working with specialized tech stacks. Without a system for preconditions and validation, their outputs are annoying at best and dangerous at worst. This lack of guardrails often leads to incorrect execution, false information, or out-of-scope actions

---
## The Solution âœ…
LLMs become reliable when integrated within a structured system of guardrails. Our solution is a validation layer built into an MCP server that uses predefined and vetted "recipes".

Hereâ€™s how it works:

1. The system **Parses** a user's request with a non-LLM Intent Classifier.
2. **Selects** a vetted â€œrecipeâ€ that matches the user's intent.
3. **Provides** the LLM with the exact instructions, resources, and constraints from that recipe.
4. Finally, **Validates** the LLM's output against the recipe's guardrails before execution.

This reduces the risk of incorrect, incomplete, or unsafe results â€” a key requirement for automation in production environments.
We continuously evaluate and improve our library of high-quality recipes through both automated processes and human refinement.

---

## âš™ï¸ Architecture

### MCP Server
Implements the [Model Context Protocol](https://modelcontextprotocol.io/) specification.  
Exposes tools that can be called by compatible clients (e.g., Cursor, Claude Code).

### Library of Recipes
A curated JSON-schema library of tested workflows.  
Each recipe contains:
- **Intent description**
- **Required tools**
- **Step-by-step workflow**
- **Optional context** (docs, boilerplates, data samples)

### Intent Classifier
Matches user input to the correct recipe without calling an LLM.  
Ensures the right plan is selected before LLM involvement.

### Guardrails & Fallback Rules
Validates LLM outputs against the recipe specification.  
Rejects unsafe, incomplete, or out-of-scope results.

---

## ğŸ’¾ Usage

To run sample generation:

```bash
python create.py
Generated recipes and outputs are stored in /generated.

âš ï¸ Known Limitations
Domain Coverage â€“ Accuracy depends on available recipes. Users can add new resipes.

LLM Hallucinations â€“ Mitigated by guardrails, but incomplete recipes can still allow errors.

Edge Cases â€“ May require additional validation layers.

ğŸš€ Possible Improvements
Expand recipe coverage for more backend domains.

Automated CI testing of generated outputs.

Metrics & monitoring of recipe performance and error rates.

Community recipe submissions via schema-based PRs.

ğŸ“œ License
MIT License.
See LICENSE for details.
