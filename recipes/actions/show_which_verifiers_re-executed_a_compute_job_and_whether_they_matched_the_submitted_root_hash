{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Step 1 provides a backend Python helper that validates a user-supplied job_id and normalizes it into the canonical format expected by Republic backend services, optionally resolving aliases via a Republic API endpoint.",
            "code": "import re\nimport uuid\nfrom typing import Dict, Any, Optional\n\nimport httpx\n\n# Base URL for Republic's public API / indexer.\n# In a real deployment this would come from configuration.\nREPUBLIC_API_BASE_URL = 'https://api.republic.example.com'\n\n\nclass JobIdValidationError(Exception):\n    \"\"\"Raised when a job_id is missing, malformed, or cannot be resolved.\"\"\"\n\n\nasync def validate_and_normalize_job_id(job_id: str) -> Dict[str, Any]:\n    \"\"\"\n    Validate and normalize a Republic job identifier.\n\n    Normalization strategy (example, adjust to your real rules):\n      - Accept UUID job_ids and normalize them to lowercase canonical UUID form.\n      - Accept 32- or 64-byte hex hashes (optionally prefixed with '0x') and\n        normalize them to lowercase without the '0x' prefix.\n      - Otherwise, treat job_id as an alias and ask the Republic API to resolve\n        it to a canonical job_id.\n\n    Returns a dict with both the original and canonical job ids.\n    \"\"\"\n    if not isinstance(job_id, str):\n        raise JobIdValidationError('job_id must be a string')\n\n    cleaned = job_id.strip()\n    if not cleaned:\n        raise JobIdValidationError('job_id is empty after trimming whitespace')\n\n    canonical_id: Optional[str] = None\n\n    # Case 1: UUID-like job_id\n    try:\n        parsed_uuid = uuid.UUID(cleaned)\n        canonical_id = str(parsed_uuid)  # standard lowercase UUID string\n    except ValueError:\n        # Not a UUID, try hex hash patterns\n        pass\n\n    # Case 2: hex-encoded hash (e.g., 64 hex chars, with optional 0x prefix)\n    if canonical_id is None:\n        hex_match = re.fullmatch(r'0x?[0-9a-fA-F]{64}', cleaned)\n        if hex_match:\n            canonical_id = cleaned.lower()\n            if canonical_id.startswith('0x'):\n                canonical_id = canonical_id[2:]\n\n    # Case 3: fall back to alias resolution via Republic backend\n    if canonical_id is None:\n        try:\n            async with httpx.AsyncClient(timeout=5.0) as client:\n                resp = await client.get(\n                    f'{REPUBLIC_API_BASE_URL}/jobs/resolve',\n                    params={'job_id': cleaned},\n                )\n                resp.raise_for_status()\n        except httpx.HTTPError as exc:\n            raise JobIdValidationError(\n                f\"Failed to resolve job_id '{cleaned}' via Republic API: {exc}\"\n            ) from exc\n\n        data = resp.json()\n        canonical_id = data.get('canonical_job_id')\n\n    if not canonical_id:\n        raise JobIdValidationError(\n            f\"Unable to normalize job_id '{cleaned}'. It is neither a valid UUID, \"\n            'a supported hash format, nor a resolvable alias.'\n        )\n\n    return {\n        'input_job_id': cleaned,\n        'canonical_job_id': canonical_id,\n    }\n",
            "usage": "Inside your BFF (e.g., a FastAPI route), call this helper with the user-supplied job_id and work with the canonical form:\n\nasync def get_normalized_job_id_handler(job_id: str):\n    result = await validate_and_normalize_job_id(job_id)\n    canonical_job_id = result['canonical_job_id']\n    # Use canonical_job_id in subsequent Republic API calls\n    return result"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Step 2 defines a backend Python helper that queries the Republic compute-proof registry to retrieve the canonical PoME root_hash for a given job_id.",
            "code": "from typing import Dict, Any\n\nimport httpx\n\n# Reuse the same base URL constant as in step 1, or import it from a shared config.\nREPUBLIC_API_BASE_URL = 'https://api.republic.example.com'\n\n\nclass PoMENotFoundError(Exception):\n    \"\"\"Raised when no canonical PoME proof exists for the provided job_id.\"\"\"\n\n\nasync def fetch_canonical_pome_root_hash(job_id: str) -> Dict[str, Any]:\n    \"\"\"\n    Fetch the canonical PoME root_hash for a job from the compute-proof registry.\n\n    Expected (example) response shape from Republic backend:\n      GET /compute-proof/jobs/{job_id}\n      {\n        'job_id': '...',\n        'canonical_root_hash': '0xabc123...',\n        'status': 'finalized',\n        'proof_height': 123456,\n        ...\n      }\n\n    Returns a dict containing job_id, canonical_root_hash, and the raw response.\n    \"\"\"\n    if not job_id:\n        raise ValueError('job_id is required to fetch PoME root hash')\n\n    url = f'{REPUBLIC_API_BASE_URL}/compute-proof/jobs/{job_id}'\n\n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            resp = await client.get(url)\n    except httpx.HTTPError as exc:\n        # Network-level or protocol-level issue\n        raise RuntimeError(f'Failed to call compute-proof registry: {exc}') from exc\n\n    if resp.status_code == 404:\n        # No proof known for this job\n        raise PoMENotFoundError(f'No canonical PoME proof found for job_id={job_id}')\n\n    try:\n        resp.raise_for_status()\n    except httpx.HTTPError as exc:\n        raise RuntimeError(\n            f'Compute-proof registry returned error status {resp.status_code}: {exc}'\n        ) from exc\n\n    data: Dict[str, Any] = resp.json()\n    root_hash = data.get('canonical_root_hash')\n\n    if not root_hash:\n        raise PoMENotFoundError(\n            f\"Response from compute-proof registry missing 'canonical_root_hash' for job_id={job_id}\"\n        )\n\n    return {\n        'job_id': job_id,\n        'canonical_root_hash': root_hash,\n        'raw_response': data,\n    }\n",
            "usage": "After obtaining a canonical_job_id (e.g., from step 1), call this helper to get the reference PoME root hash:\n\ncanonical_job_id = normalized_result['canonical_job_id']\npome_info = await fetch_canonical_pome_root_hash(canonical_job_id)\ncanonical_root_hash = pome_info['canonical_root_hash']"
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Step 3 implements a backend Python function that queries the Republic verification indexer for all verifier execution records associated with a given job_id.",
            "code": "from typing import Dict, Any, List\n\nimport httpx\n\n# Base URL for the verification/indexer module. This might be the same as\n# REPUBLIC_API_BASE_URL or a dedicated subdomain in a real deployment.\nVERIFICATION_INDEXER_BASE_URL = 'https://api.republic.example.com'\n\n\nclass VerificationQueryError(Exception):\n    \"\"\"Raised when verification records cannot be fetched or parsed.\"\"\"\n\n\nasync def fetch_verifier_execution_records(job_id: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Query the verification module or indexer for all verification_result records\n    for a given job_id.\n\n    Example expected response from Republic indexer:\n      GET /verifications?job_id={job_id}\n      {\n        'job_id': '...',\n        'verifications': [\n          {\n            'verifier_address': 'repval1...',\n            'local_root_hash': '0xabc123...',\n            'reported_match_flag': true,\n            'block_height': 123456,\n            'timestamp': '2025-01-01T12:00:00Z'\n          },\n          ...\n        ]\n      }\n\n    Returns a list of dicts, one per verification record.\n    \"\"\"\n    if not job_id:\n        raise ValueError('job_id is required to fetch verifier execution records')\n\n    url = f'{VERIFICATION_INDEXER_BASE_URL}/verifications'\n\n    try:\n        async with httpx.AsyncClient(timeout=10.0) as client:\n            resp = await client.get(url, params={'job_id': job_id})\n    except httpx.HTTPError as exc:\n        raise VerificationQueryError(f'Failed to query verification indexer: {exc}') from exc\n\n    try:\n        resp.raise_for_status()\n    except httpx.HTTPError as exc:\n        raise VerificationQueryError(\n            f'Verification indexer returned error status {resp.status_code}: {exc}'\n        ) from exc\n\n    payload: Dict[str, Any] = resp.json()\n\n    # Extract and validate records\n    raw_records = payload.get('verifications', [])\n    if not isinstance(raw_records, list):\n        raise VerificationQueryError(\n            \"Indexer response field 'verifications' is not a list as expected\"\n        )\n\n    normalized_records: List[Dict[str, Any]] = []\n    for item in raw_records:\n        # Be defensive: tolerate partially missing fields but keep shape consistent.\n        normalized_records.append(\n            {\n                'job_id': job_id,\n                'verifier_address': item.get('verifier_address', ''),\n                'local_root_hash': item.get('local_root_hash', ''),\n                'reported_match_flag': bool(item.get('reported_match_flag', False)),\n                'block_height': int(item.get('block_height', 0) or 0),\n                'timestamp': item.get('timestamp', ''),\n            }\n        )\n\n    return normalized_records\n",
            "usage": "Once you know the target job_id, call this helper in your backend to obtain all verifier execution records:\n\nrecords = await fetch_verifier_execution_records(canonical_job_id)\n# records is a list of dicts with verifier_address, local_root_hash, reported_match_flag, block_height, and timestamp."
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Step 4 adds a backend Python helper that enriches each verifier record with optional validator metadata (moniker, reputation_score, total_stake) fetched from the Republic validator registry.",
            "code": "from typing import Dict, Any, List\nimport asyncio\n\nimport httpx\n\nVALIDATOR_REGISTRY_BASE_URL = 'https://api.republic.example.com'\n\n\nclass ValidatorMetadataError(Exception):\n    \"\"\"Raised for unrecoverable errors while fetching validator metadata.\"\"\"\n\n\nasync def _fetch_single_validator_metadata(client: httpx.AsyncClient, address: str) -> Dict[str, Any]:\n    \"\"\"Fetch metadata for a single validator address.\n\n    If the validator is unknown (404), returns an empty dict instead of raising.\n    \"\"\"\n    url = f'{VALIDATOR_REGISTRY_BASE_URL}/validators/{address}'\n    try:\n        resp = await client.get(url)\n    except httpx.HTTPError as exc:\n        # Network error; record it in the metadata so the caller can inspect.\n        return {'metadata_error': f'network error: {exc}'}\n\n    if resp.status_code == 404:\n        # Unknown validator; not necessarily an error.\n        return {}\n\n    try:\n        resp.raise_for_status()\n    except httpx.HTTPError as exc:\n        return {'metadata_error': f'http error {resp.status_code}: {exc}'}\n\n    data = resp.json()\n    # Normalize selected fields; extend this as your registry evolves.\n    return {\n        'moniker': data.get('moniker'),\n        'reputation_score': data.get('reputation_score'),\n        'total_stake': data.get('total_stake'),\n    }\n\n\nasync def enrich_verifier_metadata(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    For each verifier_address in the provided verification records, fetch optional\n    validator metadata and attach it under 'validator_metadata'.\n\n    The function internally de-duplicates addresses and performs the lookups\n    concurrently for efficiency.\n    \"\"\"\n    if not records:\n        return []\n\n    unique_addresses = sorted({r.get('verifier_address', '') for r in records if r.get('verifier_address')})\n\n    # Fetch metadata concurrently for all unique addresses\n    async with httpx.AsyncClient(timeout=5.0) as client:\n        tasks = [\n            _fetch_single_validator_metadata(client, addr)\n            for addr in unique_addresses\n        ]\n        metadata_list = await asyncio.gather(*tasks)\n\n    # Build a simple cache address -> metadata\n    metadata_by_address: Dict[str, Dict[str, Any]] = {\n        addr: meta for addr, meta in zip(unique_addresses, metadata_list)\n    }\n\n    # Attach metadata to each record\n    enriched: List[Dict[str, Any]] = []\n    for record in records:\n        addr = record.get('verifier_address', '')\n        enriched_record = dict(record)\n        enriched_record['validator_metadata'] = metadata_by_address.get(addr, {})\n        enriched.append(enriched_record)\n\n    return enriched\n",
            "usage": "After you have raw verification records (from step 3), call this helper to attach validator metadata:\n\nenriched_records = await enrich_verifier_metadata(records)\n# Each element in enriched_records now includes a 'validator_metadata' field\n# with optional keys: moniker, reputation_score, total_stake, or metadata_error."
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Step 5 defines a pure backend Python function that cross-checks each verifier record against the canonical PoME root_hash, recomputes a normalized matched flag, and labels each record as consistent or inconsistent relative to the stored reported_match_flag.",
            "code": "from typing import Dict, Any, List\n\n\ndef cross_check_verifier_match_flags(\n    records: List[Dict[str, Any]],\n    canonical_root_hash: str,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    For each verification record, recompute whether local_root_hash equals the\n    canonical_root_hash and compare that to the stored reported_match_flag.\n\n    Adds two fields to each record:\n      - 'matched': a normalized boolean derived from canonical comparison.\n      - 'consistency_status': 'consistent' if reported_match_flag agrees with\n        the recomputed result, otherwise 'inconsistent'.\n\n    Returns a new list of records without mutating the input list.\n    \"\"\"\n    if not canonical_root_hash:\n        raise ValueError('canonical_root_hash is required for cross-checking')\n\n    normalized_canonical = canonical_root_hash.lower().strip()\n\n    output: List[Dict[str, Any]] = []\n\n    for record in records:\n        rec = dict(record)  # shallow copy to avoid mutating original\n\n        local_hash_raw = rec.get('local_root_hash', '') or ''\n        local_hash = local_hash_raw.lower().strip()\n\n        calculated_match = bool(local_hash and local_hash == normalized_canonical)\n        reported_match = bool(rec.get('reported_match_flag', False))\n\n        is_consistent = (calculated_match == reported_match)\n\n        rec['matched'] = calculated_match\n        rec['consistency_status'] = 'consistent' if is_consistent else 'inconsistent'\n\n        output.append(rec)\n\n    return output\n",
            "usage": "Once you know the canonical_root_hash (step 2) and have enriched verification records (steps 3\u20134), cross-check them like this:\n\nchecked_records = cross_check_verifier_match_flags(enriched_records, canonical_root_hash)\n# Each record in checked_records now includes 'matched' and 'consistency_status' fields."
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "Step 6 provides a backend Python helper that assembles a complete verifier match summary, sorting verifications by timestamp and computing aggregate counts of matches, mismatches, and inconsistencies.",
            "code": "from typing import Dict, Any, List\nfrom datetime import datetime\n\n\ndef _parse_timestamp(ts: str) -> datetime:\n    \"\"\"Best-effort ISO8601 timestamp parser with a safe fallback.\"\"\"\n    if not ts:\n        # Use epoch as a default to keep sorting stable\n        return datetime.fromtimestamp(0)\n    try:\n        # Handle common ISO8601 formats. Extend as needed.\n        # Python 3.11+ has fromisoformat that handles many variants directly.\n        return datetime.fromisoformat(ts.replace('Z', '+00:00'))\n    except ValueError:\n        return datetime.fromtimestamp(0)\n\n\ndef construct_verifier_match_summary(\n    job_id: str,\n    canonical_root_hash: str,\n    records: List[Dict[str, Any]],\n) -> Dict[str, Any]:\n    \"\"\"\n    Build a structured response summarizing verifier PoME checks for a job.\n\n    The output includes:\n      - job_id\n      - canonical_root_hash\n      - summary: aggregate counts of matches, mismatches, inconsistencies\n      - verifications: list of verifier entries sorted by timestamp, where\n        each entry contains at least:\n          * verifier_address\n          * optional validator_metadata\n          * local_root_hash\n          * matched (bool, from canonical comparison)\n          * consistency_status ('consistent' or 'inconsistent')\n          * timestamp\n    \"\"\"\n    # Sort records chronologically by timestamp (oldest first)\n    sorted_records = sorted(records, key=lambda r: _parse_timestamp(r.get('timestamp', '')))\n\n    total = len(sorted_records)\n    matches = sum(1 for r in sorted_records if r.get('matched') is True)\n    mismatches = sum(1 for r in sorted_records if r.get('matched') is False)\n    inconsistencies = sum(\n        1 for r in sorted_records if r.get('consistency_status') == 'inconsistent'\n    )\n\n    # Normalize the shape of each verification entry in the final response.\n    formatted_verifications: List[Dict[str, Any]] = []\n    for r in sorted_records:\n        formatted_verifications.append(\n            {\n                'verifier_address': r.get('verifier_address'),\n                'validator_metadata': r.get('validator_metadata') or {},\n                'local_root_hash': r.get('local_root_hash'),\n                'matched': bool(r.get('matched', False)),\n                'consistency_status': r.get('consistency_status', 'unknown'),\n                'block_height': r.get('block_height'),\n                'timestamp': r.get('timestamp'),\n            }\n        )\n\n    return {\n        'job_id': job_id,\n        'canonical_root_hash': canonical_root_hash,\n        'summary': {\n            'total_verifications': total,\n            'matches': matches,\n            'mismatches': mismatches,\n            'inconsistencies': inconsistencies,\n        },\n        'verifications': formatted_verifications,\n    }\n",
            "usage": "After cross-checking records in step 5, call this helper to generate the final payload your API can return to clients:\n\nsummary = construct_verifier_match_summary(\n    job_id=canonical_job_id,\n    canonical_root_hash=canonical_root_hash,\n    records=checked_records,\n)\n\n# In a web framework (e.g., FastAPI), you would return `summary` as JSON from\n# an endpoint like GET /jobs/{job_id}/verifier-pome-summary."
        }
    ]
}