{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Step 1 defines a backend utility to parse and normalize user-provided filters (validator, client, time range, job type, pagination) for listing Republic compute jobs, with a sensible default lookback window.",
            "code": "from dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Mapping, Optional\n\nDEFAULT_LOOKBACK_DAYS = 7\nDEFAULT_PAGE_LIMIT = 50\nMAX_PAGE_LIMIT = 200\n\n\n@dataclass\nclass JobListFilters:\n    \"\"\"Normalized filters for listing Republic compute jobs.\"\"\"\n    validator: Optional[str]\n    client: Optional[str]\n    job_type: Optional[str]\n    start_time: datetime\n    end_time: datetime\n    limit: int\n    offset: int\n\n\ndef _parse_iso8601(ts: str) -> datetime:\n    \"\"\"Parse an ISO8601 timestamp string into an aware UTC datetime.\n\n    Raises ValueError on invalid input.\n    \"\"\"\n    # Handle common \"Z\" UTC suffix\n    if ts.endswith(\"Z\"):\n        ts = ts[:-1] + \"+00:00\"\n    dt = datetime.fromisoformat(ts)\n    if dt.tzinfo is None:\n        # Assume UTC if no timezone info\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\n\ndef parse_list_jobs_filters(raw_filters: Mapping[str, str]) -> JobListFilters:\n    \"\"\"Parse and normalize user-provided filters for listing compute jobs.\n\n    raw_filters is typically request.query_params or a plain dict of strings.\n    \"\"\"\n    now = datetime.now(timezone.utc)\n\n    # Time range with sensible defaults\n    raw_start = raw_filters.get(\"start_time\")\n    raw_end = raw_filters.get(\"end_time\")\n\n    try:\n        start_time = _parse_iso8601(raw_start) if raw_start else now - timedelta(days=DEFAULT_LOOKBACK_DAYS)\n    except ValueError as exc:\n        raise ValueError(f\"Invalid start_time value: {raw_start!r}\") from exc\n\n    try:\n        end_time = _parse_iso8601(raw_end) if raw_end else now\n    except ValueError as exc:\n        raise ValueError(f\"Invalid end_time value: {raw_end!r}\") from exc\n\n    if start_time > end_time:\n        raise ValueError(\"start_time must be <= end_time\")\n\n    # Pagination\n    try:\n        limit = int(raw_filters.get(\"limit\", DEFAULT_PAGE_LIMIT))\n    except (TypeError, ValueError):\n        raise ValueError(\"limit must be an integer\")\n\n    if limit <= 0:\n        limit = DEFAULT_PAGE_LIMIT\n    if limit > MAX_PAGE_LIMIT:\n        limit = MAX_PAGE_LIMIT\n\n    try:\n        offset = int(raw_filters.get(\"offset\", 0))\n    except (TypeError, ValueError):\n        raise ValueError(\"offset must be an integer\")\n\n    if offset < 0:\n        offset = 0\n\n    return JobListFilters(\n        validator=raw_filters.get(\"validator\"),\n        client=raw_filters.get(\"client\"),\n        job_type=raw_filters.get(\"job_type\"),\n        start_time=start_time,\n        end_time=end_time,\n        limit=limit,\n        offset=offset,\n    )\n",
            "usage": "Inside your backend route (e.g., FastAPI handler for GET /api/job-violations), call:\n\nfrom typing import Dict\n\n# raw_filters is typically dict(request.query_params)\nfilters = parse_list_jobs_filters(dict(request.query_params))"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Step 2 implements a backend async function that queries the Republic job registry/indexer for jobs that violated time constraints, using the normalized filters from Step 1.",
            "code": "import os\nfrom typing import Any, Dict, List\n\nimport httpx\n\nfrom .job_violations import JobListFilters  # adjust import to your module structure\n\nREPUBLIC_INDEXER_URL = os.getenv(\"REPUBLIC_INDEXER_URL\", \"https://indexer.republic.example.com\")\n\n\nasync def fetch_time_violated_jobs(filters: JobListFilters) -> List[Dict[str, Any]]:\n    \"\"\"Query the Republic job registry / indexer for jobs that violated time constraints.\n\n    Returns a list of raw job dictionaries as returned by the indexer.\n    \"\"\"\n    params: Dict[str, Any] = {\n        \"start_time\": filters.start_time.isoformat(),\n        \"end_time\": filters.end_time.isoformat(),\n        \"limit\": filters.limit,\n        \"offset\": filters.offset,\n        # Convention: tells the indexer to filter for timed-out / late-completion jobs.\n        \"violation_type\": \"time\",\n    }\n\n    if filters.validator:\n        params[\"validator\"] = filters.validator\n    if filters.client:\n        params[\"client\"] = filters.client\n    if filters.job_type:\n        params[\"job_type\"] = filters.job_type\n\n    try:\n        async with httpx.AsyncClient(base_url=REPUBLIC_INDEXER_URL, timeout=10.0) as client:\n            response = await client.get(\"/jobs\", params=params)\n            response.raise_for_status()\n    except httpx.HTTPError as exc:\n        # Wrap low-level network errors in a more descriptive exception\n        raise RuntimeError(f\"Failed to fetch time-violated jobs from indexer: {exc}\") from exc\n\n    data = response.json()\n    # The exact response schema will depend on the Republic indexer implementation.\n    # We assume a standard paginated format: {\"jobs\": [...], \"pagination\": {...}}\n    jobs = data.get(\"jobs\", [])\n    if not isinstance(jobs, list):\n        raise RuntimeError(\"Unexpected indexer response format: 'jobs' is not a list\")\n\n    return jobs\n",
            "usage": "After normalizing filters in your backend, call:\n\njobs = await fetch_time_violated_jobs(filters)\n\nThis would typically happen inside an async web handler that orchestrates the full violation query pipeline."
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Step 3 adds a backend async function to query the slashing/penalty indexer for compute-related slashing events corresponding to the same time window and filters.",
            "code": "import os\nfrom typing import Any, Dict, List\n\nimport httpx\n\nfrom .job_violations import JobListFilters  # adjust import to your module structure\n\nREPUBLIC_INDEXER_URL = os.getenv(\"REPUBLIC_INDEXER_URL\", \"https://indexer.republic.example.com\")\n\n\nasync def fetch_compute_slashing_events(filters: JobListFilters) -> List[Dict[str, Any]]:\n    \"\"\"Query the Republic slashing / penalty indexer for compute-related slashing events.\n\n    Returns a list of slashing event dicts including at least:\n      - job_id\n      - validator_id\n      - reason_code\n      - slash_time (ISO8601 timestamp)\n    \"\"\"\n    params: Dict[str, Any] = {\n        \"start_time\": filters.start_time.isoformat(),\n        \"end_time\": filters.end_time.isoformat(),\n        # Convention: restrict events to compute misbehavior (incorrect results, missed deadlines, etc.).\n        \"reason_domain\": \"compute\",\n        \"limit\": filters.limit,\n        \"offset\": filters.offset,\n    }\n\n    if filters.validator:\n        params[\"validator\"] = filters.validator\n\n    try:\n        async with httpx.AsyncClient(base_url=REPUBLIC_INDEXER_URL, timeout=10.0) as client:\n            response = await client.get(\"/slashing_events\", params=params)\n            response.raise_for_status()\n    except httpx.HTTPError as exc:\n        raise RuntimeError(f\"Failed to fetch compute slashing events from indexer: {exc}\") from exc\n\n    data = response.json()\n    events = data.get(\"events\", [])\n    if not isinstance(events, list):\n        raise RuntimeError(\"Unexpected indexer response format: 'events' is not a list\")\n\n    return events\n",
            "usage": "In the same backend handler that fetched time-violated jobs, call:\n\nslashing_events = await fetch_compute_slashing_events(filters)\n\nYou can also run Steps 2 and 3 in parallel using asyncio.gather for better performance."
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Step 4 defines a backend function to join time-violated jobs with compute slashing events by job_id and validator_id, producing unified violation records annotated with flags and combined reasons.",
            "code": "from datetime import datetime, timezone\nfrom typing import Any, Dict, List, Tuple\n\n\ndef _parse_violation_timestamp(job: Dict[str, Any] | None, event: Dict[str, Any] | None) -> str:\n    \"\"\"Compute the canonical violation timestamp for a job/event pair.\n\n    For jobs we consider fields like deadline_time and completion_time.\n    For slashing events we consider slash_time.\n    Returns an ISO8601 UTC string.\n    \"\"\"\n    candidates: List[datetime] = []\n\n    def _maybe_add(ts: Any) -> None:\n        if not ts:\n            return\n        if isinstance(ts, datetime):\n            dt = ts\n        elif isinstance(ts, str):\n            try:\n                v = ts[:-1] + \"+00:00\" if ts.endswith(\"Z\") else ts\n                dt = datetime.fromisoformat(v)\n            except ValueError:\n                return\n        else:\n            return\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        candidates.append(dt.astimezone(timezone.utc))\n\n    if job is not None:\n        _maybe_add(job.get(\"deadline_time\"))\n        _maybe_add(job.get(\"completion_time\"))\n    if event is not None:\n        _maybe_add(event.get(\"slash_time\"))\n\n    if not candidates:\n        # Fallback: now, though in a real system you may want to treat this as an error.\n        return datetime.now(timezone.utc).isoformat()\n\n    return max(candidates).isoformat()\n\n\ndef join_jobs_with_slashing_events(\n    time_violated_jobs: List[Dict[str, Any]],\n    slashing_events: List[Dict[str, Any]],\n) -> List[Dict[str, Any]]:\n    \"\"\"Join time-violated jobs with compute slashing events by (job_id, validator_id).\n\n    Produces a unified list of violation records with flags:\n      - time_violation: bool\n      - slashed: bool\n      - violation_reason: str\n      - violation_timestamp: ISO8601 timestamp\n\n    Each record includes:\n      - job_id\n      - validator_id\n      - job (raw job payload, if available)\n      - slashing_events (list of related events)\n    \"\"\"\n    # Index jobs by (job_id, validator_id)\n    joined: Dict[Tuple[str, str], Dict[str, Any]] = {}\n\n    for job in time_violated_jobs:\n        job_id = str(job.get(\"id\") or job.get(\"job_id\") or \"\")\n        validator_id = str(job.get(\"validator_id\") or job.get(\"validator\") or \"\")\n        if not job_id or not validator_id:\n            # Skip malformed records rather than crashing\n            continue\n\n        key = (job_id, validator_id)\n        violation_timestamp = _parse_violation_timestamp(job=job, event=None)\n\n        joined[key] = {\n            \"job_id\": job_id,\n            \"validator_id\": validator_id,\n            \"job\": job,\n            \"slashing_events\": [],\n            \"time_violation\": True,\n            \"slashed\": False,\n            \"violation_reason\": \"TIME_VIOLATION\",\n            \"violation_timestamp\": violation_timestamp,\n        }\n\n    # Associate slashing events with jobs (or create new records when necessary)\n    for event in slashing_events:\n        job_id = str(event.get(\"job_id\") or \"\")\n        validator_id = str(event.get(\"validator_id\") or event.get(\"validator\") or \"\")\n        if not job_id or not validator_id:\n            continue\n\n        key = (job_id, validator_id)\n        violation_timestamp = _parse_violation_timestamp(\n            job=joined.get(key, {}).get(\"job\") if key in joined else None,\n            event=event,\n        )\n\n        record = joined.get(key)\n        if record is None:\n            # Job was not in the time-violated set, but was slashed for compute reasons.\n            record = {\n                \"job_id\": job_id,\n                \"validator_id\": validator_id,\n                \"job\": None,\n                \"slashing_events\": [],\n                \"time_violation\": False,\n                \"slashed\": True,\n                \"violation_reason\": \"\",\n                \"violation_timestamp\": violation_timestamp,\n            }\n            joined[key] = record\n\n        # Attach the slashing event\n        record[\"slashing_events\"].append(event)\n        record[\"slashed\"] = True\n\n        # Compute a combined reason string\n        reason_codes = sorted({str(ev.get(\"reason_code\") or \"UNKNOWN\") for ev in record[\"slashing_events\"]})\n        if record[\"time_violation\"] and record[\"slashed\"]:\n            prefix = \"TIME_VIOLATION_AND_SLASHED\"\n        elif record[\"slashed\"]:\n            prefix = \"SLASHED\"\n        else:\n            prefix = \"TIME_VIOLATION\"\n\n        record[\"violation_reason\"] = f\"{prefix}: {','.join(reason_codes)}\"\n        record[\"violation_timestamp\"] = violation_timestamp\n\n    # For records that only had time violations (no slashing), ensure violation_reason is set\n    for record in joined.values():\n        if record[\"time_violation\"] and not record[\"slashed\"]:\n            record.setdefault(\"violation_reason\", \"TIME_VIOLATION\")\n\n    return list(joined.values())\n",
            "usage": "Once you have both lists from Steps 2 and 3, call:\n\nunified_violations = join_jobs_with_slashing_events(time_violated_jobs, slashing_events)\n\nThe returned list can then be enriched (Step 5) and sorted/paginated (Step 6) before sending it back to the frontend."
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Step 5 provides backend helpers to optionally enrich each unified violation record with job and validator metadata (e.g., moniker, reputation, job type) from the Republic indexer, without changing which records are included.",
            "code": "import asyncio\nimport os\nfrom typing import Any, Dict, Iterable, List, Set\n\nimport httpx\n\nREPUBLIC_INDEXER_URL = os.getenv(\"REPUBLIC_INDEXER_URL\", \"https://indexer.republic.example.com\")\n\n\nasync def _fetch_validators_metadata(validator_ids: Set[str]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"Fetch basic metadata (moniker, reputation, etc.) for a set of validators.\n\n    Returns a mapping: validator_id -> metadata dict.\n    \"\"\"\n    if not validator_ids:\n        return {}\n\n    try:\n        async with httpx.AsyncClient(base_url=REPUBLIC_INDEXER_URL, timeout=10.0) as client:\n            # Example bulk endpoint; adapt to your actual indexer API.\n            response = await client.get(\"/validators\", params={\"ids\": \",\".join(sorted(validator_ids))})\n            response.raise_for_status()\n    except httpx.HTTPError as exc:\n        # On error, log and return empty metadata rather than failing the whole pipeline.\n        # In a production service, replace print with structured logging.\n        print(f\"Warning: failed to fetch validator metadata: {exc}\")\n        return {}\n\n    data = response.json()\n    items = data.get(\"validators\", [])\n    result: Dict[str, Dict[str, Any]] = {}\n    for v in items:\n        vid = str(v.get(\"id\") or v.get(\"validator_id\") or \"\")\n        if vid:\n            result[vid] = v\n    return result\n\n\nasync def _fetch_jobs_metadata(job_ids: Set[str]) -> Dict[str, Dict[str, Any]]:\n    \"\"\"Fetch additional job metadata (client, job_type, block heights, etc.) in bulk.\n\n    Returns a mapping: job_id -> metadata dict.\n    \"\"\"\n    if not job_ids:\n        return {}\n\n    try:\n        async with httpx.AsyncClient(base_url=REPUBLIC_INDEXER_URL, timeout=10.0) as client:\n            # Example bulk endpoint; adapt to your actual indexer API.\n            response = await client.get(\"/jobs/batch\", params={\"ids\": \",\".join(sorted(job_ids))})\n            response.raise_for_status()\n    except httpx.HTTPError as exc:\n        print(f\"Warning: failed to fetch job metadata: {exc}\")\n        return {}\n\n    data = response.json()\n    items = data.get(\"jobs\", [])\n    result: Dict[str, Dict[str, Any]] = {}\n    for j in items:\n        jid = str(j.get(\"id\") or j.get(\"job_id\") or \"\")\n        if jid:\n            result[jid] = j\n    return result\n\n\nasync def enrich_job_violation_records(\n    violation_records: Iterable[Dict[str, Any]],\n) -> List[Dict[str, Any]]:\n    \"\"\"Enrich unified job violation records with optional metadata.\n\n    This does NOT change which records are included; it only adds fields when available:\n      - validator_metadata: {moniker, reputation_score, ...}\n      - job_metadata: {client_id, job_type, submit_height, ...}\n    \"\"\"\n    records_list = list(violation_records)\n\n    validator_ids: Set[str] = {str(rec.get(\"validator_id\")) for rec in records_list if rec.get(\"validator_id\")}\n    job_ids: Set[str] = {str(rec.get(\"job_id\")) for rec in records_list if rec.get(\"job_id\")}\n\n    validators_metadata, jobs_metadata = await asyncio.gather(\n        _fetch_validators_metadata(validator_ids),\n        _fetch_jobs_metadata(job_ids),\n    )\n\n    for rec in records_list:\n        vid = str(rec.get(\"validator_id\") or \"\")\n        jid = str(rec.get(\"job_id\") or \"\")\n        if vid and vid in validators_metadata:\n            rec[\"validator_metadata\"] = validators_metadata[vid]\n        if jid and jid in jobs_metadata:\n            rec[\"job_metadata\"] = jobs_metadata[jid]\n\n    return records_list\n",
            "usage": "After joining jobs and slashing events in Step 4, call:\n\nenriched_violations = await enrich_job_violation_records(unified_violations)\n\nYou can safely skip this step if you only need raw IDs and flags; it only adds metadata fields."
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "Step 6 implements backend logic to sort unified (and optionally enriched) violation records by most recent violation timestamp and apply limit/offset pagination for the final API response.",
            "code": "from datetime import datetime\nfrom typing import Any, Dict, List\n\n\ndef _safe_parse_iso8601(ts: str | None) -> datetime:\n    \"\"\"Parse an ISO8601 timestamp for sorting purposes.\n\n    Returns datetime.min on invalid/missing input so such records sort last.\n    \"\"\"\n    if not ts:\n        return datetime.min\n    try:\n        v = ts[:-1] + \"+00:00\" if ts.endswith(\"Z\") else ts\n        return datetime.fromisoformat(v)\n    except (TypeError, ValueError):\n        return datetime.min\n\n\ndef sort_and_paginate_job_violations(\n    violation_records: List[Dict[str, Any]],\n    limit: int,\n    offset: int = 0,\n) -> Dict[str, Any]:\n    \"\"\"Sort violation records by most recent violation_timestamp (descending) and paginate.\n\n    Returns an API-ready payload:\n      {\n        \"items\": [...],\n        \"pagination\": {\n          \"total\": <int>,\n          \"limit\": <int>,\n          \"offset\": <int>,\n          \"has_more\": <bool>\n        }\n      }\n    \"\"\"\n    sorted_records = sorted(\n        violation_records,\n        key=lambda rec: _safe_parse_iso8601(rec.get(\"violation_timestamp\")),\n        reverse=True,\n    )\n\n    total = len(sorted_records)\n    if offset < 0:\n        offset = 0\n    if limit <= 0:\n        limit = total\n\n    paginated = sorted_records[offset : offset + limit]\n\n    return {\n        \"items\": paginated,\n        \"pagination\": {\n            \"total\": total,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"has_more\": offset + limit < total,\n        },\n    }\n",
            "usage": "After Steps 1\u20135, finalize the response in your backend handler with:\n\nresponse_payload = sort_and_paginate_job_violations(enriched_violations, limit=filters.limit, offset=filters.offset)\n\nReturn response_payload as JSON to the frontend (e.g., from a FastAPI or Flask route at GET /api/job-violations)."
        }
    ]
}