{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Step 1 defines a small configuration helper that maps the 'republic' label to concrete network settings (chain ID, REST/RPC endpoints, denoms) used by all subsequent backend queries.",
            "code": "from dataclasses import dataclass\nfrom typing import Dict\n\n@dataclass(frozen=True)\nclass RepublicNetworkConfig:\n    \"\"\"\n    Simple configuration object describing a Republic network.\n    Adjust the endpoints and chain IDs to match your actual deployment.\n    \"\"\"\n    label: str\n    chain_id: str\n    rest_endpoint: str\n    rpc_endpoint: str\n    stake_denom: str = \"urep\"   # micro-REP\n    display_denom: str = \"REP\"\n    display_exponent: int = 6   # 1 REP = 10^6 urep\n\n# Known network presets. Extend as your environments grow.\nNETWORK_CONFIGS: Dict[str, RepublicNetworkConfig] = {\n    \"republic\": RepublicNetworkConfig(\n        label=\"republic\",\n        chain_id=\"republic-1\",\n        rest_endpoint=\"https://api.republic.network\",\n        rpc_endpoint=\"https://rpc.republic.network\",\n    ),\n    \"republic-testnet\": RepublicNetworkConfig(\n        label=\"republic-testnet\",\n        chain_id=\"republic-testnet-1\",\n        rest_endpoint=\"https://api.testnet.republic.network\",\n        rpc_endpoint=\"https://rpc.testnet.republic.network\",\n    ),\n}\n\ndef resolve_network_from_label(label: str) -> RepublicNetworkConfig:\n    \"\"\"\n    Map a human-friendly network label to a concrete RepublicNetworkConfig.\n    Raises ValueError for unknown labels.\n    \"\"\"\n    try:\n        return NETWORK_CONFIGS[label]\n    except KeyError as exc:\n        supported = \", \".join(sorted(NETWORK_CONFIGS.keys()))\n        raise ValueError(\n            f\"Unknown Republic network label: {label!r}. \"\n            f\"Supported labels are: {supported}\"\n        ) from exc\n",
            "usage": "cfg = resolve_network_from_label('republic')"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Step 2 implements an async backend helper that queries the Republic staking module over REST to fetch the full validator set, transparently handling pagination.",
            "code": "import httpx\nfrom typing import Any, Dict, List, Optional\n\n# Reuse RepublicNetworkConfig and resolve_network_from_label from Step 1\n\nasync def query_republic_validators(\n    cfg: RepublicNetworkConfig,\n    status: str = \"BOND_STATUS_BONDED\",\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Fetch the full active validator set from the Republic staking module.\n\n    This uses the Cosmos SDK staking REST endpoint and follows pagination\n    until all validators are fetched.\n    \"\"\"\n    validators: List[Dict[str, Any]] = []\n    next_key: Optional[str] = None\n\n    async with httpx.AsyncClient(base_url=cfg.rest_endpoint, timeout=10.0) as client:\n        while True:\n            params: Dict[str, Any] = {\"status\": status}\n            if next_key:\n                params[\"pagination.key\"] = next_key\n\n            try:\n                resp = await client.get(\"/cosmos/staking/v1beta1/validators\", params=params)\n                resp.raise_for_status()\n            except httpx.HTTPError as exc:\n                raise RuntimeError(\n                    f\"Failed to query validators from {cfg.rest_endpoint}: {exc}\"\n                ) from exc\n\n            data = resp.json()\n            validators.extend(data.get(\"validators\", []))\n\n            pagination = data.get(\"pagination\") or {}\n            next_key = pagination.get(\"next_key\")\n            if not next_key:\n                break\n\n    return validators\n",
            "usage": "cfg = resolve_network_from_label('republic')\nvalidators = await query_republic_validators(cfg)"
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Step 3 adds an async helper that, for each validator, queries all delegations and computes the total staked REP, attaching this information to the validator record.",
            "code": "import asyncio\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\n\n# Reuse RepublicNetworkConfig from Step 1.\n\nasync def _fetch_single_validator_stake_total(\n    client: httpx.AsyncClient,\n    cfg: RepublicNetworkConfig,\n    operator_address: str,\n    semaphore: asyncio.Semaphore,\n) -> int:\n    \"\"\"\n    Helper to fetch and sum all delegations for a single validator.\n    Returns the total stake in the base denom (e.g. 'urep').\n    \"\"\"\n    endpoint = f\"/cosmos/staking/v1beta1/validators/{operator_address}/delegations\"\n    total_amount: int = 0\n    next_key: Optional[str] = None\n\n    async with semaphore:\n        while True:\n            params: Dict[str, Any] = {}\n            if next_key:\n                params[\"pagination.key\"] = next_key\n\n            resp = await client.get(endpoint, params=params)\n            resp.raise_for_status()\n            data = resp.json()\n            for dr in data.get(\"delegation_responses\", []):\n                balance = dr.get(\"balance\") or {}\n                if balance.get(\"denom\") == cfg.stake_denom:\n                    try:\n                        total_amount += int(balance.get(\"amount\", \"0\"))\n                    except (TypeError, ValueError):\n                        # Skip malformed balances rather than failing the whole request.\n                        continue\n\n            pagination = data.get(\"pagination\") or {}\n            next_key = pagination.get(\"next_key\")\n            if not next_key:\n                break\n\n    return total_amount\n\nasync def query_republic_validator_stake_totals(\n    cfg: RepublicNetworkConfig,\n    validators: List[Dict[str, Any]],\n    max_concurrency: int = 10,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    For each validator, compute the total delegated stake (self-bond + external)\n    and attach it as both micro-denom and human-readable REP to the record.\n    \"\"\"\n    if not validators:\n        return []\n\n    semaphore = asyncio.Semaphore(max_concurrency)\n    enriched: List[Dict[str, Any]] = []\n\n    async with httpx.AsyncClient(base_url=cfg.rest_endpoint, timeout=15.0) as client:\n        tasks = []\n        for val in validators:\n            operator_address = val.get(\"operator_address\")\n            if not operator_address:\n                continue\n            tasks.append(\n                _fetch_single_validator_stake_total(\n                    client, cfg, operator_address, semaphore\n                )\n            )\n\n        # Run queries concurrently while respecting the semaphore.\n        stake_totals = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Map back totals to validators by index\n    stake_idx = 0\n    for val in validators:\n        operator_address = val.get(\"operator_address\")\n        if not operator_address:\n            enriched.append(val)\n            continue\n\n        total_urep = 0\n        if stake_idx < len(stake_totals):\n            res = stake_totals[stake_idx]\n            stake_idx += 1\n            if isinstance(res, Exception):\n                # Log or handle per-validator errors; here we default to 0.\n                total_urep = 0\n            else:\n                total_urep = int(res)\n\n        # Attach stake information to a shallow copy of the validator record\n        v = dict(val)\n        v[\"total_stake_urep\"] = str(total_urep)\n        v[\"total_stake_rep\"] = total_urep / (10 ** cfg.display_exponent)\n        enriched.append(v)\n\n    return enriched\n",
            "usage": "cfg = resolve_network_from_label('republic')\nvalidators = await query_republic_validators(cfg)\nvalidators_with_stake = await query_republic_validator_stake_totals(cfg, validators)"
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Step 4 fetches the latest compute benchmarks (throughput, inference, FLOPs) for each validator from the Republic compute validation module and embeds them into the validator objects.",
            "code": "import asyncio\nfrom typing import Any, Dict, List\n\nimport httpx\n\n# Reuse RepublicNetworkConfig from Step 1.\n\nasync def _fetch_validator_benchmarks(\n    client: httpx.AsyncClient,\n    operator_address: str,\n    semaphore: asyncio.Semaphore,\n) -> Dict[str, float]:\n    \"\"\"\n    Fetch latest compute benchmarks for a single validator.\n    Returns a dict with numeric fields, defaulting to 0.0 when missing.\n    \"\"\"\n    endpoint = f\"/republic/compute/v1/validators/{operator_address}/benchmarks/latest\"\n\n    async with semaphore:\n        resp = await client.get(endpoint)\n        if resp.status_code == 404:\n            # Validator has not yet run benchmarks.\n            return {\n                \"throughput\": 0.0,\n                \"inference_score\": 0.0,\n                \"achieved_flops\": 0.0,\n            }\n\n        try:\n            resp.raise_for_status()\n        except httpx.HTTPError:\n            # On any other error, also fall back to zero benchmarks.\n            return {\n                \"throughput\": 0.0,\n                \"inference_score\": 0.0,\n                \"achieved_flops\": 0.0,\n            }\n\n        data = resp.json()\n        # Support either a wrapped or flat response structure.\n        bench = data.get(\"validator_benchmark\") or data\n\n        def to_float(value: Any, default: float = 0.0) -> float:\n            try:\n                return float(value)\n            except (TypeError, ValueError):\n                return default\n\n        return {\n            \"throughput\": to_float(bench.get(\"throughput\", 0.0)),\n            \"inference_score\": to_float(bench.get(\"inference_score\", 0.0)),\n            \"achieved_flops\": to_float(bench.get(\"achieved_flops\", 0.0)),\n        }\n\nasync def query_republic_compute_benchmarks(\n    cfg: RepublicNetworkConfig,\n    validators: List[Dict[str, Any]],\n    max_concurrency: int = 10,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the latest compute validation benchmarks for each validator and\n    attach them under the 'compute_benchmarks' key of the validator record.\n    \"\"\"\n    if not validators:\n        return []\n\n    semaphore = asyncio.Semaphore(max_concurrency)\n    enriched: List[Dict[str, Any]] = []\n\n    async with httpx.AsyncClient(base_url=cfg.rest_endpoint, timeout=15.0) as client:\n        tasks = []\n        operator_addresses: List[str] = []\n        for val in validators:\n            op_addr = val.get(\"operator_address\")\n            if not op_addr:\n                continue\n            operator_addresses.append(op_addr)\n            tasks.append(_fetch_validator_benchmarks(client, op_addr, semaphore))\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    bench_by_operator: Dict[str, Dict[str, float]] = {}\n    for op_addr, res in zip(operator_addresses, results):\n        if isinstance(res, Exception):\n            # On per-validator error, default to zero benchmarks.\n            bench_by_operator[op_addr] = {\n                \"throughput\": 0.0,\n                \"inference_score\": 0.0,\n                \"achieved_flops\": 0.0,\n            }\n        else:\n            bench_by_operator[op_addr] = res\n\n    for val in validators:\n        op_addr = val.get(\"operator_address\")\n        v = dict(val)\n        v[\"compute_benchmarks\"] = bench_by_operator.get(\n            op_addr,\n            {\n                \"throughput\": 0.0,\n                \"inference_score\": 0.0,\n                \"achieved_flops\": 0.0,\n            },\n        )\n        enriched.append(v)\n\n    return enriched\n",
            "usage": "cfg = resolve_network_from_label('republic')\nvalidators = await query_republic_validators(cfg)\nvalidators_with_compute = await query_republic_compute_benchmarks(cfg, validators)"
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Step 5 aggregates raw benchmark metrics into a single compute_performance_score per validator by normalizing throughput, inference, and FLOPs across the active set.",
            "code": "from typing import Any, Dict, List, Optional\n\ndef _min_max_normalize(values: List[float]) -> List[float]:\n    \"\"\"\n    Normalize a list of floats into [0, 1] via min-max scaling.\n    If all values are identical, return a list of 1.0 (everyone equal-best).\n    \"\"\"\n    if not values:\n        return []\n\n    min_v = min(values)\n    max_v = max(values)\n    if min_v == max_v:\n        return [1.0 for _ in values]\n\n    scale = max_v - min_v\n    return [(v - min_v) / scale for v in values]\n\ndef compute_validator_compute_score(\n    validators: List[Dict[str, Any]],\n    weights: Optional[Dict[str, float]] = None,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Aggregate raw compute benchmarks into a single compute_performance_score\n    per validator by normalizing each benchmark dimension and taking a\n    weighted average.\n\n    weights: mapping of metric -> weight, defaulting to:\n        throughput: 0.4\n        inference_score: 0.3\n        achieved_flops: 0.3\n    \"\"\"\n    if not validators:\n        return []\n\n    if weights is None:\n        weights = {\n            \"throughput\": 0.4,\n            \"inference_score\": 0.3,\n            \"achieved_flops\": 0.3,\n        }\n\n    # Extract metric arrays\n    throughputs: List[float] = []\n    inference_scores: List[float] = []\n    flops: List[float] = []\n\n    for v in validators:\n        bm = v.get(\"compute_benchmarks\") or {}\n        throughputs.append(float(bm.get(\"throughput\", 0.0)))\n        inference_scores.append(float(bm.get(\"inference_score\", 0.0)))\n        flops.append(float(bm.get(\"achieved_flops\", 0.0)))\n\n    # Normalize each metric across validators\n    norm_t = _min_max_normalize(throughputs)\n    norm_i = _min_max_normalize(inference_scores)\n    norm_f = _min_max_normalize(flops)\n\n    enriched: List[Dict[str, Any]] = []\n    for idx, v in enumerate(validators):\n        t = norm_t[idx]\n        i = norm_i[idx]\n        f = norm_f[idx]\n\n        score = (\n            weights.get(\"throughput\", 0.0) * t\n            + weights.get(\"inference_score\", 0.0) * i\n            + weights.get(\"achieved_flops\", 0.0) * f\n        )\n\n        v_new = dict(v)\n        v_new[\"compute_performance_score\"] = score\n        v_new[\"normalized_compute_metrics\"] = {\n            \"throughput\": t,\n            \"inference_score\": i,\n            \"achieved_flops\": f,\n        }\n        enriched.append(v_new)\n\n    return enriched\n",
            "usage": "validators_with_compute_score = compute_validator_compute_score(validators_with_compute)"
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "Step 6 queries the Republic reputation module for each validator and attaches the resulting reputation_score and related metadata.",
            "code": "import asyncio\nfrom typing import Any, Dict, List\n\nimport httpx\n\n# Reuse RepublicNetworkConfig from Step 1.\n\nasync def _fetch_validator_reputation(\n    client: httpx.AsyncClient,\n    operator_address: str,\n    semaphore: asyncio.Semaphore,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fetch reputation data for a single validator from the Republic reputation module.\n    \"\"\"\n    endpoint = f\"/republic/reputation/v1/validators/{operator_address}\"\n\n    async with semaphore:\n        resp = await client.get(endpoint)\n        if resp.status_code == 404:\n            # Validator has no recorded reputation yet.\n            return {\"score\": 0.0}\n\n        try:\n            resp.raise_for_status()\n        except httpx.HTTPError:\n            # On other errors, default to neutral reputation.\n            return {\"score\": 0.0}\n\n        data = resp.json()\n        rep = data.get(\"validator_reputation\") or data\n\n        try:\n            score = float(rep.get(\"score\", 0.0))\n        except (TypeError, ValueError):\n            score = 0.0\n\n        # You can include other reputation metadata if useful.\n        return {\n            \"score\": score,\n            \"lifetime_jobs\": rep.get(\"lifetime_jobs\"),\n            \"slashed_events\": rep.get(\"slashed_events\"),\n        }\n\nasync def query_republic_reputation_scores(\n    cfg: RepublicNetworkConfig,\n    validators: List[Dict[str, Any]],\n    max_concurrency: int = 10,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Fetch the current reputation score for each validator and attach it under\n    'reputation_score' and 'reputation_details'.\n    \"\"\"\n    if not validators:\n        return []\n\n    semaphore = asyncio.Semaphore(max_concurrency)\n    enriched: List[Dict[str, Any]] = []\n\n    async with httpx.AsyncClient(base_url=cfg.rest_endpoint, timeout=15.0) as client:\n        tasks = []\n        operator_addresses: List[str] = []\n        for val in validators:\n            op_addr = val.get(\"operator_address\")\n            if not op_addr:\n                continue\n            operator_addresses.append(op_addr)\n            tasks.append(_fetch_validator_reputation(client, op_addr, semaphore))\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n    rep_by_operator: Dict[str, Dict[str, Any]] = {}\n    for op_addr, res in zip(operator_addresses, results):\n        if isinstance(res, Exception):\n            rep_by_operator[op_addr] = {\"score\": 0.0}\n        else:\n            rep_by_operator[op_addr] = res\n\n    for val in validators:\n        op_addr = val.get(\"operator_address\")\n        rep = rep_by_operator.get(op_addr, {\"score\": 0.0})\n        v = dict(val)\n        v[\"reputation_score\"] = float(rep.get(\"score\", 0.0))\n        v[\"reputation_details\"] = rep\n        enriched.append(v)\n\n    return enriched\n",
            "usage": "validators_with_reputation = await query_republic_reputation_scores(cfg, validators_with_compute_score)"
        },
        {
            "step": 7,
            "label": "backend",
            "introduction": "Step 7 normalizes each validator's total stake, compute_performance_score, and reputation_score onto a common [0,1] scale so they can be meaningfully combined.",
            "code": "from typing import Any, Dict, List\n\ndef _min_max_normalize(values: List[float]) -> List[float]:\n    \"\"\"\n    Normalize values using min-max scaling into [0, 1].\n    If all values are identical, return a list of 1.0 (everyone equal-best).\n    \"\"\"\n    if not values:\n        return []\n\n    min_v = min(values)\n    max_v = max(values)\n    if min_v == max_v:\n        return [1.0 for _ in values]\n\n    scale = max_v - min_v\n    return [(v - min_v) / scale for v in values]\n\ndef normalize_validator_metrics(\n    validators: List[Dict[str, Any]],\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Normalize total stake, compute_performance_score, and reputation_score to [0, 1]\n    for comparability across validators.\n    \"\"\"\n    if not validators:\n        return []\n\n    stakes: List[float] = []\n    compute_scores: List[float] = []\n    reputation_scores: List[float] = []\n\n    for v in validators:\n        stakes.append(float(v.get(\"total_stake_rep\", 0.0)))\n        compute_scores.append(float(v.get(\"compute_performance_score\", 0.0)))\n        reputation_scores.append(float(v.get(\"reputation_score\", 0.0)))\n\n    norm_stake = _min_max_normalize(stakes)\n    norm_compute = _min_max_normalize(compute_scores)\n    norm_reputation = _min_max_normalize(reputation_scores)\n\n    enriched: List[Dict[str, Any]] = []\n    for idx, v in enumerate(validators):\n        v_new = dict(v)\n        v_new[\"norm_stake\"] = norm_stake[idx]\n        v_new[\"norm_compute\"] = norm_compute[idx]\n        v_new[\"norm_reputation\"] = norm_reputation[idx]\n        enriched.append(v_new)\n\n    return enriched\n",
            "usage": "normalized_validators = normalize_validator_metrics(validators_with_compute_score_and_reputation)"
        },
        {
            "step": 8,
            "label": "backend",
            "introduction": "Step 8 computes a combined_score per validator using weighted normalized stake, compute, and reputation metrics, reflecting Republic's emphasis on stake and verified compute quality.",
            "code": "from typing import Any, Dict, List, Optional\n\ndef compute_combined_validator_score(\n    validators: List[Dict[str, Any]],\n    weights: Optional[Dict[str, float]] = None,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Compute a final combined_score per validator using normalized stake,\n    normalized compute, and normalized reputation metrics.\n\n    Default weights (can be overridden):\n        stake: 0.4\n        compute: 0.4\n        reputation: 0.2\n    \"\"\"\n    if not validators:\n        return []\n\n    if weights is None:\n        weights = {\"stake\": 0.4, \"compute\": 0.4, \"reputation\": 0.2}\n\n    enriched: List[Dict[str, Any]] = []\n\n    for v in validators:\n        norm_stake = float(v.get(\"norm_stake\", 0.0))\n        norm_compute = float(v.get(\"norm_compute\", 0.0))\n        norm_reputation = float(v.get(\"norm_reputation\", 0.0))\n\n        combined = (\n            weights.get(\"stake\", 0.0) * norm_stake\n            + weights.get(\"compute\", 0.0) * norm_compute\n            + weights.get(\"reputation\", 0.0) * norm_reputation\n        )\n\n        v_new = dict(v)\n        v_new[\"combined_score\"] = combined\n        enriched.append(v_new)\n\n    return enriched\n",
            "usage": "validators_with_combined_score = compute_combined_validator_score(normalized_validators)"
        },
        {
            "step": 9,
            "label": "backend",
            "introduction": "Step 9 sorts validators in descending order of combined_score with deterministic tie-breakers on norm_compute and operator_address.",
            "code": "from typing import Any, Dict, List\n\ndef sort_validators(validators: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Sort validators in descending order of combined_score.\n    Tie-breakers:\n      1. Higher norm_compute first\n      2. Lexicographic operator_address\n    \"\"\"\n    return sorted(\n        validators,\n        key=lambda v: (\n            -float(v.get(\"combined_score\", 0.0)),\n            -float(v.get(\"norm_compute\", 0.0)),\n            str(v.get(\"operator_address\", \"\")),\n        ),\n    )\n",
            "usage": "sorted_validators = sort_validators(validators_with_combined_score)"
        },
        {
            "step": 10,
            "label": "backend",
            "introduction": "Step 10 formats the sorted validator data into a clean JSON payload suitable for returning from a BFF API endpoint or rendering directly in a UI.",
            "code": "from typing import Any, Dict, List\n\ndef format_validator_list_response(\n    validators: List[Dict[str, Any]],\n) -> Dict[str, Any]:\n    \"\"\"\n    Format a sorted validator list into an API / UI friendly response.\n    Expects validators to already be sorted in the desired order.\n    \"\"\"\n    items: List[Dict[str, Any]] = []\n\n    for rank, v in enumerate(validators, start=1):\n        desc = v.get(\"description\") or {}\n        compute_benchmarks = v.get(\"compute_benchmarks\") or {}\n        normalized_compute = v.get(\"normalized_compute_metrics\") or {}\n\n        items.append(\n            {\n                \"rank\": rank,\n                \"operator_address\": v.get(\"operator_address\"),\n                \"moniker\": desc.get(\"moniker\"),\n                \"identity\": desc.get(\"identity\"),\n                \"website\": desc.get(\"website\"),\n                \"total_stake\": {\n                    \"amount_urep\": v.get(\"total_stake_urep\"),\n                    \"amount_rep\": v.get(\"total_stake_rep\"),\n                    \"denom\": \"REP\",\n                },\n                \"compute_benchmarks\": {\n                    \"throughput\": compute_benchmarks.get(\"throughput\"),\n                    \"inference_score\": compute_benchmarks.get(\"inference_score\"),\n                    \"achieved_flops\": compute_benchmarks.get(\"achieved_flops\"),\n                },\n                \"reputation_score\": v.get(\"reputation_score\"),\n                \"normalized_metrics\": {\n                    \"stake\": v.get(\"norm_stake\"),\n                    \"compute\": v.get(\"norm_compute\"),\n                    \"reputation\": v.get(\"norm_reputation\"),\n                    \"normalized_compute_components\": normalized_compute,\n                },\n                \"scores\": {\n                    \"compute_performance_score\": v.get(\"compute_performance_score\"),\n                    \"combined_score\": v.get(\"combined_score\"),\n                },\n            }\n        )\n\n    return {\n        \"validators\": items,\n        \"count\": len(items),\n    }\n",
            "usage": "response_payload = format_validator_list_response(sorted_validators)\n# Return `response_payload` from your BFF endpoint for consumption by the frontend."
        }
    ]
}