{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Step 1 defines a backend helper that, given the 'republic' chain label, loads RPC/gRPC and indexer endpoints plus relevant module metadata (compute, staking, rewards) needed for later transaction queries.",
            "code": "import os\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ModuleConfig:\n    \"\"\"Configuration for a single Cosmos SDK / Republic module.\"\"\"\n    name: str\n    msg_type_prefix: str  # e.g. 'republic.compute.v1'\n\n\n@dataclass\nclass ChainConfig:\n    \"\"\"Runtime configuration for a Republic-chain client and indexer access.\"\"\"\n    chain_id: str\n    label: str\n    rpc_endpoint: str\n    grpc_endpoint: str\n    rest_endpoint: str\n    indexer_http_endpoint: str\n    compute_module: ModuleConfig\n    staking_module: ModuleConfig\n    rewards_module: ModuleConfig\n\n\ndef load_chain_config_for_label(label: str) -> ChainConfig:\n    '''\n    Load RPC/gRPC, REST, and indexer endpoints for a given chain label.\n\n    For now this is a simple mapping for the 'republic' label, but in a\n    production BFF you would typically either:\n      * Read this from environment variables / config files, or\n      * Query a chain-registry service.\n\n    Raises:\n        ValueError: if the label is unknown.\n    '''\n    if not isinstance(label, str):\n        raise TypeError('label must be a string')\n\n    normalized = label.strip().lower()\n    if normalized != 'republic':\n        raise ValueError(f\"Unsupported chain label: {label}\")\n\n    # Allow overriding via environment variables so this can be deployed\n    # across different environments (devnet, testnet, mainnet).\n    rpc_endpoint = os.getenv('REPUBLIC_RPC_ENDPOINT', 'https://rpc.republic.network:443')\n    grpc_endpoint = os.getenv('REPUBLIC_GRPC_ENDPOINT', 'https://grpc.republic.network:443')\n    rest_endpoint = os.getenv('REPUBLIC_REST_ENDPOINT', 'https://api.republic.network')\n    # A hypothetical HTTP indexer that understands compute / job queries.\n    indexer_http_endpoint = os.getenv(\n        'REPUBLIC_INDEXER_HTTP_ENDPOINT',\n        rest_endpoint.rstrip('/') + '/indexer'\n    )\n\n    logger.debug('Loaded Republic chain config', extra={\n        'rpc_endpoint': rpc_endpoint,\n        'grpc_endpoint': grpc_endpoint,\n        'rest_endpoint': rest_endpoint,\n        'indexer_http_endpoint': indexer_http_endpoint,\n    })\n\n    compute_module = ModuleConfig(\n        name='compute',\n        # Using a plausible type-prefix; adjust to the actual protobuf package\n        msg_type_prefix='republic.compute.v1',\n    )\n    staking_module = ModuleConfig(\n        name='staking',\n        msg_type_prefix='cosmos.staking.v1beta1',\n    )\n    rewards_module = ModuleConfig(\n        name='rewards',\n        # Native rewards / distribution messages\n        msg_type_prefix='cosmos.distribution.v1beta1',\n    )\n\n    return ChainConfig(\n        chain_id=os.getenv('REPUBLIC_CHAIN_ID', 'republic-1'),\n        label=normalized,\n        rpc_endpoint=rpc_endpoint,\n        grpc_endpoint=grpc_endpoint,\n        rest_endpoint=rest_endpoint,\n        indexer_http_endpoint=indexer_http_endpoint,\n        compute_module=compute_module,\n        staking_module=staking_module,\n        rewards_module=rewards_module,\n    )\n",
            "usage": "config = load_chain_config_for_label('republic')"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Step 2 normalizes raw request/UI parameters into a typed filter object for compute-related activity, validating optional address, job_id, validator, height/time ranges, and pagination.",
            "code": "import logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ComputeTxFilter:\n    \"\"\"Normalized filter for Republic compute-related transactions.\"\"\"\n    address: Optional[str] = None\n    job_id: Optional[str] = None\n    validator_address: Optional[str] = None\n    min_height: Optional[int] = None\n    max_height: Optional[int] = None\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n    page: int = 1\n    page_size: int = 50\n    cursor: Optional[str] = None  # for cursor-based pagination, if supported\n\n\ndef _parse_int(value: Any, field_name: str) -> Optional[int]:\n    if value is None or value == '':\n        return None\n    try:\n        ivalue = int(value)\n        if ivalue < 0:\n            raise ValueError\n        return ivalue\n    except (TypeError, ValueError) as exc:\n        raise ValueError(f\"Invalid integer for {field_name}: {value!r}\") from exc\n\n\ndef _parse_iso_datetime(value: Any, field_name: str) -> Optional[datetime]:\n    if value is None or value == '':\n        return None\n    if isinstance(value, datetime):\n        # Normalize to UTC\n        return value.astimezone(timezone.utc)\n    if not isinstance(value, str):\n        raise ValueError(f\"{field_name} must be an ISO8601 string or datetime, got {type(value)}\")\n    try:\n        # fromisoformat handles 'YYYY-MM-DDTHH:MM:SS[.ffffff][+HH:MM]'\n        dt = datetime.fromisoformat(value)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        else:\n            dt = dt.astimezone(timezone.utc)\n        return dt\n    except Exception as exc:  # noqa: BLE001\n        raise ValueError(f\"Invalid ISO8601 datetime for {field_name}: {value!r}\") from exc\n\n\ndef normalize_compute_tx_filter_params(raw_params: Dict[str, Any]) -> ComputeTxFilter:\n    '''\n    Normalize and validate incoming filter parameters from a request or UI form.\n\n    Expected keys (all optional):\n        - address: str (sender or participant address)\n        - job_id: str\n        - validator_address: str\n        - min_height, max_height: int or str\n        - start_time, end_time: ISO8601 datetime strings\n        - page: int (1-based)\n        - page_size: int\n        - cursor: str (opaque pagination cursor)\n\n    Raises:\n        TypeError / ValueError on invalid input.\n    '''\n    if raw_params is None:\n        raw_params = {}\n    if not isinstance(raw_params, dict):\n        raise TypeError('raw_params must be a dict-like object')\n\n    address = raw_params.get('address') or None\n    job_id = raw_params.get('job_id') or None\n    validator_address = raw_params.get('validator_address') or None\n\n    min_height = _parse_int(raw_params.get('min_height'), 'min_height')\n    max_height = _parse_int(raw_params.get('max_height'), 'max_height')\n\n    if min_height is not None and max_height is not None and min_height > max_height:\n        raise ValueError(f'min_height ({min_height}) cannot be greater than max_height ({max_height})')\n\n    start_time = _parse_iso_datetime(raw_params.get('start_time'), 'start_time')\n    end_time = _parse_iso_datetime(raw_params.get('end_time'), 'end_time')\n\n    if start_time and end_time and start_time > end_time:\n        raise ValueError('start_time cannot be later than end_time')\n\n    # Basic page-based pagination defaults\n    page = raw_params.get('page') or 1\n    try:\n        page = int(page)\n        if page < 1:\n            raise ValueError\n    except (TypeError, ValueError) as exc:\n        raise ValueError(f'page must be a positive integer, got {page!r}') from exc\n\n    page_size = raw_params.get('page_size') or 50\n    try:\n        page_size = int(page_size)\n        # Put a sane upper bound to protect the backend\n        if page_size < 1 or page_size > 200:\n            raise ValueError\n    except (TypeError, ValueError) as exc:\n        raise ValueError(f'page_size must be between 1 and 200, got {page_size!r}') from exc\n\n    cursor = raw_params.get('cursor') or None\n\n    if cursor:\n        # In cursor mode, page is often ignored by the indexer, but we still track it\n        logger.debug('Using cursor-based pagination for compute tx search', extra={'cursor': cursor})\n\n    filt = ComputeTxFilter(\n        address=address,\n        job_id=job_id,\n        validator_address=validator_address,\n        min_height=min_height,\n        max_height=max_height,\n        start_time=start_time,\n        end_time=end_time,\n        page=page,\n        page_size=page_size,\n        cursor=cursor,\n    )\n\n    logger.debug('Normalized compute-tx filter', extra={'filter': filt})\n    return filt\n",
            "usage": "normalized_filter = normalize_compute_tx_filter_params(request_json_body)"
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Step 3 converts the normalized compute filter into a concrete HTTP payload targeting the Republic transaction index, focusing on compute-job-related messages and events.",
            "code": "from dataclasses import dataclass\nfrom typing import Any, Dict, List\n\n# Reuse ChainConfig from step 1 and ComputeTxFilter from step 2.\n\n\n@dataclass\nclass TxSearchQuery:\n    \"\"\"Represents a query to the Republic transaction indexer.\"\"\"\n    url: str\n    payload: Dict[str, Any]\n\n\n# Known compute-related message type URLs for Republic. These may evolve as the\n# protocol expands; keep this list in sync with on-chain protobuf definitions.\nCOMPUTE_MSG_TYPE_URLS: List[str] = [\n    '/republic.compute.v1.MsgScheduleJob',\n    '/republic.compute.v1.MsgAssignJob',\n    '/republic.compute.v1.MsgSubmitJobResult',\n    '/republic.compute.v1.MsgSubmitJobProof',\n    '/republic.compute.v1.MsgValidateJob',\n]\n\n\ndef build_compute_tx_search_query(config: 'ChainConfig', filt: 'ComputeTxFilter') -> TxSearchQuery:\n    '''\n    Build a transaction search query for the Republic indexer from a normalized\n    ComputeTxFilter.\n\n    The returned payload assumes a hypothetical POST /txs/search endpoint on\n    the indexer that accepts filters like:\n\n        {\n          \"modules\": [\"compute\"],\n          \"msg_type_urls\": [\"/republic.compute.v1.MsgScheduleJob\", ...],\n          \"filters\": { ... },\n          \"pagination\": { ... }\n        }\n\n    This is typical of modern indexer designs and can be adapted once the\n    concrete Republic index API is finalized.\n    '''\n    if config is None:\n        raise ValueError('config must not be None')\n    if filt is None:\n        raise ValueError('filt must not be None')\n\n    base_url = config.indexer_http_endpoint.rstrip('/') + '/txs/search'\n\n    filters: Dict[str, Any] = {\n        # Restrict to compute-module messages by default\n        'modules': ['compute'],\n        'msg_type_urls': COMPUTE_MSG_TYPE_URLS,\n    }\n\n    # Optional address / job / validator filters\n    if filt.address:\n        filters['address'] = filt.address\n    if filt.job_id:\n        filters['job_id'] = filt.job_id\n    if filt.validator_address:\n        filters['validator_address'] = filt.validator_address\n\n    # Optional block-height range\n    if filt.min_height is not None:\n        filters['min_height'] = filt.min_height\n    if filt.max_height is not None:\n        filters['max_height'] = filt.max_height\n\n    # Optional time range (RFC3339 / ISO8601 in UTC)\n    if filt.start_time is not None:\n        filters['start_time'] = filt.start_time.isoformat()\n    if filt.end_time is not None:\n        filters['end_time'] = filt.end_time.isoformat()\n\n    pagination: Dict[str, Any] = {\n        'page': filt.page,\n        'page_size': filt.page_size,\n    }\n    if filt.cursor:\n        pagination['cursor'] = filt.cursor\n\n    payload: Dict[str, Any] = {\n        'filters': filters,\n        'pagination': pagination,\n        # Explicit chain context to allow multi-chain indexers\n        'chain_id': config.chain_id,\n    }\n\n    return TxSearchQuery(url=base_url, payload=payload)\n",
            "usage": "search_query = build_compute_tx_search_query(config, normalized_filter)"
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Step 4 executes the built search query against the Republic transaction indexer over HTTP, handling network and HTTP-level errors and returning the raw JSON response.",
            "code": "import logging\nfrom typing import Any, Dict\n\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n# Reuse TxSearchQuery from step 3.\n\n\nclass RepublicIndexerError(RuntimeError):\n    \"\"\"Raised when the Republic indexer returns an error or invalid payload.\"\"\"\n\n\nasync def query_transaction_index(search_query: 'TxSearchQuery', timeout: float = 10.0) -> Dict[str, Any]:\n    '''\n    Execute the provided TxSearchQuery against the Republic transaction indexer.\n\n    Args:\n        search_query: TxSearchQuery produced by build_compute_tx_search_query.\n        timeout: HTTP timeout in seconds.\n\n    Returns:\n        Parsed JSON body from the indexer as a Python dict.\n\n    Raises:\n        RepublicIndexerError: on network issues or unexpected HTTP status.\n        ValueError: if the indexer returns a non-JSON or non-dict body.\n    '''\n    if search_query is None:\n        raise ValueError('search_query must not be None')\n\n    logger.debug('Querying Republic transaction indexer', extra={\n        'url': search_query.url,\n        'payload': search_query.payload,\n    })\n\n    try:\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            response = await client.post(search_query.url, json=search_query.payload)\n            response.raise_for_status()\n    except httpx.RequestError as exc:\n        logger.error('Network error while calling Republic indexer', exc_info=exc)\n        raise RepublicIndexerError(f'Network error calling indexer: {exc!r}') from exc\n    except httpx.HTTPStatusError as exc:\n        logger.error(\n            'HTTP error from Republic indexer',\n            extra={'status_code': exc.response.status_code, 'body': exc.response.text},\n        )\n        raise RepublicIndexerError(\n            f'Indexer returned HTTP {exc.response.status_code}: {exc.response.text}'\n        ) from exc\n\n    try:\n        data = response.json()\n    except ValueError as exc:  # JSON decode error\n        logger.error('Failed to decode JSON response from indexer', exc_info=exc)\n        raise RepublicIndexerError('Indexer returned non-JSON response') from exc\n\n    if not isinstance(data, dict):\n        raise ValueError('Unexpected indexer response shape: expected a JSON object at the top level')\n\n    logger.debug('Received response from Republic indexer', extra={'keys': list(data.keys())})\n    return data\n",
            "usage": "raw_indexer_response = await query_transaction_index(search_query)"
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Step 5 post-processes the raw indexer results to keep only compute job scheduling/validation-related messages and extracts a structured view (tx_hash, height, timestamp, message type, job_id, sender, validator).",
            "code": "import logging\nfrom typing import Any, Dict, List, Optional\n\nlogger = logging.getLogger(__name__)\n\n# Map raw message type URLs to higher-level logical action names.\nCOMPUTE_MSG_TYPE_MAP: Dict[str, str] = {\n    '/republic.compute.v1.MsgScheduleJob': 'schedule_job',\n    '/republic.compute.v1.MsgAssignJob': 'assign_job',\n    '/republic.compute.v1.MsgSubmitJobResult': 'submit_job_result',\n    '/republic.compute.v1.MsgSubmitJobProof': 'submit_job_proof',\n    '/republic.compute.v1.MsgValidateJob': 'validate_job',\n}\n\n\ndef _extract_attr_from_events(events: Any, key: str) -> Optional[str]:\n    \"\"\"Best-effort extraction of an attribute (e.g., job_id) from Cosmos-style events/logs.\"\"\"\n    try:\n        # Some indexers expose a flat list of events; others nest under logs[].events.\n        if isinstance(events, list) and events and 'type' in events[0] and 'attributes' in events[0]:\n            # events: [{\"type\": ..., \"attributes\": [{\"key\": \"job_id\", \"value\": \"...\"}, ...]}]\n            for ev in events:\n                for attr in ev.get('attributes', []) or []:\n                    if attr.get('key') == key:\n                        return attr.get('value')\n        elif isinstance(events, list):\n            # Possibly logs: [{\"events\": [...]}, ...]\n            for log in events:\n                for ev in log.get('events', []) or []:\n                    for attr in ev.get('attributes', []) or []:\n                        if attr.get('key') == key:\n                            return attr.get('value')\n    except Exception:  # noqa: BLE001\n        # Never let event parsing crash tx post-processing\n        logger.debug('Failed to extract %s from events', key, exc_info=True)\n    return None\n\n\ndef postprocess_compute_transactions(raw_response: Dict[str, Any]) -> List[Dict[str, Any]]:\n    '''\n    Filter raw indexer results down to compute job scheduling / validation\n    transactions and extract a compact, structured representation suitable\n    for API responses or UI consumption.\n\n    The function is intentionally defensive regarding the indexer schema and\n    supports a few common layouts:\n        - {\"transactions\": [...]} (hypothetical Republic indexer)\n        - {\"txs\": [...]} (Cosmos /cosmos/tx style)\n\n    Each returned item has at least:\n        - tx_hash: str\n        - block_height: int | None\n        - timestamp: str | None\n        - message_type: str (logical type, e.g. 'schedule_job')\n        - job_id: str | None\n        - sender: str | None\n        - validator: str | None\n    '''\n    if raw_response is None or not isinstance(raw_response, dict):\n        raise ValueError('raw_response must be a dict returned from the indexer')\n\n    txs = raw_response.get('transactions') or raw_response.get('txs') or []\n    if not isinstance(txs, list):\n        raise ValueError('Expected a list under \"transactions\" or \"txs\" in indexer response')\n\n    results: List[Dict[str, Any]] = []\n\n    for tx in txs:\n        if not isinstance(tx, dict):\n            continue\n\n        tx_hash = tx.get('hash') or tx.get('txhash')\n        height_raw = tx.get('height') or tx.get('block_height')\n        try:\n            block_height = int(height_raw) if height_raw is not None else None\n        except (TypeError, ValueError):\n            block_height = None\n\n        timestamp = tx.get('timestamp')\n\n        # Messages may be directly under tx['body']['messages'] or under tx['tx']['body']['messages']\n        body = tx.get('body') or tx.get('tx', {}).get('body') or {}\n        messages = body.get('messages') or []\n\n        # Events/logs may carry additional info like job_id\n        events = tx.get('events') or tx.get('logs') or []\n\n        for msg in messages:\n            if not isinstance(msg, dict):\n                continue\n\n            msg_type = msg.get('@type') or msg.get('type_url')\n            logical_type = COMPUTE_MSG_TYPE_MAP.get(msg_type)\n            if logical_type is None:\n                # Not a compute-job-related message we care about\n                continue\n\n            # Extract fields that are common in compute job messages\n            job_id = msg.get('job_id')\n            sender = msg.get('sender') or msg.get('from_address')\n            validator = msg.get('validator') or msg.get('validator_address')\n\n            if job_id is None and events:\n                # Fallback: look up job_id from events\n                job_id = _extract_attr_from_events(events, 'job_id')\n\n            item: Dict[str, Any] = {\n                'tx_hash': tx_hash,\n                'block_height': block_height,\n                'timestamp': timestamp,\n                'message_type': logical_type,\n                'job_id': job_id,\n                'sender': sender,\n                'validator': validator,\n            }\n\n            results.append(item)\n\n    logger.debug('Post-processed compute transactions', extra={'count': len(results)})\n    return results\n",
            "usage": "compute_transactions = postprocess_compute_transactions(raw_indexer_response)"
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "Step 6 wraps the filtered compute transactions into a standardized paginated response object, normalizing various possible pagination fields from the indexer.",
            "code": "from typing import Any, Dict, List, Optional\n\n\ndef _extract_pagination_meta(raw_response: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Normalize pagination metadata from different possible indexer shapes.\"\"\"\n    # Try a top-level 'pagination' first\n    pagination = raw_response.get('pagination') or {}\n    if not isinstance(pagination, dict):\n        pagination = {}\n\n    # Some indexers may expose cursor-based fields directly\n    next_page_token: Optional[str] = (\n        pagination.get('next_cursor')\n        or pagination.get('next_key')\n        or pagination.get('next_page_token')\n    )\n    prev_page_token: Optional[str] = (\n        pagination.get('prev_cursor')\n        or pagination.get('prev_page_token')\n    )\n\n    # Page-based pagination\n    page = pagination.get('page')\n    page_size = pagination.get('limit') or pagination.get('page_size')\n    total = pagination.get('total')\n\n    return {\n        'next_page_token': next_page_token,\n        'prev_page_token': prev_page_token,\n        'page': page,\n        'page_size': page_size,\n        'total': total,\n    }\n\n\ndef format_paginated_tx_response(\n    items: List[Dict[str, Any]],\n    raw_response: Dict[str, Any],\n) -> Dict[str, Any]:\n    '''\n    Assemble a clean, paginated response for compute-related transactions.\n\n    Args:\n        items: List of structured tx dicts produced by postprocess_compute_transactions.\n        raw_response: Original indexer response, used to pull pagination metadata.\n\n    Returns:\n        A dict ready to be returned from a BFF endpoint, e.g.:\n\n        {\n          \"transactions\": [...],\n          \"pagination\": {\n            \"next_page_token\": \"...\",\n            \"prev_page_token\": null,\n            \"page\": 1,\n            \"page_size\": 50,\n            \"total\": 123\n          }\n        }\n    '''\n    if not isinstance(raw_response, dict):\n        raise ValueError('raw_response must be a dict')\n\n    pagination_meta = _extract_pagination_meta(raw_response)\n\n    # Derived helpers for clients\n    has_next = bool(pagination_meta.get('next_page_token'))\n    has_prev = bool(pagination_meta.get('prev_page_token'))\n    pagination_meta['has_next'] = has_next\n    pagination_meta['has_prev'] = has_prev\n\n    return {\n        'transactions': items,\n        'pagination': pagination_meta,\n    }\n",
            "usage": "response_body = format_paginated_tx_response(compute_transactions, raw_indexer_response)"
        }
    ]
}