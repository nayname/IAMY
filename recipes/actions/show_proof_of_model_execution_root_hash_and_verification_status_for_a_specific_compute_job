{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "This backend utility validates and normalizes a Republic job_id into a canonical form used by other backend queries.",
            "code": "import re\nimport uuid\n\n# Compiled regex for a 64-character hex job ID, with optional `0x` prefix.\nHEX_JOB_ID_RE = re.compile(r'^(0x)?[0-9a-fA-F]{64}$')\n\n\ndef validate_and_normalize_job_id(job_id: str) -> str:\n    # Validate a Republic job identifier and normalize it to the\n    # canonical form used by backend queries.\n    if not isinstance(job_id, str):\n        raise TypeError('job_id must be a string')\n\n    raw = job_id.strip()\n    if not raw:\n        raise ValueError('job_id cannot be empty or whitespace')\n\n    # Hex-encoded job ID (typical for content-addressed identifiers).\n    if HEX_JOB_ID_RE.match(raw):\n        # Strip optional 0x prefix and lowercase for canonical form.\n        if raw.startswith(('0x', '0X')):\n            raw = raw[2:]\n        return raw.lower()\n\n    # UUID-style job ID.\n    try:\n        parsed_uuid = uuid.UUID(raw)\n        # uuid.UUID.__str__() returns a canonical lowercase form with hyphens.\n        return str(parsed_uuid)\n    except ValueError:\n        pass\n\n    # If we reach here, the format is invalid.\n    raise ValueError(\n        'Invalid job_id format. Expected 64-char hex (optionally 0x-prefixed) '\n        'or RFC 4122 UUID string.'\n    )\n",
            "usage": "normalized_job_id = validate_and_normalize_job_id(raw_job_id)"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "This backend function queries the Republic job registry or indexer to fetch metadata for a given job_id, with a mock fallback until real APIs are available.",
            "code": "import os\nfrom typing import Optional, Any, Dict\n\nimport httpx\n\n\nclass JobNotFoundError(Exception):\n    pass\n\n\nclass JobRegistryError(Exception):\n    pass\n\n\nREPUBLIC_JOB_REGISTRY_URL = os.getenv(\n    'REPUBLIC_JOB_REGISTRY_URL',\n    'https://indexer.republic.org/jobs'\n)\n\nMOCK_REPUBLIC = os.getenv('MOCK_REPUBLIC', '1') == '1'\n\n\nasync def fetch_job_metadata(\n    job_id: str,\n    *,\n    client: Optional[httpx.AsyncClient] = None,\n) -> Dict[str, Any]:\n    # Fetch basic job metadata from the Republic job registry or indexer.\n    if MOCK_REPUBLIC:\n        # Mocked response; replace with real registry call when available.\n        return {\n            'job_id': job_id,\n            'validator_id': 'repval1mockvalidatorxxxxxxxxxxxxxxxxxxxx',\n            'client_id': 'repclient1mockclientxxxxxxxxxxxxxxxxxxxxx',\n            'job_type': 'inference_benchmark',\n            'assigned_time': '2025-01-01T00:00:00Z',\n            'completion_time': '2025-01-01T00:10:00Z',\n            'status': 'completed',\n        }\n\n    should_close = False\n    if client is None:\n        client = httpx.AsyncClient(timeout=5.0)\n        should_close = True\n\n    try:\n        url = f'{REPUBLIC_JOB_REGISTRY_URL}/{job_id}'\n        resp = await client.get(url)\n        if resp.status_code == 404:\n            raise JobNotFoundError(f'Job {job_id} not found in registry')\n        resp.raise_for_status()\n        data = resp.json()\n        # Normalize the response into a stable shape expected by callers.\n        return {\n            'job_id': job_id,\n            'validator_id': data.get('validator_id') or data.get('validatorAddress'),\n            'client_id': data.get('client_id') or data.get('clientAddress'),\n            'job_type': data.get('job_type') or data.get('type'),\n            'assigned_time': data.get('assigned_time') or data.get('assignedAt'),\n            'completion_time': data.get('completion_time') or data.get('completedAt'),\n            'status': data.get('status'),\n        }\n    except httpx.HTTPError as exc:\n        raise JobRegistryError(f'Failed to query job registry: {exc}') from exc\n    finally:\n        if should_close:\n            await client.aclose()\n",
            "usage": "job_metadata = await fetch_job_metadata(normalized_job_id)"
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "This backend function retrieves the PoME root hash and proof metadata for a job from the PoME store or returns None if no proof has been submitted yet.",
            "code": "import os\nfrom typing import Optional, Dict, Any\n\nimport httpx\n\n\nclass PoMEStoreError(Exception):\n    pass\n\n\nREPUBLIC_POME_STORE_URL = os.getenv(\n    'REPUBLIC_POME_STORE_URL',\n    'https://indexer.republic.org/pome'\n)\n\nMOCK_REPUBLIC = os.getenv('MOCK_REPUBLIC', '1') == '1'\n\n\nasync def fetch_pome_root_hash(\n    job_id: str,\n    *,\n    client: Optional[httpx.AsyncClient] = None,\n) -> Optional[Dict[str, Any]]:\n    # Fetch the PoME root hash and associated proof metadata for a job.\n    if MOCK_REPUBLIC:\n        # Example mocked PoME record; replace with a real query once the\n        # Republic PoME API is available.\n        return {\n            'job_id': job_id,\n            'root_hash': 'abcd' * 16,  # 64-character hex string\n            'checkpoint_count': 128,\n            'proof_version': 'v1',\n            'submission_block_height': 123456,\n            'submission_timestamp': '2025-01-01T00:10:05Z',\n        }\n\n    should_close = False\n    if client is None:\n        client = httpx.AsyncClient(timeout=5.0)\n        should_close = True\n\n    try:\n        url = f'{REPUBLIC_POME_STORE_URL}/jobs/{job_id}'\n        resp = await client.get(url)\n        if resp.status_code == 404:\n            # No PoME proof has been submitted yet for this job.\n            return None\n        resp.raise_for_status()\n        data = resp.json()\n        return {\n            'job_id': job_id,\n            'root_hash': data.get('root_hash'),\n            'checkpoint_count': data.get('checkpoint_count') or data.get('numCheckpoints'),\n            'proof_version': data.get('proof_version') or data.get('version'),\n            'submission_block_height': data.get('submission_block_height') or data.get('blockHeight'),\n            'submission_timestamp': data.get('submission_timestamp') or data.get('submittedAt'),\n        }\n    except httpx.HTTPError as exc:\n        raise PoMEStoreError(f'Failed to query PoME store: {exc}') from exc\n    finally:\n        if should_close:\n            await client.aclose()\n",
            "usage": "pome_proof = await fetch_pome_root_hash(normalized_job_id)"
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "This backend function fetches all verifier re-execution records associated with a job_id from the verification indexer.",
            "code": "import os\nfrom datetime import datetime\nfrom typing import Optional, List, Dict, Any\n\nimport httpx\n\n\nclass VerificationIndexerError(Exception):\n    pass\n\n\nREPUBLIC_VERIFICATION_INDEXER_URL = os.getenv(\n    'REPUBLIC_VERIFICATION_INDEXER_URL',\n    'https://indexer.republic.org/verifications'\n)\n\nMOCK_REPUBLIC = os.getenv('MOCK_REPUBLIC', '1') == '1'\n\n\nasync def fetch_verification_results_for_job(\n    job_id: str,\n    *,\n    client: Optional[httpx.AsyncClient] = None,\n) -> List[Dict[str, Any]]:\n    # Query the verification indexer for all verifier re-execution records\n    # associated with a given job_id.\n    if MOCK_REPUBLIC:\n        # Return an example set of verifier records for local development.\n        now_iso = datetime.utcnow().isoformat() + 'Z'\n        return [\n            {\n                'job_id': job_id,\n                'verifier_address': 'repval1verifieraaaaaaaaaaaaaaaaaaaaaa',\n                'local_root_hash': 'abcd' * 16,\n                'verification_outcome': 'match',\n                'timestamp': now_iso,\n            },\n            {\n                'job_id': job_id,\n                'verifier_address': 'repval1verifierbbbbbbbbbbbbbbbbbbbbbb',\n                'local_root_hash': 'abcd' * 16,\n                'verification_outcome': 'match',\n                'timestamp': now_iso,\n            },\n        ]\n\n    should_close = False\n    if client is None:\n        client = httpx.AsyncClient(timeout=5.0)\n        should_close = True\n\n    try:\n        url = f'{REPUBLIC_VERIFICATION_INDEXER_URL}?job_id={job_id}'\n        resp = await client.get(url)\n        resp.raise_for_status()\n        raw_items = resp.json() or []\n        results: List[Dict[str, Any]] = []\n\n        for item in raw_items:\n            results.append(\n                {\n                    'job_id': job_id,\n                    'verifier_address': item.get('verifier_address') or item.get('verifier'),\n                    'local_root_hash': item.get('local_root_hash'),\n                    'verification_outcome': (item.get('verification_outcome') or item.get('outcome') or '').lower(),\n                    'timestamp': item.get('timestamp') or item.get('verified_at'),\n                }\n            )\n\n        return results\n    except httpx.HTTPError as exc:\n        raise VerificationIndexerError(f'Failed to query verification indexer: {exc}') from exc\n    finally:\n        if should_close:\n            await client.aclose()\n",
            "usage": "verifications = await fetch_verification_results_for_job(normalized_job_id)"
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "This backend helper aggregates per-verifier results into an overall verification status for the job.",
            "code": "from datetime import datetime, timezone\nfrom typing import List, Dict, Any, Optional\n\n\ndef _parse_timestamp(ts: Optional[str]) -> datetime:\n    # Helper to robustly parse ISO 8601 timestamps; falls back to datetime.min.\n    if not ts or not isinstance(ts, str):\n        return datetime.min.replace(tzinfo=timezone.utc)\n    try:\n        # Support trailing 'Z' for UTC.\n        if ts.endswith('Z'):\n            ts = ts[:-1] + '+00:00'\n        dt = datetime.fromisoformat(ts)\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt\n    except ValueError:\n        return datetime.min.replace(tzinfo=timezone.utc)\n\n\ndef compute_overall_verification_status(\n    job_id: str,\n    recorded_root_hash: Optional[str],\n    verifications: List[Dict[str, Any]],\n    *,\n    min_matching_verifiers: int = 2,\n    min_matching_ratio: float = 0.66,\n) -> Dict[str, Any]:\n    # Aggregate per-verifier outcomes into a single verification status for the job.\n    total = len(verifications)\n\n    # Parse timestamps up front so we can compute last_verified_at.\n    timestamps = [_parse_timestamp(v.get('timestamp')) for v in verifications]\n    real_timestamps = [t for t in timestamps if t != datetime.min.replace(tzinfo=timezone.utc)]\n    last_verified_at = max(real_timestamps).isoformat() if real_timestamps else None\n\n    # If there is no recorded PoME root hash or no verifications yet, we cannot decide.\n    if not recorded_root_hash or total == 0:\n        return {\n            'job_id': job_id,\n            'recorded_root_hash': recorded_root_hash,\n            'overall_status': 'PENDING',\n            'number_of_verifiers': total,\n            'number_of_matches': 0,\n            'number_of_mismatches': 0,\n            'last_verified_at': last_verified_at,\n        }\n\n    num_matches = 0\n    num_mismatches = 0\n\n    for v in verifications:\n        outcome = (v.get('verification_outcome') or '').lower()\n        local_root_hash = v.get('local_root_hash')\n\n        if outcome == 'match' and local_root_hash == recorded_root_hash:\n            num_matches += 1\n        elif outcome in ('mismatch', 'error') or (\n            outcome == 'match' and local_root_hash != recorded_root_hash\n        ):\n            num_mismatches += 1\n\n    if total < min_matching_verifiers:\n        # Not enough verifiers to make a strong decision.\n        status = 'PENDING' if num_mismatches == 0 else 'FAILED'\n    else:\n        match_ratio = num_matches / total if total > 0 else 0.0\n        if num_mismatches == 0 and num_matches > 0 and match_ratio >= min_matching_ratio:\n            status = 'VERIFIED'\n        elif num_matches == 0 and num_mismatches > 0:\n            status = 'FAILED'\n        else:\n            # Mixed or low-confidence results.\n            status = 'DISPUTED'\n\n    return {\n        'job_id': job_id,\n        'recorded_root_hash': recorded_root_hash,\n        'overall_status': status,\n        'number_of_verifiers': total,\n        'number_of_matches': num_matches,\n        'number_of_mismatches': num_mismatches,\n        'last_verified_at': last_verified_at,\n    }\n",
            "usage": "summary = compute_overall_verification_status(normalized_job_id, pome_proof.get('root_hash') if pome_proof else None, verifications)"
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "This backend helper assembles a final PoME status response object that can be returned by an API endpoint.",
            "code": "from typing import Any, Dict, Optional\n\n\ndef construct_pome_status_response(\n    job_id: str,\n    normalized_job_id: str,\n    job_metadata: Dict[str, Any],\n    pome_proof: Optional[Dict[str, Any]],\n    verification_summary: Dict[str, Any],\n) -> Dict[str, Any]:\n    # Assemble a stable response object for clients querying PoME status.\n    root_hash = None\n    proof_metadata: Optional[Dict[str, Any]] = None\n\n    if pome_proof:\n        root_hash = pome_proof.get('root_hash')\n        proof_metadata = {\n            'checkpoint_count': pome_proof.get('checkpoint_count'),\n            'proof_version': pome_proof.get('proof_version'),\n            'submission_block_height': pome_proof.get('submission_block_height'),\n            'submission_timestamp': pome_proof.get('submission_timestamp'),\n        }\n\n    response: Dict[str, Any] = {\n        'job_id': job_id,\n        'normalized_job_id': normalized_job_id,\n        'validator_id': job_metadata.get('validator_id'),\n        'client_id': job_metadata.get('client_id'),\n        'job_type': job_metadata.get('job_type'),\n        'assigned_time': job_metadata.get('assigned_time'),\n        'completion_time': job_metadata.get('completion_time'),\n        'job_status': job_metadata.get('status'),\n        'root_hash': root_hash,\n        'proof_metadata': proof_metadata,\n        'overall_verification_status': verification_summary.get('overall_status'),\n        'number_of_verifiers': verification_summary.get('number_of_verifiers', 0),\n        'number_of_matches': verification_summary.get('number_of_matches', 0),\n        'number_of_mismatches': verification_summary.get('number_of_mismatches', 0),\n        'last_verified_at': verification_summary.get('last_verified_at'),\n    }\n\n    # Optionally include links to related entities such as slashing or dispute records.\n    response['links'] = {\n        'job': f'/api/jobs/{normalized_job_id}',\n        'pome_proof': f'/api/pome/proofs/{normalized_job_id}',\n        'verifications': f'/api/pome/verifications/{normalized_job_id}',\n        'slashing_events': f'/api/slashing?job_id={normalized_job_id}',\n        'disputes': f'/api/disputes?job_id={normalized_job_id}',\n    }\n\n    return response\n",
            "usage": "response = construct_pome_status_response(raw_job_id, normalized_job_id, job_metadata, pome_proof, summary)"
        }
    ]
}