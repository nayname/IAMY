{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Step 1 defines a backend helper that interprets a caller\u2019s request (start_epoch, end_epoch, last_n) and returns a safe, bounded epoch range, using an indexer call to discover the latest epoch when needed.",
            "code": "from dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport httpx\n\n# Default number of epochs to return when no explicit range is provided\nDEFAULT_WINDOW = 20\n# Hard cap on how many epochs can be requested in one call to protect the backend\nMAX_WINDOW = 100\n\n\n@dataclass\nclass EpochRangeRequest:\n    \"\"\"Represents caller-supplied filters for selecting an epoch window.\n\n    All fields are optional; the helper will infer a sensible range:\n    - If last_n is set, we return the last_n most recent epochs (capped by MAX_WINDOW).\n    - If start_epoch/end_epoch are provided, we normalize and cap the window.\n    - If nothing is provided, we default to the most recent DEFAULT_WINDOW epochs.\n    \"\"\"\n\n    start_epoch: Optional[int] = None\n    end_epoch: Optional[int] = None\n    last_n: Optional[int] = None\n\n\nclass EpochRangeError(ValueError):\n    \"\"\"Raised when the requested epoch range is invalid.\"\"\"\n\n    pass\n\n\nasync def _fetch_latest_epoch_id(indexer_base_url: str) -> int:\n    \"\"\"Query the Republic indexer for the most recent epoch id.\n\n    This assumes a hypothetical endpoint:\n        GET {indexer_base_url}/epochs/latest\n\n    Adjust the URL/JSON parsing to match your actual indexer API.\n    \"\"\"\n\n    try:\n        async with httpx.AsyncClient(timeout=5.0) as client:\n            resp = await client.get(f\"{indexer_base_url}/epochs/latest\")\n            resp.raise_for_status()\n    except httpx.HTTPError as exc:\n        raise RuntimeError(f\"Failed to fetch latest epoch from indexer: {exc}\") from exc\n\n    data = resp.json()\n    try:\n        return int(data[\"epoch_id\"])\n    except (KeyError, TypeError, ValueError) as exc:\n        raise RuntimeError(f\"Indexer response missing valid 'epoch_id': {data}\") from exc\n\n\nasync def derive_epoch_range(\n    params: EpochRangeRequest,\n    indexer_base_url: str,\n) -> Tuple[int, int]:\n    \"\"\"Derive an inclusive [start_epoch, end_epoch] window from user-supplied parameters.\n\n    Rules\n    -----\n    - If last_n is provided: return the last_n most recent epochs (capped by MAX_WINDOW).\n    - If start_epoch/end_epoch are provided: normalize and cap window length to MAX_WINDOW.\n    - If nothing is provided: default to the most recent DEFAULT_WINDOW epochs.\n\n    All epoch ids are assumed to be non-negative integers.\n    \"\"\"\n\n    # Basic validation of input parameters\n    if params.last_n is not None and params.last_n <= 0:\n        raise EpochRangeError(\"last_n must be a positive integer.\")\n\n    if params.start_epoch is not None and params.start_epoch < 0:\n        raise EpochRangeError(\"start_epoch must be non-negative.\")\n\n    if params.end_epoch is not None and params.end_epoch < 0:\n        raise EpochRangeError(\"end_epoch must be non-negative.\")\n\n    # Case 1: explicit \"last N epochs\" request\n    if params.last_n is not None:\n        last_n = min(params.last_n, MAX_WINDOW)\n        latest_epoch = await _fetch_latest_epoch_id(indexer_base_url)\n        start_epoch = max(0, latest_epoch - last_n + 1)\n        end_epoch = latest_epoch\n        return start_epoch, end_epoch\n\n    # Case 2: explicit start/end bounds\n    if params.start_epoch is not None or params.end_epoch is not None:\n        # We may need latest_epoch to fill missing bounds\n        latest_epoch = await _fetch_latest_epoch_id(indexer_base_url)\n\n        start_epoch = params.start_epoch\n        end_epoch = params.end_epoch\n\n        if start_epoch is None and end_epoch is not None:\n            # Backfill start_epoch from end_epoch with a DEFAULT_WINDOW\n            start_epoch = max(0, end_epoch - DEFAULT_WINDOW + 1)\n        elif start_epoch is not None and end_epoch is None:\n            # Backfill end_epoch from start_epoch with a DEFAULT_WINDOW,\n            # but never go beyond the current latest_epoch.\n            tentative_end = start_epoch + DEFAULT_WINDOW - 1\n            end_epoch = min(latest_epoch, tentative_end)\n        elif start_epoch is None and end_epoch is None:\n            # Fallback safety (should not happen under the enclosing if)\n            start_epoch = max(0, latest_epoch - DEFAULT_WINDOW + 1)\n            end_epoch = latest_epoch\n\n        if end_epoch < start_epoch:\n            raise EpochRangeError(\n                f\"end_epoch ({end_epoch}) must be greater than or equal to start_epoch ({start_epoch}).\"\n            )\n\n        # Enforce MAX_WINDOW by shrinking the oldest part of the range\n        window_size = end_epoch - start_epoch + 1\n        if window_size > MAX_WINDOW:\n            start_epoch = end_epoch - MAX_WINDOW + 1\n\n        return start_epoch, end_epoch\n\n    # Case 3: nothing was specified -> default to most recent DEFAULT_WINDOW epochs\n    latest_epoch = await _fetch_latest_epoch_id(indexer_base_url)\n    last_n = min(DEFAULT_WINDOW, MAX_WINDOW)\n    start_epoch = max(0, latest_epoch - last_n + 1)\n    end_epoch = latest_epoch\n    return start_epoch, end_epoch\n",
            "usage": "from republic_epoch_range import EpochRangeRequest, derive_epoch_range\n\n# Example backend usage (e.g., inside a FastAPI route handler)\nparams = EpochRangeRequest(start_epoch=100, end_epoch=150)\nstart_epoch, end_epoch = await derive_epoch_range(\n    params,\n    indexer_base_url=\"https://indexer.republic.example\",\n)\n# Use start_epoch and end_epoch in subsequent backend calls."
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Step 2 implements a backend function that queries a Republic indexer or node for epoch metadata and checkpoint information for the requested range, using batched HTTP queries instead of per-epoch RPC calls.",
            "code": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Any, Dict, List\n\nimport httpx\n\n\n@dataclass\nclass EpochMetadata:\n    \"\"\"Minimal metadata for an epoch, including its checkpoint block and committee.\n\n    committee_validator_ids is the list of validator identifiers (addresses or operator IDs)\n    that form the consensus committee for this epoch. Detailed stake and quality\n    metrics will be fetched in a separate step.\n    \"\"\"\n\n    epoch_id: int\n    checkpoint_height: int\n    checkpoint_hash: str\n    checkpoint_time: datetime\n    committee_validator_ids: List[str]\n\n\nclass EpochQueryError(RuntimeError):\n    \"\"\"Raised when epoch metadata cannot be fetched from the indexer.\"\"\"\n\n    pass\n\n\nasync def get_epochs_with_checkpoints(\n    start_epoch: int,\n    end_epoch: int,\n    indexer_base_url: str,\n    page_size: int = 50,\n) -> List[EpochMetadata]:\n    \"\"\"Fetch epoch metadata and checkpoint information for a contiguous epoch range.\n\n    This function assumes a hypothetical Republic indexer API of the form:\n\n        GET {indexer_base_url}/epochs?start={start}&end={end}&include_committees=true\n\n    Expected JSON response shape (example):\n\n        {\n          \"epochs\": [\n            {\n              \"epoch_id\": 123,\n              \"checkpoint\": {\n                \"height\": 987654,\n                \"hash\": \"ABCDEF...\",\n                \"time\": \"2025-01-01T12:34:56Z\"\n              },\n              \"committee\": {\n                \"validator_ids\": [\"repval1...\", \"repval2...\", ...]\n              }\n            },\n            ...\n          ]\n        }\n\n    Adjust URLs and parsing logic to match your actual indexer schema.\n    \"\"\"\n\n    if start_epoch < 0 or end_epoch < 0:\n        raise ValueError(\"start_epoch and end_epoch must be non-negative.\")\n    if end_epoch < start_epoch:\n        raise ValueError(\"end_epoch must be greater than or equal to start_epoch.\")\n\n    epochs: List[EpochMetadata] = []\n\n    async with httpx.AsyncClient(timeout=10.0) as client:\n        current_start = start_epoch\n        while current_start <= end_epoch:\n            current_end = min(current_start + page_size - 1, end_epoch)\n            try:\n                resp = await client.get(\n                    f\"{indexer_base_url}/epochs\",\n                    params={\n                        \"start\": current_start,\n                        \"end\": current_end,\n                        \"include_committees\": \"true\",\n                    },\n                )\n                resp.raise_for_status()\n            except httpx.HTTPError as exc:\n                raise EpochQueryError(\n                    f\"Failed to fetch epochs {current_start}-{current_end}: {exc}\"\n                ) from exc\n\n            payload: Dict[str, Any] = resp.json()\n            raw_epochs = payload.get(\"epochs\", [])\n            for item in raw_epochs:\n                try:\n                    epoch_id = int(item[\"epoch_id\"])\n                    checkpoint = item[\"checkpoint\"]\n                    committee = item.get(\"committee\", {})\n\n                    checkpoint_height = int(checkpoint[\"height\"])\n                    checkpoint_hash = str(checkpoint[\"hash\"])\n                    checkpoint_time = datetime.fromisoformat(\n                        str(checkpoint[\"time\"]).replace(\"Z\", \"+00:00\")\n                    )\n\n                    validator_ids = [\n                        str(v_id) for v_id in committee.get(\"validator_ids\", [])\n                    ]\n\n                except (KeyError, TypeError, ValueError) as exc:\n                    raise EpochQueryError(\n                        f\"Malformed epoch record in indexer response: {item}\"\n                    ) from exc\n\n                epochs.append(\n                    EpochMetadata(\n                        epoch_id=epoch_id,\n                        checkpoint_height=checkpoint_height,\n                        checkpoint_hash=checkpoint_hash,\n                        checkpoint_time=checkpoint_time,\n                        committee_validator_ids=validator_ids,\n                    )\n                )\n\n            current_start = current_end + 1\n\n    # Optional sanity check: ensure we did not miss epochs in the requested range.\n    epoch_ids = {e.epoch_id for e in epochs}\n    missing = [eid for eid in range(start_epoch, end_epoch + 1) if eid not in epoch_ids]\n    if missing:\n        # Depending on your tolerance, you might instead log this and continue.\n        raise EpochQueryError(f\"Indexer did not return metadata for epochs: {missing}\")\n\n    return epochs\n",
            "usage": "from republic_epochs import get_epochs_with_checkpoints\n\nepochs = await get_epochs_with_checkpoints(\n    start_epoch=120,\n    end_epoch=140,\n    indexer_base_url=\"https://indexer.republic.example\",\n)\n# `epochs` is a list of EpochMetadata objects that include checkpoint info and committee members."
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Step 3 adds a backend function that, for the same epoch set, fetches committee stake distributions and optional reputation/compute quality metrics in a single batched request to the Republic indexer.",
            "code": "from dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\n\n\n@dataclass\nclass ValidatorStakeInfo:\n    \"\"\"Stake and optional quality metrics for a validator in a specific epoch.\"\"\"\n\n    validator_id: str\n    stake: int\n    voting_power: int\n    reputation_score: Optional[float] = None\n    throughput_score: Optional[float] = None\n    inference_score: Optional[float] = None\n    flops_score: Optional[float] = None\n\n\nclass StakeDistributionError(RuntimeError):\n    \"\"\"Raised when stake distribution cannot be fetched or parsed.\"\"\"\n\n    pass\n\n\nasync def get_stake_distribution_for_epochs(\n    epoch_ids: List[int],\n    indexer_base_url: str,\n) -> Dict[int, List[ValidatorStakeInfo]]:\n    \"\"\"Fetch stake distributions for multiple epochs in a single batched request.\n\n    This function assumes a hypothetical Republic indexer API of the form:\n\n        POST {indexer_base_url}/epochs/stake-distribution\n        body: { \"epoch_ids\": [123, 124, ...] }\n\n    Example response shape:\n\n        {\n          \"epochs\": [\n            {\n              \"epoch_id\": 123,\n              \"validators\": [\n                {\n                  \"validator_id\": \"repval1...\",\n                  \"stake\": \"1000000000\",\n                  \"voting_power\": \"100\",\n                  \"reputation_score\": 0.98,\n                  \"throughput_score\": 0.92,\n                  \"inference_score\": 0.94,\n                  \"flops_score\": 0.91\n                },\n                ...\n              ]\n            },\n            ...\n          ]\n        }\n\n    Adjust the endpoint path and JSON field names to match your actual indexer implementation.\n    \"\"\"\n\n    if not epoch_ids:\n        return {}\n\n    if any(eid < 0 for eid in epoch_ids):\n        raise ValueError(\"epoch_ids must be non-negative integers.\")\n\n    async with httpx.AsyncClient(timeout=15.0) as client:\n        try:\n            resp = await client.post(\n                f\"{indexer_base_url}/epochs/stake-distribution\",\n                json={\"epoch_ids\": epoch_ids},\n            )\n            resp.raise_for_status()\n        except httpx.HTTPError as exc:\n            raise StakeDistributionError(\n                f\"Failed to fetch stake distribution for epochs {epoch_ids}: {exc}\"\n            ) from exc\n\n    payload: Dict[str, Any] = resp.json()\n    raw_epochs = payload.get(\"epochs\", [])\n\n    result: Dict[int, List[ValidatorStakeInfo]] = {}\n\n    for epoch_record in raw_epochs:\n        try:\n            epoch_id = int(epoch_record[\"epoch_id\"])\n            validators_raw = epoch_record.get(\"validators\", [])\n        except (KeyError, TypeError, ValueError) as exc:\n            raise StakeDistributionError(\n                f\"Malformed epoch stake record in indexer response: {epoch_record}\"\n            ) from exc\n\n        validator_infos: List[ValidatorStakeInfo] = []\n        for v in validators_raw:\n            try:\n                validator_id = str(v[\"validator_id\"])\n                stake = int(v[\"stake\"])\n                voting_power = int(v.get(\"voting_power\", stake))\n                reputation_score = (\n                    float(v[\"reputation_score\"]) if \"reputation_score\" in v else None\n                )\n                throughput_score = (\n                    float(v[\"throughput_score\"]) if \"throughput_score\" in v else None\n                )\n                inference_score = (\n                    float(v[\"inference_score\"]) if \"inference_score\" in v else None\n                )\n                flops_score = (\n                    float(v[\"flops_score\"]) if \"flops_score\" in v else None\n                )\n            except (KeyError, TypeError, ValueError) as exc:\n                raise StakeDistributionError(\n                    f\"Malformed validator stake record: {v}\"\n                ) from exc\n\n            validator_infos.append(\n                ValidatorStakeInfo(\n                    validator_id=validator_id,\n                    stake=stake,\n                    voting_power=voting_power,\n                    reputation_score=reputation_score,\n                    throughput_score=throughput_score,\n                    inference_score=inference_score,\n                    flops_score=flops_score,\n                )\n            )\n\n        result[epoch_id] = validator_infos\n\n    return result\n",
            "usage": "from republic_stake import get_stake_distribution_for_epochs\n\nstake_map = await get_stake_distribution_for_epochs(\n    epoch_ids=[120, 121, 122],\n    indexer_base_url=\"https://indexer.republic.example\",\n)\n# stake_map is a dict: { epoch_id: List[ValidatorStakeInfo], ... }"
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Step 4 builds a helper that merges epoch checkpoint metadata with stake distributions into a chronological timeline, computing committee size, total stake, and each validator\u2019s stake share and quality metrics.",
            "code": "from decimal import Decimal, InvalidOperation\nfrom typing import Any, Dict, List\n\n# Type hints reference the classes defined in previous steps.\n# from republic_epochs import EpochMetadata\n# from republic_stake import ValidatorStakeInfo\n\n\ndef assemble_epoch_committee_stake_timeline(\n    epochs: List[\"EpochMetadata\"],\n    stake_distributions: Dict[int, List[\"ValidatorStakeInfo\"]],\n) -> List[Dict[str, Any]]:\n    \"\"\"Combine checkpoint metadata, committee composition, and stake distribution.\n\n    Parameters\n    ----------\n    epochs:\n        List of EpochMetadata objects returned by get_epochs_with_checkpoints.\n    stake_distributions:\n        Mapping of epoch_id -> List[ValidatorStakeInfo] from get_stake_distribution_for_epochs.\n\n    Returns\n    -------\n    List[dict]\n        Chronological timeline; each entry has the structure:\n\n            {\n              \"epoch_id\": int,\n              \"checkpoint_block_height\": int,\n              \"checkpoint_block_hash\": str,\n              \"checkpoint_time\": str,  # ISO 8601\n              \"committee_size\": int,\n              \"total_stake\": str,      # integer string, e.g. amount of REP staked\n              \"validators\": [\n                {\n                  \"validator_id\": str,\n                  \"stake\": str,         # integer string\n                  \"stake_share\": float, # 0.0 - 1.0\n                  \"voting_power\": int,\n                  \"reputation_score\": float | None,\n                  \"throughput_score\": float | None,\n                  \"inference_score\": float | None,\n                  \"flops_score\": float | None,\n                },\n                ...\n              ],\n            }\n    \"\"\"\n\n    # Sort epochs chronologically by epoch_id\n    epochs_sorted = sorted(epochs, key=lambda e: e.epoch_id)\n\n    timeline: List[Dict[str, Any]] = []\n\n    for epoch in epochs_sorted:\n        validators = stake_distributions.get(epoch.epoch_id, [])\n        total_stake_int = sum(v.stake for v in validators) if validators else 0\n\n        # Use Decimal for safer ratio computations; still return float shares for clients.\n        total_stake_decimal = Decimal(total_stake_int) if total_stake_int > 0 else Decimal(0)\n\n        validator_entries: List[Dict[str, Any]] = []\n        for v in validators:\n            if total_stake_decimal > 0:\n                try:\n                    stake_share = float(Decimal(v.stake) / total_stake_decimal)\n                except (InvalidOperation, ZeroDivisionError):\n                    stake_share = 0.0\n            else:\n                stake_share = 0.0\n\n            validator_entries.append(\n                {\n                  \"validator_id\": v.validator_id,\n                  \"stake\": str(v.stake),\n                  \"stake_share\": stake_share,\n                  \"voting_power\": v.voting_power,\n                  \"reputation_score\": v.reputation_score,\n                  \"throughput_score\": v.throughput_score,\n                  \"inference_score\": v.inference_score,\n                  \"flops_score\": v.flops_score,\n                }\n            )\n\n        timeline.append(\n            {\n              \"epoch_id\": epoch.epoch_id,\n              \"checkpoint_block_height\": epoch.checkpoint_height,\n              \"checkpoint_block_hash\": epoch.checkpoint_hash,\n              \"checkpoint_time\": epoch.checkpoint_time.isoformat(),\n              \"committee_size\": len(validator_entries),\n              \"total_stake\": str(total_stake_int),\n              \"validators\": validator_entries,\n            }\n        )\n\n    return timeline\n",
            "usage": "from republic_timeline import assemble_epoch_committee_stake_timeline\n\n# Assume epochs_metadata and stake_map were obtained from previous steps.\ntimeline = assemble_epoch_committee_stake_timeline(\n    epochs=epochs_metadata,\n    stake_distributions=stake_map,\n)\n# `timeline` is a list of epoch-level objects combining checkpoint and stake data."
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Step 5 formats the assembled timeline into a consumer-friendly response, supporting summary vs detailed views and basic pagination so a frontend can render epochs and committees efficiently.",
            "code": "from typing import Any, Dict, List\n\n\nclass PaginationError(ValueError):\n    \"\"\"Raised when pagination parameters are invalid.\"\"\"\n\n    pass\n\n\ndef format_epochs_overview_response(\n    timeline: List[Dict[str, Any]],\n    view: str = \"summary\",\n    offset: int = 0,\n    limit: int = 20,\n) -> Dict[str, Any]:\n    \"\"\"Format the aggregated epoch data into a compact, client-facing structure.\n\n    Parameters\n    ----------\n    timeline:\n        The output of assemble_epoch_committee_stake_timeline (list of dicts).\n    view:\n        \"summary\" for epoch-level aggregates only, or \"detailed\" to include\n        the full per-validator breakdown.\n    offset:\n        Zero-based index of the first epoch to include (for pagination).\n    limit:\n        Maximum number of epochs to return in this page.\n\n    Returns\n    -------\n    dict\n        {\n          \"view\": \"summary\" | \"detailed\",\n          \"pagination\": {\n            \"total_epochs\": int,\n            \"offset\": int,\n            \"limit\": int,\n            \"has_more\": bool,\n          },\n          \"epochs\": [...],  # summary or detailed epoch objects\n        }\n    \"\"\"\n\n    if offset < 0:\n        raise PaginationError(\"offset must be non-negative.\")\n    if limit <= 0:\n        raise PaginationError(\"limit must be a positive integer.\")\n\n    total_epochs = len(timeline)\n    start = offset\n    end = min(offset + limit, total_epochs)\n\n    # Gracefully handle offset beyond total size\n    if start >= total_epochs:\n        page_items: List[Dict[str, Any]] = []\n    else:\n        page_items = timeline[start:end]\n\n    view_normalized = view.lower()\n    if view_normalized not in (\"summary\", \"detailed\"):\n        raise ValueError(\"view must be either 'summary' or 'detailed'.\")\n\n    if view_normalized == \"summary\":\n        epochs_payload: List[Dict[str, Any]] = [\n            {\n              \"epoch_id\": item[\"epoch_id\"],\n              \"checkpoint_block_height\": item[\"checkpoint_block_height\"],\n              \"checkpoint_time\": item[\"checkpoint_time\"],\n              \"committee_size\": item[\"committee_size\"],\n              \"total_stake\": item[\"total_stake\"],\n            }\n            for item in page_items\n        ]\n    else:\n        # \"detailed\" view includes the full validator breakdown\n        epochs_payload = page_items\n\n    has_more = end < total_epochs\n\n    return {\n      \"view\": view_normalized,\n      \"pagination\": {\n        \"total_epochs\": total_epochs,\n        \"offset\": offset,\n        \"limit\": limit,\n        \"has_more\": has_more,\n      },\n      \"epochs\": epochs_payload,\n    }\n",
            "usage": "from republic_format import format_epochs_overview_response\n\n# Given a `timeline` produced by assemble_epoch_committee_stake_timeline:\nresponse_body = format_epochs_overview_response(\n    timeline=timeline,\n    view=\"summary\",   # or \"detailed\" for per-validator breakdown\n    offset=0,\n    limit=10,\n)\n# In a BFF-style backend, you would return `response_body` as JSON from your HTTP route,\n# allowing the frontend to call a single endpoint like `/api/republic/epochs`."
        }
    ]
}