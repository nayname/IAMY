{
    "label": "others",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Step 1 defines a backend helper to parse and normalize user-provided latency filters, applying sensible defaults when none are supplied.",
            "code": "from dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclass\nclass LatencyFilter:\n    start_time: datetime\n    end_time: datetime\n    job_types: Optional[List[str]] = None\n    validator_ids: Optional[List[str]] = None\n    include_only_successful: bool = True\n\n\ndef _parse_iso_datetime(value: Optional[str], field_name: str) -> Optional[datetime]:\n    # Helper to parse ISO8601 datetime strings; returns None if value is falsy.\n    if not value:\n        return None\n    try:\n        dt = datetime.fromisoformat(value)\n        # If no timezone info is present, assume UTC.\n        if dt.tzinfo is None:\n            dt = dt.replace(tzinfo=timezone.utc)\n        return dt\n    except (TypeError, ValueError) as exc:\n        raise ValueError(f'Invalid {field_name} value: {value}') from exc\n\n\ndef parse_latency_distribution_filters(raw_filters: Optional[Dict[str, Any]] = None) -> LatencyFilter:\n    # Parse user-provided filters for latency analysis.\n    # If no filters are provided, default to the last 24 hours and all validators.\n    now_utc = datetime.now(timezone.utc)\n\n    raw_filters = raw_filters or {}\n\n    # Parse time range; default to [now - 24h, now]\n    start_time = _parse_iso_datetime(raw_filters.get('start_time'), 'start_time')\n    end_time = _parse_iso_datetime(raw_filters.get('end_time'), 'end_time')\n\n    if end_time is None:\n        end_time = now_utc\n    if start_time is None:\n        start_time = end_time - timedelta(hours=24)\n\n    if start_time >= end_time:\n        raise ValueError('start_time must be earlier than end_time')\n\n    # job_types and validator_ids may come as list or comma-separated string\n    job_types = raw_filters.get('job_types')\n    if isinstance(job_types, str):\n        job_types = [j.strip() for j in job_types.split(',') if j.strip()]\n\n    validator_ids = raw_filters.get('validator_ids')\n    if isinstance(validator_ids, str):\n        validator_ids = [v.strip() for v in validator_ids.split(',') if v.strip()]\n\n    include_only_successful = bool(raw_filters.get('include_only_successful', True))\n\n    return LatencyFilter(\n        start_time=start_time,\n        end_time=end_time,\n        job_types=job_types or None,\n        validator_ids=validator_ids or None,\n        include_only_successful=include_only_successful,\n    )",
            "usage": "filters = parse_latency_distribution_filters(raw_filters_dict)"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Step 2 adds a backend abstraction for the Republic analytics/indexer and a function to fetch job events matching the parsed filters.",
            "code": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclass\nclass JobEvent:\n    job_id: str\n    validator_id: str\n    job_type: str\n    assignment_time: datetime\n    completion_time: Optional[datetime]\n    status: str\n\n\nclass JobAnalyticsClient:\n    # Thin abstraction over the Republic indexer or analytics DB.\n    # You are expected to implement the `fetch_jobs` method for your environment.\n    async def fetch_jobs(\n        self,\n        start_time: datetime,\n        end_time: datetime,\n        job_types: Optional[List[str]] = None,\n        validator_ids: Optional[List[str]] = None,\n        include_only_successful: bool = True,\n    ) -> List[Dict[str, Any]]:\n        raise NotImplementedError('fetch_jobs must be implemented for your backend')\n\n\nasync def fetch_job_assignment_and_completion_events(\n    db_client: JobAnalyticsClient,\n    filters: 'LatencyFilter',\n) -> List[JobEvent]:\n    # Fetch raw job rows from the analytics backend and map them into JobEvent objects.\n    try:\n        raw_rows = await db_client.fetch_jobs(\n            start_time=filters.start_time,\n            end_time=filters.end_time,\n            job_types=filters.job_types,\n            validator_ids=filters.validator_ids,\n            include_only_successful=filters.include_only_successful,\n        )\n    except Exception as exc:\n        # Wrap lower-level errors with a higher-level, user-friendly message.\n        raise RuntimeError(f'Failed to fetch job events from analytics backend: {exc}') from exc\n\n    events: List[JobEvent] = []\n    for row in raw_rows:\n        try:\n            events.append(\n                JobEvent(\n                    job_id=str(row['job_id']),\n                    validator_id=str(row['validator_id']),\n                    job_type=str(row.get('job_type', 'unknown')),\n                    assignment_time=row['assignment_time'],\n                    completion_time=row.get('completion_time'),\n                    status=str(row.get('status', 'unknown')),\n                )\n            )\n        except KeyError:\n            # Skip malformed rows but continue processing.\n            # In production, you would also log this condition.\n            continue\n\n    return events",
            "usage": "events = await fetch_job_assignment_and_completion_events(db_client, filters)"
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Step 3 computes per-job latency from assignment/completion timestamps and tracks jobs that cannot be used.",
            "code": "from dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Optional, Tuple\n\n\n@dataclass\nclass JobLatencySample:\n    job_id: str\n    validator_id: str\n    status: str\n    latency_seconds: float\n\n\n@dataclass\nclass SkippedJobSample:\n    job_id: str\n    validator_id: str\n    status: str\n    reason: str\n\n\ndef compute_per_job_latency(\n    events: List['JobEvent'],\n) -> Tuple[List[JobLatencySample], List[SkippedJobSample]]:\n    # For each job that has both assignment_time and completion_time, compute latency.\n    latencies: List[JobLatencySample] = []\n    skipped: List[SkippedJobSample] = []\n\n    for event in events:\n        assignment_time: Optional[datetime] = getattr(event, 'assignment_time', None)\n        completion_time: Optional[datetime] = getattr(event, 'completion_time', None)\n\n        if assignment_time is None:\n            skipped.append(\n                SkippedJobSample(\n                    job_id=event.job_id,\n                    validator_id=event.validator_id,\n                    status=event.status,\n                    reason='missing_assignment_time',\n                )\n            )\n            continue\n\n        if completion_time is None:\n            skipped.append(\n                SkippedJobSample(\n                    job_id=event.job_id,\n                    validator_id=event.validator_id,\n                    status=event.status,\n                    reason='missing_completion_time',\n                )\n            )\n            continue\n\n        try:\n            delta = completion_time - assignment_time\n            latency_seconds = float(delta.total_seconds())\n        except Exception as exc:\n            skipped.append(\n                SkippedJobSample(\n                    job_id=event.job_id,\n                    validator_id=event.validator_id,\n                    status=event.status,\n                    reason=f'error_computing_latency: {exc}',\n                )\n            )\n            continue\n\n        latencies.append(\n            JobLatencySample(\n                job_id=event.job_id,\n                validator_id=event.validator_id,\n                status=event.status,\n                latency_seconds=latency_seconds,\n            )\n        )\n\n    return latencies, skipped",
            "usage": "job_latencies, skipped_jobs = compute_per_job_latency(events)"
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Step 4 aggregates per-job latencies into global and per-validator statistics and builds histogram buckets.",
            "code": "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass BasicStats:\n    count: int\n    min: Optional[float]\n    max: Optional[float]\n    mean: Optional[float]\n    median: Optional[float]\n    p90: Optional[float]\n    p95: Optional[float]\n\n\n@dataclass\nclass HistogramBucket:\n    start: float  # inclusive\n    end: float    # exclusive, except for the last bucket\n    count: int\n\n\n@dataclass\nclass LatencyAggregates:\n    global_stats: BasicStats\n    per_validator: Dict[str, BasicStats]\n    histogram: List[HistogramBucket]\n\n\ndef _compute_percentile(sorted_values: List[float], percentile: float) -> Optional[float]:\n    if not sorted_values:\n        return None\n    if percentile <= 0:\n        return sorted_values[0]\n    if percentile >= 100:\n        return sorted_values[-1]\n\n    k = (percentile / 100.0) * (len(sorted_values) - 1)\n    lower_index = int(k)\n    upper_index = min(lower_index + 1, len(sorted_values) - 1)\n    weight = k - lower_index\n    return sorted_values[lower_index] * (1 - weight) + sorted_values[upper_index] * weight\n\n\ndef _compute_basic_stats(values: List[float]) -> BasicStats:\n    if not values:\n        return BasicStats(\n            count=0,\n            min=None,\n            max=None,\n            mean=None,\n            median=None,\n            p90=None,\n            p95=None,\n        )\n\n    sorted_values = sorted(values)\n    count = len(sorted_values)\n    min_v = sorted_values[0]\n    max_v = sorted_values[-1]\n    mean_v = sum(sorted_values) / float(count)\n    median_v = _compute_percentile(sorted_values, 50.0)\n    p90_v = _compute_percentile(sorted_values, 90.0)\n    p95_v = _compute_percentile(sorted_values, 95.0)\n\n    return BasicStats(\n        count=count,\n        min=min_v,\n        max=max_v,\n        mean=mean_v,\n        median=median_v,\n        p90=p90_v,\n        p95=p95_v,\n    )\n\n\ndef _build_histogram(values: List[float], bucket_count: int = 20) -> List[HistogramBucket]:\n    if not values or bucket_count <= 0:\n        return []\n\n    min_v = min(values)\n    max_v = max(values)\n\n    if min_v == max_v:\n        # All values are the same; create a single bucket around that value.\n        return [HistogramBucket(start=min_v, end=min_v + 1.0, count=len(values))]\n\n    bucket_width = (max_v - min_v) / float(bucket_count)\n    buckets = [HistogramBucket(start=min_v + i * bucket_width,\n                               end=min_v + (i + 1) * bucket_width,\n                               count=0) for i in range(bucket_count)]\n\n    for v in values:\n        # Determine bucket index, clamping to the last bucket if needed.\n        index = int((v - min_v) / bucket_width)\n        if index >= bucket_count:\n            index = bucket_count - 1\n        buckets[index].count += 1\n\n    return buckets\n\n\ndef aggregate_latency_statistics(\n    samples: List['JobLatencySample'],\n    bucket_count: int = 20,\n) -> LatencyAggregates:\n    # Aggregate per-job latencies into global and per-validator statistics.\n    latencies = [s.latency_seconds for s in samples]\n\n    global_stats = _compute_basic_stats(latencies)\n\n    per_validator_map: Dict[str, List[float]] = {}\n    for sample in samples:\n        per_validator_map.setdefault(sample.validator_id, []).append(sample.latency_seconds)\n\n    per_validator_stats: Dict[str, BasicStats] = {\n        validator_id: _compute_basic_stats(vals)\n        for validator_id, vals in per_validator_map.items()\n    }\n\n    histogram = _build_histogram(latencies, bucket_count=bucket_count)\n\n    return LatencyAggregates(\n        global_stats=global_stats,\n        per_validator=per_validator_stats,\n        histogram=histogram,\n    )",
            "usage": "aggregates = aggregate_latency_statistics(job_latencies, bucket_count=20)"
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Step 5 detects anomalous or outlier latencies, removes them from the dataset, and recomputes cleaned aggregates while recording data-quality issues.",
            "code": "from dataclasses import dataclass\nfrom typing import List, Tuple\n\n\n@dataclass\nclass OutlierInfo:\n    negative_latency_count: int\n    over_max_latency_count: int\n    max_latency_seconds: float\n    data_quality_issues: List[str]\n\n\ndef clean_and_clip_latency_outliers(\n    samples: List['JobLatencySample'],\n    max_latency_seconds: float = 3600.0,\n    bucket_count: int = 20,\n) -> Tuple['LatencyAggregates', OutlierInfo]:\n    # Detect anomalous latency values and recompute statistics without them.\n    if max_latency_seconds <= 0:\n        raise ValueError('max_latency_seconds must be positive')\n\n    clean_samples: List['JobLatencySample'] = []\n    negative_latency_count = 0\n    over_max_latency_count = 0\n\n    for sample in samples:\n        if sample.latency_seconds < 0:\n            negative_latency_count += 1\n            continue\n        if sample.latency_seconds > max_latency_seconds:\n            over_max_latency_count += 1\n            continue\n        clean_samples.append(sample)\n\n    data_quality_issues: List[str] = []\n    if negative_latency_count:\n        data_quality_issues.append(\n            f'Found {negative_latency_count} jobs with negative latency; excluded from statistics.'\n        )\n    if over_max_latency_count:\n        data_quality_issues.append(\n            f'Found {over_max_latency_count} jobs with latency above {max_latency_seconds} seconds; '\n            f'excluded from statistics and treated as outliers.'\n        )\n\n    cleaned_aggregates = aggregate_latency_statistics(clean_samples, bucket_count=bucket_count)\n\n    outlier_info = OutlierInfo(\n        negative_latency_count=negative_latency_count,\n        over_max_latency_count=over_max_latency_count,\n        max_latency_seconds=max_latency_seconds,\n        data_quality_issues=data_quality_issues,\n    )\n\n    return cleaned_aggregates, outlier_info",
            "usage": "cleaned_aggregates, outlier_info = clean_and_clip_latency_outliers(job_latencies, max_latency_seconds=3600.0)"
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "Step 6 orchestrates the full pipeline into a single backend action that returns a structured latency-distribution response suitable for an API endpoint.",
            "code": "from typing import Any, Dict, Optional\n\n\nasync def construct_latency_distribution_response(\n    db_client: 'JobAnalyticsClient',\n    raw_filters: Optional[Dict[str, Any]] = None,\n    max_latency_seconds: float = 3600.0,\n    bucket_count: int = 20,\n) -> Dict[str, Any]:\n    # Orchestrate the full latency distribution pipeline and build a response dict\n    # suitable for returning from an API endpoint.\n\n    # Step 1: parse filters\n    try:\n        filters = parse_latency_distribution_filters(raw_filters or {})\n    except ValueError as exc:\n        # In an API, you might translate this into a 400 Bad Request.\n        raise exc\n\n    # Step 2: fetch matching job events\n    events = await fetch_job_assignment_and_completion_events(db_client, filters)\n\n    # Step 3: compute per-job latency\n    job_latencies, skipped_jobs = compute_per_job_latency(events)\n\n    # Step 4 + 5: aggregate statistics and clean outliers\n    cleaned_aggregates, outlier_info = clean_and_clip_latency_outliers(\n        job_latencies,\n        max_latency_seconds=max_latency_seconds,\n        bucket_count=bucket_count,\n    )\n\n    # Helper to convert dataclass-based stats objects into plain dicts.\n    def _stats_to_dict(stats: 'BasicStats') -> Dict[str, Any]:\n        return {\n            'count': stats.count,\n            'min': stats.min,\n            'max': stats.max,\n            'mean': stats.mean,\n            'median': stats.median,\n            'p90': stats.p90,\n            'p95': stats.p95,\n        }\n\n    histogram = [\n        {\n            'start': bucket.start,\n            'end': bucket.end,\n            'count': bucket.count,\n        }\n        for bucket in cleaned_aggregates.histogram\n    ]\n\n    per_validator_stats = {\n        validator_id: _stats_to_dict(stats)\n        for validator_id, stats in cleaned_aggregates.per_validator.items()\n    }\n\n    response: Dict[str, Any] = {\n        'filters_used': {\n            'start_time': filters.start_time.isoformat(),\n            'end_time': filters.end_time.isoformat(),\n            'job_types': filters.job_types,\n            'validator_ids': filters.validator_ids,\n            'include_only_successful': filters.include_only_successful,\n            'max_latency_seconds': max_latency_seconds,\n            'bucket_count': bucket_count,\n        },\n        'total_jobs_matched': len(events),\n        'total_jobs_with_completion': len(job_latencies),\n        'total_jobs_without_completion': len(skipped_jobs),\n        'global_latency_stats': _stats_to_dict(cleaned_aggregates.global_stats),\n        'per_validator_stats': per_validator_stats,\n        'histogram': histogram,\n        'outliers': {\n            'negative_latency_count': outlier_info.negative_latency_count,\n            'over_max_latency_count': outlier_info.over_max_latency_count,\n            'max_latency_seconds': outlier_info.max_latency_seconds,\n            'data_quality_issues': outlier_info.data_quality_issues,\n        },\n        'skipped_jobs_summary': [\n            {\n                'job_id': s.job_id,\n                'validator_id': s.validator_id,\n                'status': s.status,\n                'reason': s.reason,\n            }\n            for s in skipped_jobs\n        ],\n    }\n\n    return response",
            "usage": "response_dict = await construct_latency_distribution_response(db_client, raw_filters=request_filters)"
        }
    ]
}