{
    "label": "query_contract",
    "workflow": [
        {
            "step": 1,
            "label": "backend",
            "introduction": "Validate and canonicalize a Republic job_id so it is safe to use in downstream PoME and checkpoint queries.",
            "code": "import re\nfrom typing import Final\n\n\nclass JobIdValidationError(ValueError):\n    '''Raised when a job_id is missing or does not conform to the expected Republic format.'''\n\n\n# Canonical Republic job_id pattern:\n# - optional 'job_' prefix\n# - 64 hex characters (deterministic job hash)\n_JOB_ID_REGEX: Final[re.Pattern] = re.compile(r'^(?:job_)?([0-9a-fA-F]{64})$')\n\n\ndef validate_and_normalize_job_id(job_id: str) -> str:\n    '''\n    Validate and canonicalize a Republic job_id.\n\n    Canonical format: 'job_<lowercase-64-hex>'.\n\n    :param job_id: Raw job identifier provided by the client.\n    :return: Canonical job_id string safe to use in backend queries.\n    :raises JobIdValidationError: if job_id is empty or malformed.\n    '''\n    if job_id is None:\n        raise JobIdValidationError('job_id is required.')\n\n    raw = job_id.strip()\n    if not raw:\n        raise JobIdValidationError('job_id must not be empty or whitespace.')\n\n    match = _JOB_ID_REGEX.match(raw)\n    if not match:\n        raise JobIdValidationError(\n            'job_id must be a 64-character hex string; optional prefix job_ is allowed.'\n        )\n\n    hex_part = match.group(1).lower()\n    canonical = f'job_{hex_part}'\n    return canonical\n",
            "usage": "from pome_workflow import validate_and_normalize_job_id, JobIdValidationError\n\ntry:\n    canonical_job_id = validate_and_normalize_job_id(raw_job_id)\nexcept JobIdValidationError as exc:\n    # Return 400 from your API or log the validation error\n    print(f'Invalid job_id: {exc}')\n"
        },
        {
            "step": 2,
            "label": "backend",
            "introduction": "Fetch PoME metadata for a given job_id from the Republic PoME/compute-proof registry, including root_hash, expected checkpoint count, and storage hints.",
            "code": "import os\nfrom typing import Any, Dict\n\nimport requests\n\n# Reuse validation from step 1 (adjust import to your project layout)\n# from pome_workflow import validate_and_normalize_job_id, JobIdValidationError\n\n\nclass PomeRegistryError(RuntimeError):\n    '''Generic error while talking to the PoME metadata registry. '''\n\n\nclass PomeMetadataNotFound(PomeRegistryError):\n    '''Raised when no PoME metadata exists for the given job_id. '''\n\n\nREGISTRY_BASE_URL = os.getenv(\n    'REPUBLIC_POME_REGISTRY_URL',\n    'https://pome-registry.republic.example.com',  # placeholder; configure for your deployment\n)\n\n\ndef fetch_pome_metadata_for_job(job_id: str) -> Dict[str, Any]:\n    '''\n    Retrieve PoME metadata for a Republic job from the registry service.\n\n    Expected response shape (example):\n        {\n            'job_id': 'job_<64hex>',\n            'root_hash': '<hex>',\n            'expected_checkpoint_count': 128,\n            'checkpoint_storage': {\n                'backend': 'on_chain' | 'indexer' | 'object_store' | None,\n                'location': { ... backend specific ... }\n            }\n        }\n\n    :param job_id: Raw or canonical job identifier.\n    :return: Dict with keys: job_id, root_hash, expected_checkpoint_count, checkpoint_storage.\n    :raises JobIdValidationError: if job_id is malformed.\n    :raises PomeMetadataNotFound: if the job is unknown to the registry.\n    :raises PomeRegistryError: on network / protocol / shape errors.\n    '''\n    canonical_job_id = validate_and_normalize_job_id(job_id)\n\n    url = f\"{REGISTRY_BASE_URL.rstrip('/')}/jobs/{canonical_job_id}/pome-metadata\"\n\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code == 404:\n            raise PomeMetadataNotFound(\n                f'No PoME metadata found for job_id={canonical_job_id}'\n            )\n        response.raise_for_status()\n        data = response.json()\n    except requests.RequestException as exc:\n        raise PomeRegistryError(\n            f'Failed to fetch PoME metadata for job_id={canonical_job_id}: {exc}'\n        ) from exc\n    except ValueError as exc:\n        # .json() failed\n        raise PomeRegistryError('Registry returned invalid JSON payload.') from exc\n\n    root_hash = data.get('root_hash')\n    expected_checkpoint_count = data.get('expected_checkpoint_count')\n    checkpoint_storage = data.get('checkpoint_storage')\n\n    if not isinstance(root_hash, str) or not root_hash:\n        raise PomeRegistryError(\"PoME metadata missing non-empty field 'root_hash'.\")\n\n    if not isinstance(expected_checkpoint_count, int) or expected_checkpoint_count < 0:\n        raise PomeRegistryError(\n            \"PoME metadata has invalid 'expected_checkpoint_count'; expected non-negative integer.\"\n        )\n\n    if checkpoint_storage is None:\n        # Represent absence of per-checkpoint data explicitly\n        checkpoint_storage = {'backend': None, 'location': {}}\n    elif not isinstance(checkpoint_storage, dict):\n        raise PomeRegistryError(\n            \"PoME metadata has malformed 'checkpoint_storage' field; expected an object.\"\n        )\n\n    return {\n        'job_id': canonical_job_id,\n        'root_hash': root_hash,\n        'expected_checkpoint_count': expected_checkpoint_count,\n        'checkpoint_storage': checkpoint_storage,\n    }\n",
            "usage": "from pome_workflow import fetch_pome_metadata_for_job, PomeRegistryError, PomeMetadataNotFound\n\ntry:\n    metadata = fetch_pome_metadata_for_job(canonical_job_id)\nexcept PomeMetadataNotFound:\n    # Return 404 from your API: no proof known for this job\n    metadata = None\nexcept PomeRegistryError as exc:\n    # Return 502/500 from your API\n    print(f'Error contacting PoME registry: {exc}')\n"
        },
        {
            "step": 3,
            "label": "backend",
            "introduction": "Resolve where checkpoint hashes are stored (on-chain, indexer, or object store) and construct a concrete fetch plan.",
            "code": "from typing import Any, Dict\nfrom urllib.parse import urlparse\n\n\nclass CheckpointStorageResolutionError(RuntimeError):\n    '''Raised when checkpoint storage metadata cannot be mapped to a concrete fetch plan. '''\n\n\ndef resolve_checkpoint_storage_location(pome_metadata: Dict[str, Any]) -> Dict[str, Any]:\n    '''\n    Interpret PoME checkpoint_storage metadata and produce a concrete fetch plan.\n\n    The function normalizes the storage description into a plan object that later\n    steps can execute without needing to understand registry internals.\n\n    Expected pome_metadata shape (relevant subset):\n        {\n            'job_id': 'job_<64hex>',\n            'checkpoint_storage': {\n                'backend': 'on_chain' | 'indexer' | 'object_store' | None,\n                'location': { ... backend specific ... }\n            }\n        }\n\n    Returns a dict like:\n        {\n            'backend': 'on_chain' | 'indexer' | 'object_store' | 'none',\n            'job_id': 'job_<64hex>',\n\n            # for backend == 'object_store' or 'indexer'\n            'url': 'https://.../checkpoints',\n\n            # for backend == 'on_chain'\n            'rest_url': 'https://rpc.republic.example.com',\n            'contract_address': 'republic1...',\n\n            'description': 'Human readable explanation.'\n        }\n\n    :raises CheckpointStorageResolutionError: if metadata is inconsistent.\n    '''\n    job_id = pome_metadata.get('job_id')\n    storage = pome_metadata.get('checkpoint_storage') or {}\n\n    backend = storage.get('backend')\n    location = storage.get('location') or {}\n\n    # No per-checkpoint material recorded: still return a plan so\n    # downstream code can handle it gracefully.\n    if backend in (None, 'none'):\n        return {\n            'backend': 'none',\n            'job_id': job_id,\n            'description': 'No checkpoint storage backend configured; only aggregate PoME may be available.',\n        }\n\n    if backend == 'object_store':\n        uri = location.get('uri')\n        if not uri or not isinstance(uri, str):\n            raise CheckpointStorageResolutionError(\n                \"object_store backend requires string field 'uri'.\"\n            )\n\n        parsed = urlparse(uri)\n        if parsed.scheme in ('http', 'https'):\n            url = uri\n        elif parsed.scheme == 'ipfs':\n            # Map ipfs://CID[/path] to a configurable HTTP gateway\n            gateway = location.get('gateway', 'https://ipfs.io/ipfs')\n            url = f\"{gateway.rstrip('/')}/{parsed.netloc}{parsed.path}\"\n        else:\n            raise CheckpointStorageResolutionError(\n                f'Unsupported object_store URI scheme: {parsed.scheme!r}'\n            )\n\n        return {\n            'backend': 'object_store',\n            'job_id': job_id,\n            'url': url,\n            'description': f'HTTP GET to object store for checkpoints of {job_id}.',\n        }\n\n    if backend == 'indexer':\n        base_url = location.get('base_url')\n        if not base_url or not isinstance(base_url, str):\n            raise CheckpointStorageResolutionError(\n                \"indexer backend requires string field 'base_url'.\"\n            )\n\n        table = location.get('table', 'checkpoints')\n        url = f\"{base_url.rstrip('/')}/jobs/{job_id}/{table}\"\n\n        return {\n            'backend': 'indexer',\n            'job_id': job_id,\n            'url': url,\n            'description': 'HTTP GET to Republic indexer for checkpoint hashes.',\n        }\n\n    if backend == 'on_chain':\n        rest_url = location.get('rest_url')\n        contract_address = location.get('contract_address')\n\n        if not rest_url or not isinstance(rest_url, str):\n            raise CheckpointStorageResolutionError(\n                \"on_chain backend requires string field 'rest_url'.\"\n            )\n        if not contract_address or not isinstance(contract_address, str):\n            raise CheckpointStorageResolutionError(\n                \"on_chain backend requires string field 'contract_address'.\"\n            )\n\n        return {\n            'backend': 'on_chain',\n            'job_id': job_id,\n            'rest_url': rest_url.rstrip('/'),\n            'contract_address': contract_address,\n            'description': 'CosmWasm smart query to PoME contract for checkpoint hashes.',\n        }\n\n    raise CheckpointStorageResolutionError(\n        f'Unsupported checkpoint storage backend: {backend!r}'\n    )\n",
            "usage": "from pome_workflow import resolve_checkpoint_storage_location, CheckpointStorageResolutionError\n\ntry:\n    fetch_plan = resolve_checkpoint_storage_location(metadata)\nexcept CheckpointStorageResolutionError as exc:\n    # Return 500 from your API or mark checkpoints as unavailable\n    print(f'Could not build checkpoint fetch plan: {exc}')\n"
        },
        {
            "step": 4,
            "label": "backend",
            "introduction": "Execute the previously resolved fetch plan to obtain the ordered list of checkpoint hashes for the job.",
            "code": "import base64\nimport json\nfrom typing import Any, Dict, List\n\nimport requests\n\n\nclass CheckpointFetchError(RuntimeError):\n    '''Raised when checkpoint hashes cannot be fetched or parsed correctly. '''\n\n\ndef fetch_checkpoint_hashes_for_job(fetch_plan: Dict[str, Any]) -> List[Dict[str, Any]]:\n    '''\n    Execute a checkpoint fetch plan and return an ordered list of checkpoints.\n\n    Each returned checkpoint has at least:\n        {\n            'checkpoint_index': int,\n            'hash': str,\n            'timestamp': Optional[str],\n            'segment_id': Optional[str]\n        }\n\n    :param fetch_plan: Output from resolve_checkpoint_storage_location.\n    :return: Sorted list of checkpoint records (may be empty).\n    :raises CheckpointFetchError: on network / protocol / shape errors.\n    '''\n    backend = fetch_plan.get('backend')\n    job_id = fetch_plan.get('job_id')\n\n    # If no backend is configured, simply return an empty list.\n    if backend == 'none':\n        return []\n\n    raw_list: Any\n\n    if backend in ('object_store', 'indexer'):\n        url = fetch_plan.get('url')\n        if not url or not isinstance(url, str):\n            raise CheckpointFetchError(\n                f\"Fetch plan for backend={backend} is missing a valid 'url'.\"\n            )\n        try:\n            response = requests.get(url, timeout=20)\n            response.raise_for_status()\n            payload = response.json()\n        except requests.RequestException as exc:\n            raise CheckpointFetchError(\n                f'Failed to fetch checkpoints from {backend} backend: {exc}'\n            ) from exc\n        except ValueError as exc:\n            raise CheckpointFetchError(\n                'Checkpoint backend returned invalid JSON payload.'\n            ) from exc\n\n        # Convention: backends expose checkpoints under a 'checkpoints' array.\n        raw_list = payload.get('checkpoints')\n\n    elif backend == 'on_chain':\n        rest_url = fetch_plan.get('rest_url')\n        contract_address = fetch_plan.get('contract_address')\n\n        if not rest_url or not contract_address:\n            raise CheckpointFetchError(\n                \"on_chain fetch plan must include 'rest_url' and 'contract_address'.\"\n            )\n\n        query_msg = {'get_checkpoints': {'job_id': job_id}}\n        try:\n            encoded_query = base64.b64encode(\n                json.dumps(query_msg).encode('utf-8')\n            ).decode('ascii')\n            url = f\"{rest_url}/cosmwasm/wasm/v1/contract/{contract_address}/smart/{encoded_query}\"\n\n            response = requests.get(url, timeout=20)\n            response.raise_for_status()\n            payload = response.json()\n        except requests.RequestException as exc:\n            raise CheckpointFetchError(\n                'Failed to query PoME contract for checkpoints: {exc}'\n            ) from exc\n        except ValueError as exc:\n            raise CheckpointFetchError(\n                'Chain REST endpoint returned invalid JSON payload.'\n            ) from exc\n\n        # Different node versions may wrap contract results slightly differently.\n        raw_list = (\n            payload.get('checkpoints')\n            or payload.get('data')\n            or (payload.get('result') or {}).get('checkpoints')\n        )\n\n    else:\n        raise CheckpointFetchError(\n            f'Unsupported checkpoint backend in fetch plan: {backend!r}'\n        )\n\n    if raw_list is None:\n        # Treat as no checkpoints available, rather than crashing.\n        return []\n\n    if not isinstance(raw_list, list):\n        raise CheckpointFetchError(\n            \"Backend response field 'checkpoints' must be a list.\"\n        )\n\n    checkpoints: List[Dict[str, Any]] = []\n    for item in raw_list:\n        if not isinstance(item, dict):\n            continue\n\n        # Accept both 'checkpoint_index' and short 'index' for flexibility.\n        idx = item.get('checkpoint_index', item.get('index'))\n        h = item.get('hash')\n        if idx is None or h is None:\n            # Skip malformed entries instead of failing the whole request.\n            continue\n\n        try:\n            idx_int = int(idx)\n        except (TypeError, ValueError):\n            continue\n\n        checkpoints.append(\n            {\n                'checkpoint_index': idx_int,\n                'hash': str(h),\n                'timestamp': item.get('timestamp'),\n                'segment_id': item.get('segment_id'),\n            }\n        )\n\n    # Ensure checkpoints are in ascending order by index.\n    checkpoints.sort(key=lambda c: c['checkpoint_index'])\n    return checkpoints\n",
            "usage": "from pome_workflow import fetch_checkpoint_hashes_for_job, CheckpointFetchError\n\ntry:\n    checkpoints = fetch_checkpoint_hashes_for_job(fetch_plan)\nexcept CheckpointFetchError as exc:\n    # Return 502/500 from your API or set verification status to error\n    print(f'Failed to fetch checkpoint hashes: {exc}')\n"
        },
        {
            "step": 5,
            "label": "backend",
            "introduction": "Recompute the aggregated root hash from checkpoint hashes using a HashedModel-style scheme and compare it against the stored root_hash.",
            "code": "import hashlib\nfrom typing import Any, Dict, List, Optional\n\n\nclass CheckpointVerificationError(RuntimeError):\n    '''Raised when checkpoint hashes cannot be normalized or verified. '''\n\n\ndef _normalize_hex_hash(value: str) -> str:\n    '''\n    Normalize a hex-encoded hash string:\n    - strip whitespace\n    - drop optional 0x prefix\n    - validate that it is valid hex\n    Returns lowercase hex without 0x.\n    '''\n    if not isinstance(value, str):\n        raise CheckpointVerificationError('Hash values must be strings.')\n\n    v = value.strip().lower()\n    if v.startswith('0x'):\n        v = v[2:]\n\n    if not v:\n        raise CheckpointVerificationError('Hash value must not be empty.')\n\n    if len(v) % 2 != 0:\n        raise CheckpointVerificationError(\n            f'Invalid hex hash length for value: {value!r}'\n        )\n\n    try:\n        bytes.fromhex(v)\n    except ValueError as exc:\n        raise CheckpointVerificationError(\n            f'Invalid hex hash contents: {value!r}'\n        ) from exc\n\n    return v\n\n\ndef aggregate_checkpoint_hashes(checkpoints: List[Dict[str, Any]]) -> Optional[str]:\n    '''\n    Aggregate a sequence of checkpoint hashes into a single root hash.\n\n    This uses a simple iterative SHA-256 combiner consistent with a HashedModel-style\n    proof-of-execution scheme:\n\n        agg_0 = H(hash_0)\n        agg_i = H(agg_{i-1} || hash_i) for i > 0\n\n    where hash_i is the raw checkpoint leaf hash in bytes.\n\n    Returns a hex-encoded root hash string, or None if no checkpoints exist.\n    '''\n    if not checkpoints:\n        return None\n\n    agg: Optional[bytes] = None\n\n    for cp in checkpoints:\n        leaf_raw = cp.get('hash')\n        if leaf_raw is None:\n            raise CheckpointVerificationError(\n                'Checkpoint record missing required field hash.'\n            )\n\n        leaf_hex = _normalize_hex_hash(str(leaf_raw))\n        leaf_bytes = bytes.fromhex(leaf_hex)\n\n        if agg is None:\n            agg = hashlib.sha256(leaf_bytes).digest()\n        else:\n            agg = hashlib.sha256(agg + leaf_bytes).digest()\n\n    return agg.hex() if agg is not None else None\n\n\ndef verify_checkpoint_hash_chain(\n    root_hash: str,\n    expected_checkpoint_count: int,\n    checkpoints: List[Dict[str, Any]],\n) -> Dict[str, Any]:\n    '''\n    Recompute the aggregated root hash from checkpoint hashes and compare it to\n    the stored PoME root_hash, also checking the expected vs. actual count.\n\n    :param root_hash: Root hash recorded in PoME metadata.\n    :param expected_checkpoint_count: Expected number of checkpoints from metadata.\n    :param checkpoints: Ordered list of checkpoint records.\n    :return: Dict with verification result and discrepancy details.\n    :raises CheckpointVerificationError: if inputs are inconsistent.\n    '''\n    if expected_checkpoint_count < 0:\n        raise CheckpointVerificationError(\n            'expected_checkpoint_count cannot be negative.'\n        )\n\n    normalized_expected_root = _normalize_hex_hash(root_hash)\n    computed_root = aggregate_checkpoint_hashes(checkpoints)\n\n    actual_count = len(checkpoints)\n    checkpoints_match_root = (\n        computed_root is not None and computed_root == normalized_expected_root\n    )\n\n    discrepancies: List[str] = []\n\n    if computed_root is None:\n        discrepancies.append('NO_CHECKPOINTS_AVAILABLE')\n    elif not checkpoints_match_root:\n        discrepancies.append('ROOT_HASH_MISMATCH')\n\n    if expected_checkpoint_count != actual_count:\n        discrepancies.append(\n            f'CHECKPOINT_COUNT_MISMATCH(expected={expected_checkpoint_count}, actual={actual_count})'\n        )\n\n    return {\n        'checkpoints_match_root': checkpoints_match_root,\n        'expected_root_hash': normalized_expected_root,\n        'computed_root_hash': computed_root,\n        'expected_checkpoint_count': expected_checkpoint_count,\n        'actual_checkpoint_count': actual_count,\n        'discrepancies': discrepancies,\n    }\n",
            "usage": "from pome_workflow import verify_checkpoint_hash_chain, CheckpointVerificationError\n\ntry:\n    verification = verify_checkpoint_hash_chain(\n        root_hash=metadata['root_hash'],\n        expected_checkpoint_count=metadata['expected_checkpoint_count'],\n        checkpoints=checkpoints,\n    )\nexcept CheckpointVerificationError as exc:\n    # Treat as verification failure; return 500 or mark proof as invalid\n    print(f'Checkpoint verification failed: {exc}')\n"
        },
        {
            "step": 6,
            "label": "backend",
            "introduction": "Assemble the final response object combining job_id, PoME metadata, ordered checkpoints, and a high-level verification result.",
            "code": "from typing import Any, Dict, List\n\n\ndef construct_checkpoint_hashes_response(\n    job_id: str,\n    pome_metadata: Dict[str, Any],\n    checkpoints: List[Dict[str, Any]],\n    verification_result: Dict[str, Any],\n) -> Dict[str, Any]:\n    '''\n    Build the final API response describing checkpoint hashes and verification status.\n\n    The response includes:\n        - job_id\n        - root_hash\n        - expected_checkpoint_count\n        - actual_checkpoint_count\n        - ordered list of {index, hash, ...}\n        - verification: {status, checkpoints_match_root, expected_root_hash, computed_root_hash, discrepancies}\n\n    Status codes:\n        - 'MATCHES_ROOT'           : checkpoints exist and recomputed root == stored root_hash\n        - 'MISMATCH_WITH_ROOT'     : checkpoints exist but recomputed root != stored root_hash\n        - 'CHECKPOINTS_NOT_AVAILABLE': no checkpoints were retrievable for this job\n    '''\n    root_hash = pome_metadata.get('root_hash')\n    expected_checkpoint_count = pome_metadata.get('expected_checkpoint_count', 0)\n    actual_checkpoint_count = len(checkpoints)\n\n    # Derive a high-level verification status from the detailed result.\n    if actual_checkpoint_count == 0:\n        verification_status = 'CHECKPOINTS_NOT_AVAILABLE'\n    elif not verification_result.get('checkpoints_match_root', False):\n        verification_status = 'MISMATCH_WITH_ROOT'\n    else:\n        verification_status = 'MATCHES_ROOT'\n\n    # Normalize checkpoint representation for the API consumer.\n    normalized_checkpoints: List[Dict[str, Any]] = [\n        {\n            'index': cp.get('checkpoint_index'),\n            'hash': cp.get('hash'),\n            'timestamp': cp.get('timestamp'),\n            'segment_id': cp.get('segment_id'),\n        }\n        for cp in checkpoints\n    ]\n\n    response: Dict[str, Any] = {\n        'job_id': job_id,\n        'root_hash': root_hash,\n        'expected_checkpoint_count': expected_checkpoint_count,\n        'actual_checkpoint_count': actual_checkpoint_count,\n        'checkpoints': normalized_checkpoints,\n        'verification': {\n            'status': verification_status,\n            'checkpoints_match_root': verification_result.get('checkpoints_match_root', False),\n            'expected_root_hash': verification_result.get('expected_root_hash'),\n            'computed_root_hash': verification_result.get('computed_root_hash'),\n            'discrepancies': verification_result.get('discrepancies', []),\n        },\n    }\n\n    return response\n",
            "usage": "from pome_workflow import (\n    construct_checkpoint_hashes_response,\n    validate_and_normalize_job_id,\n    fetch_pome_metadata_for_job,\n    resolve_checkpoint_storage_location,\n    fetch_checkpoint_hashes_for_job,\n    verify_checkpoint_hash_chain,\n)\n\n# End-to-end usage inside a backend handler (e.g., FastAPI route):\nraw_job_id = payload['job_id']\ncanonical_job_id = validate_and_normalize_job_id(raw_job_id)\nmetadata = fetch_pome_metadata_for_job(canonical_job_id)\nfetch_plan = resolve_checkpoint_storage_location(metadata)\ncheckpoints = fetch_checkpoint_hashes_for_job(fetch_plan)\nverification = verify_checkpoint_hash_chain(\n    root_hash=metadata['root_hash'],\n    expected_checkpoint_count=metadata['expected_checkpoint_count'],\n    checkpoints=checkpoints,\n)\nresponse_body = construct_checkpoint_hashes_response(\n    job_id=canonical_job_id,\n    pome_metadata=metadata,\n    checkpoints=checkpoints,\n    verification_result=verification,\n)\n# Return response_body as JSON to the frontend.\n"
        }
    ]
}