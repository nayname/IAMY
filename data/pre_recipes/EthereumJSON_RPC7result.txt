{
    "messages": [
        {
            "content": "[{\"label\": \"EthereumJSON_RPC\", \"query\": \"Write a CPU profile to cpu.prof for 30 seconds\"}, {\"label\": \"EthereumJSON_RPC\", \"query\": \"Write a block profile to block.prof for 30 seconds\"}, {\"label\": \"EthereumJSON_RPC\", \"query\": \"Start indefinite CPU profiling to cpu.prof\"}, {\"label\": \"EthereumJSON_RPC\", \"query\": \"Stop the current CPU profile\"}, {\"label\": \"EthereumJSON_RPC\", \"query\": \"Set garbage collection percentage to 100\"}]",
            "additional_kwargs": {},
            "response_metadata": {},
            "type": "human",
            "name": null,
            "id": "0f5371cd-12dc-4478-a131-bfd0111b6491",
            "example": false
        },
        {
            "content": "",
            "additional_kwargs": {
                "tool_calls": [
                    {
                        "id": "call_cOYl9CuzVBe7rKwdat9qUzgb",
                        "function": {
                            "arguments": "{\"query\": \"pprof profile Cosmos SDK node cpu profile 30 seconds\"}",
                            "name": "SearchCosmosDocumentation"
                        },
                        "type": "function"
                    }
                ],
                "refusal": null
            },
            "response_metadata": {
                "token_usage": {
                    "completion_tokens": 484,
                    "prompt_tokens": 705,
                    "total_tokens": 1189,
                    "completion_tokens_details": {
                        "accepted_prediction_tokens": 0,
                        "audio_tokens": 0,
                        "reasoning_tokens": 448,
                        "rejected_prediction_tokens": 0
                    },
                    "prompt_tokens_details": {
                        "audio_tokens": 0,
                        "cached_tokens": 0
                    }
                },
                "model_name": "o3-2025-04-16",
                "system_fingerprint": null,
                "id": "chatcmpl-CPvtI1unP4TK6YzVY4Vn7QO1HG9aX",
                "service_tier": "default",
                "finish_reason": "tool_calls",
                "logprobs": null
            },
            "type": "ai",
            "name": null,
            "id": "run--8fc2c65d-ba70-4a32-8e9a-453cf345de56-0",
            "example": false,
            "tool_calls": [
                {
                    "name": "SearchCosmosDocumentation",
                    "args": {
                        "query": "pprof profile Cosmos SDK node cpu profile 30 seconds"
                    },
                    "id": "call_cOYl9CuzVBe7rKwdat9qUzgb",
                    "type": "tool_call"
                }
            ],
            "invalid_tool_calls": [],
            "usage_metadata": {
                "input_tokens": 705,
                "output_tokens": 484,
                "total_tokens": 1189,
                "input_token_details": {
                    "audio": 0,
                    "cache_read": 0
                },
                "output_token_details": {
                    "audio": 0,
                    "reasoning": 448
                }
            }
        },
        {
            "content": "[\"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: flowchart TD\\n    A[Transaction relayed from the full-node's CometBFT engine to the node's application via DeliverTx] --> B[APPLICATION]\\n    B -->|\\\"Using baseapp's methods: Decode the Tx, extract and route the message(s)\\\"| C[Message routed to the correct module to be processed]\\n    C --> D1[AUTH MODULE]\\n    C --> D2[BANK MODULE]\\n    C --> D3[STAKING MODULE]\\n    C --> D4[GOV MODULE]\\n    D1 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n    D2 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n    D3 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n    D4 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n\\n\", \"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: Each module can be seen as a little state-machine. Developers need to define the subset of the state handled by the module, as well as custom message types that modify the state ( Note: messages are extracted from transactions by baseapp ). In general, each module declares its own KVStore in the multistore to persist the subset of the state it defines. Most developers will need to access other 3rd party modules when building their own modules. Given that the Cosmos SDK is an open framework, some of the modules may be malicious, which means there is a need for security principles to reason about inter-module interactions. These principles are based on object-capabilities . In practice, this means that instead of having each module keep an access control list for other modules, each module implements special objects called keepers that can be passed to other modules to grant a pre-defined set of capabilities. Cosmos SDK modules are defined in the x/ folder of the Cosmos SDK. Some core modules include: x/auth : Used to manage accounts and signatures. x/bank : Used to enable tokens and token transfers. x/staking + x/slashing : Used to build Proof-Of-Stake blockchains. In addition to the already existing modules in x/ , that anyone can use in their app, the Cosmos SDK lets you build your own custom modules. You can check an example of that in the tutorial.\\n\\n\", \"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: Each module can be seen as a little state-machine. Developers need to define the subset of the state handled by the module, as well as custom message types that modify the state ( Note: messages are extracted from transactions by baseapp ). In general, each module declares its own KVStore in the multistore to persist the subset of the state it defines. Most developers will need to access other 3rd party modules when building their own modules. Given that the Cosmos SDK is an open framework, some of the modules may be malicious, which means there is a need for security principles to reason about inter-module interactions. These principles are based on object-capabilities . In practice, this means that instead of having each module keep an access control list for other modules, each module implements special objects called keepers that can be passed to other modules to grant a pre-defined set of capabilities. Cosmos SDK modules are defined in the x/ folder of the Cosmos SDK. Some core modules include: x/auth : Used to manage accounts and signatures. x/bank : Used to enable tokens and token transfers. x/staking + x/slashing : Used to build Proof-Of-Stake blockchains. In addition to the already existing modules in x/ , that anyone can use in their app, the Cosmos SDK lets you build your own custom modules. You can check an example of that in the tutorial.\\n\\n\", \"Title: What is the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/intro/overview\\nContent: The Cosmos SDK is an open-source framework for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains. We envision the Cosmos SDK as the npm-like framework to build secure blockchain applications on top of CometBFT . SDK-based blockchains are built out of composable modules , most of which are open-source and readily available for any developers to use. Anyone can create a module for the Cosmos SDK, and integrating already-built modules is as simple as importing them into your blockchain application. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model.\\n\\nWhat is the Cosmos SDK\\nThe Cosmos SDK is an open-source framework for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains. We envision the Cosmos SDK as the npm-like framework to build secure blockchain applications on top of CometBFT . SDK-based blockchains are built out of composable modules , most of which are open-source and readily available for any developers to use. Anyone can create a module for the Cosmos SDK, and integrating already-built modules is as simple as importing them into your blockchain application. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model.\\n\\nWhy the Cosmos SDK\\nThe Cosmos SDK is the most advanced framework for building custom application-specific blockchains today. Here are a few reasons why you might want to consider building your decentralized application with the Cosmos SDK: The default consensus engine available within the Cosmos SDK is CometBFT . CometBFT is the most (and only) mature BFT consensus engine in existence. It is widely used across the industry and is considered the gold standard consensus engine for building Proof-of-Stake systems. The Cosmos SDK is open-source and designed to make it easy to build blockchains out of composable modules . As the ecosystem of open-source Cosmos SDK modules grows, it will become increasingly easier to build complex decentralized platforms with it. The Cosmos SDK is inspired by capabilities-based security, and informed by years of wrestling with blockchain state-machines. This makes the Cosmos SDK a very secure environment to build blockchains. Most importantly, the Cosmos SDK has already been used to build many application-specific blockchains that are already in production. Among others, we can cite Cosmos Hub , IRIS Hub , Binance Chain , Terra or Kava . Many more are building on the Cosmos SDK.\\n\\n\", \"Title: What is the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/intro/overview\\nContent: The Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhat is the Cosmos SDK\\nThe Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhy the Cosmos SDK\\nThe Cosmos SDK is the most advanced framework for building custom modular application-specific blockchains today. Here are a few reasons why you might want to consider building your decentralized application with the Cosmos SDK: It allows you to plug and play and customize your consensus layer. As above you can use Rollkit and Celestia as your consensus and data availability layer. This offers a lot of flexibility and customisation. Previously the default consensus engine available within the Cosmos SDK is CometBFT . CometBFT is the most (and only) mature BFT consensus engine in existence. It is widely used across the industry and is considered the gold standard consensus engine for building Proof-of-Stake systems. The Cosmos SDK is open-source and designed to make it easy to build blockchains out of composable modules . As the ecosystem of open-source Cosmos SDK modules grows, it will become increasingly easier to build complex decentralized platforms with it. The Cosmos SDK is inspired by capabilities-based security, and informed by years of wrestling with blockchain state-machines. This makes the Cosmos SDK a very secure environment to build blockchains. Most importantly, the Cosmos SDK has already been used to build many application-specific blockchains that are already in production. Among others, we can cite Cosmos Hub , IRIS Hub , Binance Chain , Terra or Kava . Many more are building on the Cosmos SDK.\\n\\n\", \"Title: What is the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/intro/overview\\nContent: The Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhat is the Cosmos SDK\\nThe Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhy the Cosmos SDK\\nThe Cosmos SDK is the most advanced framework for building custom modular application-specific blockchains today. Here are a few reasons why you might want to consider building your decentralized application with the Cosmos SDK: It allows you to plug and play and customize your consensus layer. As above you can use Rollkit and Celestia as your consensus and data availability layer. This offers a lot of flexibility and customisation. Previously the default consensus engine available within the Cosmos SDK is CometBFT . CometBFT is the most (and only) mature BFT consensus engine in existence. It is widely used across the industry and is considered the gold standard consensus engine for building Proof-of-Stake systems. The Cosmos SDK is open-source and designed to make it easy to build blockchains out of composable modules . As the ecosystem of open-source Cosmos SDK modules grows, it will become increasingly easier to build complex decentralized platforms with it. The Cosmos SDK is inspired by capabilities-based security, and informed by years of wrestling with blockchain state-machines. This makes the Cosmos SDK a very secure environment to build blockchains. Most importantly, the Cosmos SDK has already been used to build many application-specific blockchains that are already in production. Among others, we can cite Cosmos Hub , IRIS Hub , Binance Chain , Terra or Kava . Many more are building on the Cosmos SDK.\\n\\n\", \"Title: Node Client (Daemon)\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/advanced/node\\nContent: Synopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nNode Client (Daemon)\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nstart command\\nThe start command is defined in the /server folder of the Cosmos SDK. It is added to the root command of the full-node client in the main function and called by the end-user to start their node: As a reminder, the full-node is composed of three conceptual layers: the networking layer, the consensus layer and the application layer. The first two are generally bundled together in an entity called the consensus engine (CometBFT by default), while the third is the state-machine defined with the help of the Cosmos SDK. Currently, the Cosmos SDK uses CometBFT as the default consensus engine, meaning the start command is implemented to boot up a CometBFT node. The flow of the start command is pretty straightforward. First, it retrieves the config from the context in order to open the db (a leveldb instance by default). This db contains the latest known state of the application (empty if the application is started from the first time. With the db , the start command creates a new instance of the application using an appCreator function: Note that an appCreator is a function that fulfills the AppCreator signature: In practice, the constructor of the application is passed as the appCreator. Then, the instance of app is used to instantiate a new CometBFT node: The CometBFT node can be created with app because the latter satisfies the abci.Application interface (given that app extends baseapp ). As part of the node.New method, CometBFT makes sure that the height of the application (i.e. number of blocks since genesis) is equal to the height of the CometBFT node. The difference between these two heights should always be negative or null. If it is strictly negative, node.New will replay blocks until the height of the application reaches the height of the CometBFT node. Finally, if the height of the application is 0 , the CometBFT node will call InitChain on the application to initialize the state from the genesis file. Once the CometBFT node is instantiated and in sync with the application, the node can be started: package server\\n\\nimport (\\n    \\n\\t\\\"bufio\\\"\\n    \\\"context\\\"\\n    \\\"fmt\\\"\\n    \\\"io\\\"\\n    \\\"net\\\"\\n    \\\"os\\\"\\n    \\\"path/filepath\\\"\\n    \\\"runtime/pprof\\\"\\n    \\\"strings\\\"\\n    \\\"time\\\"\\n    \\\"github.com/cometbft/cometbft/abci/server\\\"\\n\\tcmtcmd \\\"github.com/cometbft/cometbft/cmd/cometbft/commands\\\"\\n\\tcmtcfg \\\"github.com/cometbft/cometbft/config\\\"\\n\\tcmtjson \\\"github.com/cometbft/cometbft/libs/json\\\"\\n    \\\"github.com/cometbft/cometbft/node\\\"\\n    \\\"github.com/cometbft/cometbft/p2p\\\"\\n\\tpvm \\\"github.com/cometbft/cometbft/privval\\\"\\n\\tcmtstate \\\"github.com/cometbft/cometbft/proto/tendermint/state\\\"\\n\\tcmtproto \\\"github.com/cometbft/cometbft/proto/tendermint/types\\\"\\n    \\\"github.com/cometbft/cometbft/proxy\\\"\\n\\trpchttp \\\"github.com/cometbft/cometbft/rpc/client/http\\\"\\n    \\\"github.com/cometbft/cometbft/rpc/client/local\\\"\\n\\tsm \\\"github.com/cometbft/cometbft/state\\\"\\n    \\\"github.com/cometbft/cometbft/store\\\"\\n\\tcmttypes \\\"github.com/cometbft/cometbft/types\\\"\\n\\tdbm \\\"github.com/cosmos/cosmos-db\\\"\\n    \\\"github.com/hashicorp/go-metrics\\\"\\n    \\\"github.com/spf13/cobra\\\"\\n    \\\"github.com/spf13/pflag\\\"\\n    \\\"golang.org/x/sync/errgroup\\\"\\n    \\\"google.golang.org/grpc\\\"\\n    \\\"google.golang.org/grpc/credentials/insecure\\\"\\n\\n\\tpruningtypes \\\"cosmossdk.io/store/pruning/types\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client/flags\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/codec\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/api\\\"\\n\\tserverconfig \\\"github.com/cosmos/cosmos-sdk/server/config\\\"\\n\\tservergrpc \\\"github.com/cosmos/cosmos-sdk/server/grpc\\\"\\n\\tservercmtlog \\\"github.com/cosmos/cosmos-sdk/server/log\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/types\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/telemetry\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/types/mempool\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/version\\\"\\n\\tgenutiltypes \\\"github.com/cosmos/cosmos-sdk/x/genutil/types\\\"\\n)\\n\\nconst (\\n\\t// CometBFT full-node start flags\\n\\tflagWithComet          = \\\"with-comet\\\"\\n\\tflagAddress            = \\\"address\\\"\\n\\tflagTransport          = \\\"transport\\\"\\n\\tflagTraceStore         = \\\"trace-store\\\"\\n\\tflagCPUProfile         = \\\"cpu-profile\\\"\\n\\tFlagMinGasPrices       = \\\"minimum-gas-prices\\\"\\n\\tFlagQueryGasLimit      = \\\"query-gas-limit\\\"\\n\\tFlagHaltHeight         = \\\"halt-height\\\"\\n\\tFlagHaltTime           = \\\"halt-time\\\"\\n\\tFlagInterBlockCache    = \\\"inter-block-cache\\\"\\n\\tFlagUnsafeSkipUpgrades = \\\"unsafe-skip-upgrades\\\"\\n\\tFlagTrace              = \\\"trace\\\"\\n\\tFlagInvCheckPeriod     = \\\"inv-check-period\\\"\\n\\n\\tFlagPruning             = \\\"pruning\\\"\\n\\tFlagPruningKeepRecent   = \\\"pruning-keep-recent\\\"\\n\\tFlagPruningInterval     = \\\"pruning-interval\\\"\\n\\tFlagIndexEvents         = \\\"index-events\\\"\\n\\tFlagMinRetainBlocks     = \\\"min-retain-blocks\\\"\\n\\tFlagIAVLCacheSize       = \\\"iavl-cache-size\\\"\\n\\tFlagDisableIAVLFastNode = \\\"iavl-disable-fastnode\\\"\\n\\tFlagIAVLSyncPruning     = \\\"iavl-sync-pruning\\\"\\n\\tFlagShutdownGrace       = \\\"shutdown-grace\\\"\\n\\n\\t// state sync-related flags\\n\\tFlagStateSyncSnapshotInterval   = \\\"state-sync.snapshot-interval\\\"\\n\\tFlagStateSyncSnapshotKeepRecent = \\\"state-sync.snapshot-keep-recent\\\"\\n\\n\\t// api-related flags\\n\\tFlagAPIEnable             = \\\"api.enable\\\"\\n\\tFlagAPISwagger            = \\\"api.swagger\\\"\\n\\tFlagAPIAddress            = \\\"api.address\\\"\\n\\tFlagAPIMaxOpenConnections = \\\"api.max-open-connections\\\"\\n\\tFlagRPCReadTimeout        = \\\"api.rpc-read-timeout\\\"\\n\\tFlagRPCWriteTimeout       = \\\"api.rpc-write-timeout\\\"\\n\\tFlagRPCMaxBodyBytes       = \\\"api.rpc-max-body-bytes\\\"\\n\\tFlagAPIEnableUnsafeCORS   = \\\"api.enabled-unsafe-cors\\\"\\n\\n\\t// gRPC-related flags\\n\\tflagGRPCOnly            = \\\"grpc-only\\\"\\n\\tflagGRPCEnable          = \\\"grpc.enable\\\"\\n\\tflagGRPCAddress         = \\\"grpc.address\\\"\\n\\tflagGRPCWebEnable       = \\\"grpc-web.enable\\\"\\n\\tflagGRPCSkipCheckHeader = \\\"grpc.skip-check-header\\\"\\n\\n\\t// mempool flags\\n\\tFlagMempoolMaxTxs = \\\"mempool.max-txs\\\"\\n\\n\\t// testnet keys\\n\\tKeyIsTestnet             = \\\"is-testnet\\\"\\n\\tKeyNewChainID            = \\\"new-chain-ID\\\"\\n\\tKeyNewOpAddr             = \\\"new-operator-addr\\\"\\n\\tKeyNewValAddr            = \\\"new-validator-addr\\\"\\n\\tKeyUserPubKey            = \\\"user-pub-key\\\"\\n\\tKeyTriggerTestnetUpgrade = \\\"trigger-testnet-upgrade\\\"\\n)\\n\\n// StartCmdOptions defines options that can be customized in `StartCmdWithOptions`,\\ntype StartCmdOptions struct {\\n\\t// DBOpener can be used to customize db opening, for example customize db options or support different db backends,\\n\\t// default to the builtin db opener.\\n\\tDBOpener func(rootDir string, backendType dbm.BackendType) (dbm.DB, error)\\n\\t// PostSetup can be used to setup extra services under the same cancellable context,\\n\\t// it's not called in stand-alone mode, only for in-process mode.\\n\\tPostSetup func(svrCtx *Context, clientCtx client.Context, ctx context.Context, g *errgroup.Group)\\n\\nerror\\n\\t// PostSetupStandalone can be used to setup extra services under the same cancellable context,\\n\\tPostSetupStandalone func(svrCtx *Context, clientCtx client.Context, ctx context.Context, g *errgroup.Group)\\n\\nerror\\n\\t// AddFlags add custom flags to start cmd\\n\\tAddFlags func(cmd *cobra.Command)\\n\\t// StartCommandHanlder can be used to customize the start command handler\\n\\tStartCommandHandler func(svrCtx *Context, clientCtx client.Context, appCreator types.AppCreator, inProcessConsensus bool, opts StartCmdOptions)\\n\\nerror\\n}\\n\\n// StartCmd runs the service passed in, either stand-alone or in-process with\\n// CometBFT.\\nfunc StartCmd(appCreator types.AppCreator, defaultNodeHome string) *cobra.Command {\\n    return StartCmdWithOptions(appCreator, defaultNodeHome, StartCmdOptions{\\n})\\n}\\n\\n// StartCmdWithOptions runs the service passed in, either stand-alone or in-process with\\n// CometBFT.\\nfunc StartCmdWithOptions(appCreator types.AppCreator, defaultNodeHome string, opts StartCmdOptions) *cobra.Command {\\n    if opts.DBOpener == nil {\\n    opts.DBOpener = openDB\\n}\\n    if opts.StartCommandHandler == nil {\\n    opts.StartCommandHandler = start\\n}\\n    cmd := &cobra.Command{\\n    Use:   \\\"start\\\",\\n    Short: \\\"Run the full node\\\",\\n    Long: `Run the full node application with CometBFT in or out of process. By\\ndefault, the application will run with CometBFT in process.\\n\\nPruning options can be provided via the '--pruning' flag or alternatively with '--pruning-keep-recent', and\\n'pruning-interval' together.\\n\\nFor '--pruning' the options are as follows:\\n\\ndefault: the last 362880 states are kept, pruning at 10 block intervals\\nnothing: all historic states will be saved, nothing will be deleted (i.e. archiving node)\\n\\neverything: 2 latest states will be kept; pruning at 10 block intervals.\\ncustom: allow pruning options to be manually specified through 'pruning-keep-recent', and 'pruning-interval'\\n\\nNode halting configurations exist in the form of two flags: '--halt-height' and '--halt-time'. During\\nthe ABCI Commit phase, the node will check if the current block height is greater than or equal to\\nthe halt-height or if the current block time is greater than or equal to the halt-time. If so, the\\nnode will attempt to gracefully shutdown and the block will not be committed. In addition, the node\\nwill not be able to commit subsequent blocks.\\n\\nFor profiling and benchmarking purposes, CPU profiling can be enabled via the '--cpu-profile' flag\\nwhich accepts a path for the resulting pprof file.\\n\\nThe node may be started in a 'query only' mode where only the gRPC and JSON HTTP\\nAPI services are enabled via the 'grpc-only' flag. In this mode, CometBFT is\\nbypassed and can be used when legacy queries are needed after an on-chain upgrade\\nis performed. Note, when enabled, gRPC will also be automatically enabled.\\n`,\\n    RunE: func(cmd *cobra.Command, _ []string)\\n\\nerror {\\n    serverCtx := GetServerContextFromCmd(cmd)\\n\\n\\t\\t\\t_, err := GetPruningOptionsFromFlags(serverCtx.Viper)\\n    if err != nil {\\n    return err\\n}\\n\\nclientCtx, err := client.GetClientQueryContext(cmd)\\n    if err != nil {\\n    return err\\n}\\n\\nwithCMT, _ := cmd.Flags().GetBool(flagWithComet)\\n    if !withCMT {\\n    serverCtx.Logger.Info(\\\"starting ABCI without CometBFT\\\")\\n}\\n\\nerr = wrapCPUProfile(serverCtx, func()\\n\\nerror {\\n    return opts.StartCommandHandler(serverCtx, clientCtx, appCreator, withCMT, opts)\\n})\\n\\nserverCtx.Logger.Debug(\\\"received quit signal\\\")\\n\\ngraceDuration, _ := cmd.Flags().GetDuration(FlagShutdownGrace)\\n    if graceDuration > 0 {\\n    serverCtx.Logger.Info(\\\"graceful shutdown start\\\", FlagShutdownGrace, graceDuration)\\n\\t\\t\\t\\t<-time.After(graceDuration)\\n\\nserverCtx.Logger.Info(\\\"graceful shutdown complete\\\")\\n}\\n\\nreturn err\\n},\\n}\\n\\ncmd.Flags().String(flags.FlagHome, defaultNodeHome, \\\"The application home directory\\\")\\n\\naddStartNodeFlags(cmd, opts)\\n\\nreturn cmd\\n}\\n\\nfunc start(svrCtx *Context, clientCtx client.Context, appCreator types.AppCreator, withCmt bool, opts StartCmdOptions)\\n\\nerror {\\n    svrCfg, err := getAndValidateConfig(svrCtx)\\n    if err != nil {\\n    return err\\n}\\n\\napp, appCleanupFn, err := startApp(svrCtx, appCreator, opts)\\n    if err != nil {\\n    return err\\n}\\n\\ndefer appCleanupFn()\\n\\nmetrics, err := startTelemetry(svrCfg)\\n    if err != nil {\\n    return err\\n}\\n\\nemitServerInfoMetrics()\\n    if !withCmt {\\n    return startStandAlone(svrCtx, svrCfg, clientCtx, app, metrics, opts)\\n}\\n\\nreturn startInProcess(svrCtx, svrCfg, clientCtx, app, metrics, opts)\\n}\\n\\nfunc startStandAlone(svrCtx *Context, svrCfg serverconfig.Config, clientCtx client.Context, app types.Application, metrics *telemetry.Metrics, opts StartCmdOptions)\\n\\nerror {\\n    addr := svrCtx.Viper.GetString(flagAddress)\\n    transport := svrCtx.Viper.GetString(flagTransport)\\n    cmtApp := NewCometABCIWrapper(app)\\n\\nsvr, err := server.NewServer(addr, transport, cmtApp)\\n    if err != nil {\\n    return fmt.Errorf(\\\"error creating listener: %w\\\", err)\\n}\\n\\nsvr.SetLogger(servercmtlog.CometLoggerWrapper{\\n    Logger: svrCtx.Logger.With(\\\"module\\\", \\\"abci-server\\\")\\n})\\n\\ng, ctx := getCtx(svrCtx, false)\\n\\n\\t// Add the tx service to the gRPC router. We only need to register this\\n\\t// service if API or gRPC is enabled, and avoid doing so in the general\\n\\t// case, because it spawns a new local CometBFT RPC client.\\n    if svrCfg.API.Enable || svrCfg.GRPC.Enable {\\n\\t\\t// create tendermint client\\n\\t\\t// assumes the rpc listen address is where tendermint has its rpc server\\n\\t\\trpcclient, err := rpchttp.New(svrCtx.Config.RPC.ListenAddress, \\\"/websocket\\\")\\n    if err != nil {\\n    return err\\n}\\n\\t\\t// re-assign for making the client available below\\n\\t\\t// do not use := to avoid shadowing clientCtx\\n\\t\\tclientCtx = clientCtx.WithClient(rpcclient)\\n\\n\\t\\t// use the provided clientCtx to register the services\\n\\t\\tapp.RegisterTxService(clientCtx)\\n\\napp.RegisterTendermintService(clientCtx)\\n\\napp.RegisterNodeService(clientCtx, svrCfg)\\n}\\n\\ngrpcSrv, clientCtx, err := startGrpcServer(ctx, g, svrCfg.GRPC, clientCtx, svrCtx, app)\\n    if err != nil {\\n    return err\\n}\\n\\nerr = startAPIServer(ctx, g, svrCfg, clientCtx, svrCtx, app, svrCtx.Config.RootDir, grpcSrv, metrics)\\n    if err != nil {\\n    return err\\n}\\n    if opts.PostSetupStandalone != nil {\\n    if err := opts.PostSetupStandalone(svrCtx, clientCtx, ctx, g); err != nil {\\n    return err\\n}\\n\\t\\n}\\n\\ng.Go(func()\\n\\nerror {\\n    if err := svr.Start(); err != nil {\\n    svrCtx.Logger.Error(\\\"failed to start out-of-process ABCI server\\\", \\\"err\\\", err)\\n\\nreturn err\\n}\\n\\n\\t\\t// Wait for the calling process to be canceled or close the provided context,\\n\\t\\t// so we can gracefully stop the ABCI server.\\n\\t\\t<-ctx.Done()\\n\\nsvrCtx.Logger.Info(\\\"stopping the ABCI server...\\\")\\n\\nreturn svr.Stop()\\n})\\n\\nreturn g.Wait()\\n}\\n\\nfunc startInProcess(svrCtx *Context, svrCfg serverconfig.Config, clientCtx client.Context, app types.Application,\\n\\tmetrics *telemetry.Metrics, opts StartCmdOptions,\\n)\\n\\nerror {\\n    cmtCfg := svrCtx.Config\\n    gRPCOnly := svrCtx.Viper.GetBool(flagGRPCOnly)\\n\\ng, ctx := getCtx(svrCtx, true)\\n    if gRPCOnly {\\n\\t\\t// TODO: Generalize logic so that gRPC only is really in startStandAlone\\n\\t\\tsvrCtx.Logger.Info(\\\"starting node in gRPC only mode; CometBFT is disabled\\\")\\n\\nsvrCfg.GRPC.Enable = true\\n}\\n\\nelse {\\n    svrCtx.Logger.Info(\\\"starting node with ABCI CometBFT in-process\\\")\\n\\ntmNode, cleanupFn, err := startCmtNode(ctx, cmtCfg, app, svrCtx)\\n    if err != nil {\\n    return err\\n}\\n\\ndefer cleanupFn()\\n\\n\\t\\t// Add the tx service to the gRPC router. We only need to register this\\n\\t\\t// service if API or gRPC is enabled, and avoid doing so in the general\\n\\t\\t// case, because it spawns a new local CometBFT RPC client.\\n    if svrCfg.API.Enable || svrCfg.GRPC.Enable {\\n\\t\\t\\t// Re-assign for making the client available below do not use := to avoid\\n\\t\\t\\t// shadowing the clientCtx variable.\\n\\t\\t\\tclientCtx = clientCtx.WithClient(local.New(tmNode))\\n\\napp.RegisterTxService(clientCtx)\\n\\napp.RegisterTendermintService(clientCtx)\\n\\napp.RegisterNodeService(clientCtx, svrCfg)\\n}\\n\\t\\n}\\n\\ngrpcSrv, clientCtx, err := startGrpcServer(ctx, g, svrCfg.GRPC, clientCtx, svrCtx, app)\\n    if err != nil {\\n    return err\\n}\\n\\nerr = startAPIServer(ctx, g, svrCfg, clientCtx, svrCtx, app, cmtCfg.RootDir, grpcSrv, metrics)\\n    if err != nil {\\n    return err\\n}\\n    if opts.PostSetup != nil {\\n    if err := opts.PostSetup(svrCtx, clientCtx, ctx, g); err != nil {\\n    return err\\n}\\n\\t\\n}\\n\\n\\t// wait for signal capture and gracefully return\\n\\t// we are guaranteed to be waiting for the \\\"ListenForQuitSignals\\\" goroutine.\\n\\treturn g.Wait()\\n}\\n\\n// TODO: Move nodeKey into being created within the function.\\nfunc startCmtNode(\\n\\tctx context.Context,\\n\\tcfg *cmtcfg.Config,\\n\\tapp types.Application,\\n\\tsvrCtx *Context,\\n) (tmNode *node.Node, cleanupFn func(), err error) {\\n    nodeKey, err := p2p.LoadOrGenNodeKey(cfg.NodeKeyFile())\\n    if err != nil {\\n    return nil, cleanupFn, err\\n}\\n    cmtApp := NewCometABCIWrapper(app)\\n\\ntmNode, err = node.NewNodeWithContext(\\n\\t\\tctx,\\n\\t\\tcfg,\\n\\t\\tpvm.LoadOrGenFilePV(cfg.PrivValidatorKeyFile(), cfg.PrivValidatorStateFile()),\\n\\t\\tnodeKey,\\n\\t\\tproxy.NewLocalClientCreator(cmtApp),\\n\\t\\tgetGenDocProvider(cfg),\\n\\t\\tcmtcfg.DefaultDBProvider,\\n\\t\\tnode.DefaultMetricsProvider(cfg.Instrumentation),\\n\\t\\tservercmtlog.CometLoggerWrapper{\\n    Logger: svrCtx.Logger\\n},\\n\\t)\\n    if err != nil {\\n    return tmNode, cleanupFn, err\\n}\\n    if err := tmNode.Start(); err != nil {\\n    return tmNode, cleanupFn, err\\n}\\n\\ncleanupFn = func() {\\n    if tmNode != nil && tmNode.IsRunning() {\\n\\t\\t\\t_ = tmNode.Stop()\\n}\\n\\t\\n}\\n\\nreturn tmNode, cleanupFn, nil\\n}\\n\\nfunc getAndValidateConfig(svrCtx *Context) (serverconfig.Config, error) {\\n    config, err := serverconfig.GetConfig(svrCtx.Viper)\\n    if err != nil {\\n    return config, err\\n}\\n    if err := config.ValidateBasic(); err != nil {\\n    return config, err\\n}\\n\\nreturn config, nil\\n}\\n\\n// returns a function which returns the genesis doc from the genesis file.\\nfunc getGenDocProvider(cfg *cmtcfg.Config)\\n\\nfunc() (*cmttypes.GenesisDoc, error) {\\n    return func() (*cmttypes.GenesisDoc, error) {\\n    appGenesis, err := genutiltypes.AppGenesisFromFile(cfg.GenesisFile())\\n    if err != nil {\\n    return nil, err\\n}\\n\\nreturn appGenesis.ToGenesisDoc()\\n}\\n}\\n\\nfunc setupTraceWriter(svrCtx *Context) (traceWriter io.WriteCloser, cleanup func(), err error) {\\n\\t// clean up the traceWriter when the server is shutting down\\n\\tcleanup = func() {\\n}\\n    traceWriterFile := svrCtx.Viper.GetString(flagTraceStore)\\n\\ntraceWriter, err = openTraceWriter(traceWriterFile)\\n    if err != nil {\\n    return traceWriter, cleanup, err\\n}\\n\\n\\t// if flagTraceStore is not used then traceWriter is nil\\n    if traceWriter != nil {\\n    cleanup = func() {\\n    if err = traceWriter.Close(); err != nil {\\n    svrCtx.Logger.Error(\\\"failed to close trace writer\\\", \\\"err\\\", err)\\n}\\n\\t\\n}\\n\\t\\n}\\n\\nreturn traceWriter, cleanup, nil\\n}\\n\\nfunc startGrpcServer(\\n\\tctx context.Context,\\n\\tg *errgroup.Group,\\n\\tconfig serverconfig.GRPCConfig,\\n\\tclientCtx client.Context,\\n\\tsvrCtx *Context,\\n\\tapp types.Application,\\n) (*grpc.Server, client.Context, error) {\\n    if !config.Enable {\\n\\t\\t// return grpcServer as nil if gRPC is disabled\\n\\t\\treturn nil, clientCtx, nil\\n}\\n\\t_, _, err := net.SplitHostPort(config.Address)\\n    if err != nil {\\n    return nil, clientCtx, err\\n}\\n    maxSendMsgSize := config.MaxSendMsgSize\\n    if maxSendMsgSize == 0 {\\n    maxSendMsgSize = serverconfig.DefaultGRPCMaxSendMsgSize\\n}\\n    maxRecvMsgSize := config.MaxRecvMsgSize\\n    if maxRecvMsgSize == 0 {\\n    maxRecvMsgSize = serverconfig.DefaultGRPCMaxRecvMsgSize\\n}\\n\\n\\t// if gRPC is enabled, configure gRPC client for gRPC gateway\\n\\tgrpcClient, err := grpc.Dial( //nolint: staticcheck // ignore this line for this linter\\n\\t\\tconfig.Address,\\n\\t\\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\\n\\t\\tgrpc.WithDefaultCallOptions(\\n\\t\\t\\tgrpc.ForceCodec(codec.NewProtoCodec(clientCtx.InterfaceRegistry).GRPCCodec()),\\n\\t\\t\\tgrpc.MaxCallRecvMsgSize(maxRecvMsgSize),\\n\\t\\t\\tgrpc.MaxCallSendMsgSize(maxSendMsgSize),\\n\\t\\t),\\n\\t)\\n    if err != nil {\\n    return nil, clientCtx, err\\n}\\n\\nclientCtx = clientCtx.WithGRPCClient(grpcClient)\\n\\nsvrCtx.Logger.Debug(\\\"gRPC client assigned to client context\\\", \\\"target\\\", config.Address)\\n\\ngrpcSrv, err := servergrpc.NewGRPCServer(clientCtx, app, config)\\n    if err != nil {\\n    return nil, clientCtx, err\\n}\\n\\n\\t// Start the gRPC server in a goroutine. Note, the provided ctx will ensure\\n\\t// that the server is gracefully shut down.\\n\\tg.Go(func()\\n\\nerror {\\n    return servergrpc.StartGRPCServer(ctx, svrCtx.Logger.With(\\\"module\\\", \\\"grpc-server\\\"), config, grpcSrv)\\n})\\n\\nreturn grpcSrv, clientCtx, nil\\n}\\n\\nfunc startAPIServer(\\n\\tctx context.Context,\\n\\tg *errgroup.Group,\\n\\tsvrCfg serverconfig.Config,\\n\\tclientCtx client.Context,\\n\\tsvrCtx *Context,\\n\\tapp types.Application,\\n\\thome string,\\n\\tgrpcSrv *grpc.Server,\\n\\tmetrics *telemetry.Metrics,\\n)\\n\\nerror {\\n    if !svrCfg.API.Enable {\\n    return nil\\n}\\n\\nclientCtx = clientCtx.WithHomeDir(home)\\n    apiSrv := api.New(clientCtx, svrCtx.Logger.With(\\\"module\\\", \\\"api-server\\\"), grpcSrv)\\n\\napp.RegisterAPIRoutes(apiSrv, svrCfg.API)\\n    if svrCfg.Telemetry.Enabled {\\n    apiSrv.SetTelemetry(metrics)\\n}\\n\\ng.Go(func()\\n\\nerror {\\n    return apiSrv.Start(ctx, svrCfg)\\n})\\n\\nreturn nil\\n}\\n\\nfunc startTelemetry(cfg serverconfig.Config) (*telemetry.Metrics, error) {\\n    return telemetry.New(cfg.Telemetry)\\n}\\n\\n// wrapCPUProfile starts CPU profiling, if enabled, and executes the provided\\n// callbackFn in a separate goroutine, then will wait for that callback to\\n// return.\\n//\\n// NOTE: We expect the caller to handle graceful shutdown and signal handling.\\nfunc wrapCPUProfile(svrCtx *Context, callbackFn func()\\n\\nerror)\\n\\nerror {\\n    if cpuProfile := svrCtx.Viper.GetString(flagCPUProfile); cpuProfile != \\\"\\\" {\\n    f, err := os.Create(cpuProfile)\\n    if err != nil {\\n    return err\\n}\\n\\nsvrCtx.Logger.Info(\\\"starting CPU profiler\\\", \\\"profile\\\", cpuProfile)\\n    if err := pprof.StartCPUProfile(f); err != nil {\\n    return err\\n}\\n\\ndefer func() {\\n    svrCtx.Logger.Info(\\\"stopping CPU profiler\\\", \\\"profile\\\", cpuProfile)\\n\\npprof.StopCPUProfile()\\n    if err := f.Close(); err != nil {\\n    svrCtx.Logger.Info(\\\"failed to close cpu-profile file\\\", \\\"profile\\\", cpuProfile, \\\"err\\\", err.Error())\\n}\\n\\t\\n}()\\n}\\n\\nreturn callbackFn()\\n}\\n\\n// emitServerInfoMetrics emits server info related metrics using application telemetry.\\nfunc emitServerInfoMetrics() {\\n    var ls []metrics.Label\\n    versionInfo := version.NewInfo()\\n    if len(versionInfo.GoVersion) > 0 {\\n    ls = append(ls, telemetry.NewLabel(\\\"go\\\", versionInfo.GoVersion))\\n}\\n    if len(versionInfo.CosmosSdkVersion) > 0 {\\n    ls = append(ls, telemetry.NewLabel(\\\"version\\\", versionInfo.CosmosSdkVersion))\\n}\\n    if len(ls) == 0 {\\n    return\\n}\\n\\ntelemetry.SetGaugeWithLabels([]string{\\\"server\\\", \\\"info\\\"\\n}, 1, ls)\\n}\\n\\nfunc getCtx(svrCtx *Context, block bool) (*errgroup.Group, context.Context) {\\n    ctx, cancelFn := context.WithCancel(context.Background())\\n\\ng, ctx := errgroup.WithContext(ctx)\\n\\t// listen for quit signals so the calling parent process can gracefully exit\\n\\tListenForQuitSignals(g, block, cancelFn, svrCtx.Logger)\\n\\nreturn g, ctx\\n}\\n\\nfunc startApp(svrCtx *Context, appCreator types.AppCreator, opts StartCmdOptions) (app types.Application, cleanupFn func(), err error) {\\n    traceWriter, traceCleanupFn, err := setupTraceWriter(svrCtx)\\n    if err != nil {\\n    return app, traceCleanupFn, err\\n}\\n    home := svrCtx.Config.RootDir\\n\\tdb, err := opts.DBOpener(home, GetAppDBBackend(svrCtx.Viper))\\n    if err != nil {\\n    return app, traceCleanupFn, err\\n}\\n    if isTestnet, ok := svrCtx.Viper.Get(KeyIsTestnet).(bool); ok && isTestnet {\\n    app, err = testnetify(svrCtx, appCreator, db, traceWriter)\\n    if err != nil {\\n    return app, traceCleanupFn, err\\n}\\n\\t\\n}\\n\\nelse {\\n    app = appCreator(svrCtx.Logger, db, traceWriter, svrCtx.Viper)\\n}\\n\\ncleanupFn = func() {\\n    traceCleanupFn()\\n    if localErr := app.Close(); localErr != nil {\\n    svrCtx.Logger.Error(localErr.Error())\\n}\\n\\t\\n}\\n\\nreturn app, cleanupFn, nil\\n}\\n\\n// InPlaceTestnetCreator utilizes the provided chainID and operatorAddress as well as the local private validator key to\\n// control the network represented in the data folder. This is useful to create testnets nearly identical to your\\n// mainnet environment.\\nfunc InPlaceTestnetCreator(testnetAppCreator types.AppCreator) *cobra.Command {\\n    opts := StartCmdOptions{\\n}\\n    if opts.DBOpener == nil {\\n    opts.DBOpener = openDB\\n}\\n    if opts.StartCommandHandler == nil {\\n    opts.StartCommandHandler = start\\n}\\n    cmd := &cobra.Command{\\n    Use:   \\\"in-place-testnet [newChainID] [newOperatorAddress]\\\",\\n    Short: \\\"Create and start a testnet from current local state\\\",\\n    Long: `Create and start a testnet from current local state.\\nAfter utilizing this command the network will start. If the network is stopped,\\nthe normal \\\"start\\\" command should be used. Re-using this command on state that\\nhas already been modified by this command could result in unexpected behavior.\\n\\nAdditionally, the first block may take up to one minute to be committed, depending\\non how old the block is. For instance, if a snapshot was taken weeks ago and we want\\nto turn this into a testnet, it is possible lots of pending state needs to be committed\\n(expiring locks, etc.). It is recommended that you should wait for this block to be committed\\nbefore stopping the daemon.\\n\\nIf the --trigger-testnet-upgrade flag is set, the upgrade handler specified by the flag will be run\\non the first block of the testnet.\\n\\nRegardless of whether the flag is set or not, if any new stores are introduced in the daemon being run,\\nthose stores will be registered in order to prevent panics. Therefore, you only need to set the flag if\\nyou want to test the upgrade handler itself.\\n`,\\n    Example: \\\"in-place-testnet localosmosis osmo12smx2wdlyttvyzvzg54y2vnqwq2qjateuf7thj\\\",\\n    Args:    cobra.ExactArgs(2),\\n    RunE: func(cmd *cobra.Command, args []string)\\n\\nerror {\\n    serverCtx := GetServerContextFromCmd(cmd)\\n\\t\\t\\t_, err := GetPruningOptionsFromFlags(serverCtx.Viper)\\n    if err != nil {\\n    return err\\n}\\n\\nclientCtx, err := client.GetClientQueryContext(cmd)\\n    if err != nil {\\n    return err\\n}\\n\\nwithCMT, _ := cmd.Flags().GetBool(flagWithComet)\\n    if !withCMT {\\n    serverCtx.Logger.Info(\\\"starting ABCI without CometBFT\\\")\\n}\\n    newChainID := args[0]\\n    newOperatorAddress := args[1]\\n\\n\\t\\t\\tskipConfirmation, _ := cmd.Flags().GetBool(\\\"skip-confirmation\\\")\\n    if !skipConfirmation {\\n\\t\\t\\t\\t// Confirmation prompt to prevent accidental modification of state.\\n    reader := bufio.NewReader(os.Stdin)\\n\\nfmt.Println(\\\"This operation will modify state in your data folder and cannot be undone. Do you want to continue? (y/n)\\\")\\n\\ntext, _ := reader.ReadString('\\\\n')\\n    response := strings.TrimSpace(strings.ToLower(text))\\n    if response != \\\"y\\\" && response != \\\"yes\\\" {\\n    fmt.Println(\\\"Operation canceled.\\\")\\n\\nreturn nil\\n}\\n\\t\\n}\\n\\n\\t\\t\\t// Set testnet keys to be used by the application.\\n\\t\\t\\t// This is done to prevent changes to existing start API.\\n\\t\\t\\tserverCtx.Viper.Set(KeyIsTestnet, true)\\n\\nserverCtx.Viper.Set(KeyNewChainID, newChainID)\\n\\nserverCtx.Viper.Set(KeyNewOpAddr, newOperatorAddress)\\n\\nerr = wrapCPUProfile(serverCtx, func()\\n\\nerror {\\n    return opts.StartCommandHandler(serverCtx, clientCtx, testnetAppCreator, withCMT, opts)\\n})\\n\\nserverCtx.Logger.Debug(\\\"received quit signal\\\")\\n\\ngraceDuration, _ := cmd.Flags().GetDuration(FlagShutdownGrace)\\n    if graceDuration > 0 {\\n    serverCtx.Logger.Info(\\\"graceful shutdown start\\\", FlagShutdownGrace, graceDuration)\\n\\t\\t\\t\\t<-time.After(graceDuration)\\n\\nserverCtx.Logger.Info(\\\"graceful shutdown complete\\\")\\n}\\n\\nreturn err\\n},\\n}\\n\\naddStartNodeFlags(cmd, opts)\\n\\ncmd.Flags().String(KeyTriggerTestnetUpgrade, \\\"\\\", \\\"If set (example: \\\\\\\"v21\\\\\\\"), triggers the v21 upgrade handler to run on the first block of the testnet\\\")\\n\\ncmd.Flags().Bool(\\\"skip-confirmation\\\", false, \\\"Skip the confirmation prompt\\\")\\n\\nreturn cmd\\n}\\n\\n// testnetify modifies both state and blockStore, allowing the provided operator address and local validator key to control the network\\n// that the state in the data folder represents. The chainID of the local genesis file is modified to match the provided chainID.\\nfunc testnetify(ctx *Context, testnetAppCreator types.AppCreator, db dbm.DB, traceWriter io.WriteCloser) (types.Application, error) {\\n    config := ctx.Config\\n\\n\\tnewChainID, ok := ctx.Viper.Get(KeyNewChainID).(string)\\n    if !ok {\\n    return nil, fmt.Errorf(\\\"expected string for key %s\\\", KeyNewChainID)\\n}\\n\\n\\t// Modify app genesis chain ID and save to genesis file.\\n    genFilePath := config.GenesisFile()\\n\\nappGen, err := genutiltypes.AppGenesisFromFile(genFilePath)\\n    if err != nil {\\n    return nil, err\\n}\\n\\nappGen.ChainID = newChainID\\n    if err := appGen.ValidateAndComplete(); err != nil {\\n    return nil, err\\n}\\n    if err := appGen.SaveAs(genFilePath); err != nil {\\n    return nil, err\\n}\\n\\n\\t// Regenerate addrbook.json to prevent peers on old network from causing error logs.\\n    addrBookPath := filepath.Join(config.RootDir, \\\"config\\\", \\\"addrbook.json\\\")\\n    if err := os.Remove(addrBookPath); err != nil && !os.IsNotExist(err) {\\n    return nil, fmt.Errorf(\\\"failed to remove existing addrbook.json: %w\\\", err)\\n}\\n    emptyAddrBook := []byte(\\\"{\\n}\\\")\\n    if err := os.WriteFile(addrBookPath, emptyAddrBook, 0o600); err != nil {\\n    return nil, fmt.Errorf(\\\"failed to create empty addrbook.json: %w\\\", err)\\n}\\n\\n\\t// Load the comet genesis doc provider.\\n    genDocProvider := node.DefaultGenesisDocProviderFunc(config)\\n\\n\\t// Initialize blockStore and stateDB.\\n\\tblockStoreDB, err := cmtcfg.DefaultDBProvider(&cmtcfg.DBContext{\\n    ID: \\\"blockstore\\\",\\n    Config: config\\n})\\n    if err != nil {\\n    return nil, err\\n}\\n    blockStore := store.NewBlockStore(blockStoreDB)\\n\\nstateDB, err := cmtcfg.DefaultDBProvider(&cmtcfg.DBContext{\\n    ID: \\\"state\\\",\\n    Config: config\\n})\\n    if err != nil {\\n    return nil, err\\n}\\n\\ndefer blockStore.Close()\\n\\ndefer stateDB.Close()\\n    privValidator := pvm.LoadOrGenFilePV(config.PrivValidatorKeyFile(), config.PrivValidatorStateFile())\\n\\nuserPubKey, err := privValidator.GetPubKey()\\n    if err != nil {\\n    return nil, err\\n}\\n    validatorAddress := userPubKey.Address()\\n    stateStore := sm.NewStore(stateDB, sm.StoreOptions{\\n    DiscardABCIResponses: config.Storage.DiscardABCIResponses,\\n})\\n\\nstate, genDoc, err := node.LoadStateFromDBOrGenesisDocProvider(stateDB, genDocProvider)\\n    if err != nil {\\n    return nil, err\\n}\\n\\nctx.Viper.Set(KeyNewValAddr, validatorAddress)\\n\\nctx.Viper.Set(KeyUserPubKey, userPubKey)\\n    testnetApp := testnetAppCreator(ctx.Logger, db, traceWriter, ctx.Viper)\\n\\n\\t// We need to create a temporary proxyApp to get the initial state of the application.\\n\\t// Depending on how the node was stopped, the application height can differ from the blockStore height.\\n\\t// This height difference changes how we go about modifying the state.\\n    cmtApp := NewCometABCIWrapper(testnetApp)\\n\\t_, context := getCtx(ctx, true)\\n    clientCreator := proxy.NewLocalClientCreator(cmtApp)\\n    metrics := node.DefaultMetricsProvider(cmtcfg.DefaultConfig().Instrumentation)\\n\\t_, _, _, _, proxyMetrics, _, _ := metrics(genDoc.ChainID)\\n    proxyApp := proxy.NewAppConns(clientCreator, proxyMetrics)\\n    if err := proxyApp.Start(); err != nil {\\n    return nil, fmt.Errorf(\\\"error starting proxy app connections: %w\\\", err)\\n}\\n\\nres, err := proxyApp.Query().Info(context, proxy.RequestInfo)\\n    if err != nil {\\n    return nil, fmt.Errorf(\\\"error calling Info: %w\\\", err)\\n}\\n\\nerr = proxyApp.Stop()\\n    if err != nil {\\n    return nil, err\\n}\\n    appHash := res.LastBlockAppHash\\n    appHeight := res.LastBlockHeight\\n\\n\\tvar block *cmttypes.Block\\n    switch {\\n    case appHeight == blockStore.Height():\\n\\t\\tblock = blockStore.LoadBlock(blockStore.Height())\\n\\t\\t// If the state's last blockstore height does not match the app and blockstore height, we likely stopped with the halt height flag.\\n    if state.LastBlockHeight != appHeight {\\n    state.LastBlockHeight = appHeight\\n\\t\\t\\tblock.AppHash = appHash\\n\\t\\t\\tstate.AppHash = appHash\\n}\\n\\nelse {\\n\\t\\t\\t// Node was likely stopped via SIGTERM, delete the next block's seen commit\\n    err := blockStoreDB.Delete(fmt.Appendf(nil, \\\"SC:%v\\\", blockStore.Height()+1))\\n    if err != nil {\\n    return nil, err\\n}\\n\\t\\n}\\n    case blockStore.Height() > state.LastBlockHeight:\\n\\t\\t// This state usually occurs when we gracefully stop the node.\\n\\t\\terr = blockStore.DeleteLatestBlock()\\n    if err != nil {\\n    return nil, err\\n}\\n\\nblock = blockStore.LoadBlock(blockStore.Height())\\n\\ndefault:\\n\\t\\t// If there is any other state, we just load the block\\n\\t\\tblock = blockStore.LoadBlock(blockStore.Height())\\n}\\n\\nblock.ChainID = newChainID\\n\\tstate.ChainID = newChainID\\n\\n\\tblock.LastBlockID = state.LastBlockID\\n\\tblock.LastCommit.BlockID = state.LastBlockID\\n\\n\\t// Create a vote from our validator\\n    vote := cmttypes.Vote{\\n    Type:             cmtproto.PrecommitType,\\n    Height:           state.LastBlockHeight,\\n    Round:            0,\\n    BlockID:          state.LastBlockID,\\n    Timestamp:        time.Now(),\\n    ValidatorAddress: validatorAddress,\\n    ValidatorIndex:   0,\\n    Signature:        []byte{\\n},\\n}\\n\\n\\t// Sign the vote, and copy the proto changes from the act of signing to the vote itself\\n    voteProto := vote.ToProto()\\n\\nerr = privValidator.SignVote(newChainID, voteProto)\\n    if err != nil {\\n    return nil, err\\n}\\n\\nvote.Signature = voteProto.Signature\\n\\tvote.Timestamp = voteProto.Timestamp\\n\\n\\t// Modify the block's lastCommit to be signed only by our validator\\n\\tblock.LastCommit.Signatures[0].ValidatorAddress = validatorAddress\\n\\tblock.LastCommit.Signatures[0].Signature = vote.Signature\\n\\tblock.LastCommit.Signatures = []cmttypes.CommitSig{\\n    block.LastCommit.Signatures[0]\\n}\\n\\n\\t// Load the seenCommit of the lastBlockHeight and modify it to be signed from our validator\\n    seenCommit := blockStore.LoadSeenCommit(state.LastBlockHeight)\\n\\nseenCommit.BlockID = state.LastBlockID\\n\\tseenCommit.Round = vote.Round\\n\\tseenCommit.Signatures[0].Signature = vote.Signature\\n\\tseenCommit.Signatures[0].ValidatorAddress = validatorAddress\\n\\tseenCommit.Signatures[0].Timestamp = vote.Timestamp\\n\\tseenCommit.Signatures = []cmttypes.CommitSig{\\n    seenCommit.Signatures[0]\\n}\\n\\nerr = blockStore.SaveSeenCommit(state.LastBlockHeight, seenCommit)\\n    if err != nil {\\n    return nil, err\\n}\\n\\n\\t// Create ValidatorSet struct containing just our valdiator.\\n    newVal := &cmttypes.Validator{\\n    Address:     validatorAddress,\\n    PubKey:      userPubKey,\\n    VotingPower: 900000000000000,\\n}\\n    newValSet := &cmttypes.ValidatorSet{\\n    Validators: []*cmttypes.Validator{\\n    newVal\\n},\\n    Proposer:   newVal,\\n}\\n\\n\\t// Replace all valSets in state to be the valSet with just our validator.\\n\\tstate.Validators = newValSet\\n\\tstate.LastValidators = newValSet\\n\\tstate.NextValidators = newValSet\\n\\tstate.LastHeightValidatorsChanged = blockStore.Height()\\n\\nerr = stateStore.Save(state)\\n    if err != nil {\\n    return nil, err\\n}\\n\\n\\t// Create a ValidatorsInfo struct to store in stateDB.\\n\\tvalSet, err := state.Validators.ToProto()\\n    if err != nil {\\n    return nil, err\\n}\\n    valInfo := &cmtstate.ValidatorsInfo{\\n    ValidatorSet:      valSet,\\n    LastHeightChanged: state.LastBlockHeight,\\n}\\n\\nbuf, err := valInfo.Marshal()\\n    if err != nil {\\n    return nil, err\\n}\\n\\n\\t// Modfiy Validators stateDB entry.\\n\\terr = stateDB.Set(fmt.Appendf(nil, \\\"validatorsKey:%v\\\", blockStore.Height()), buf)\\n    if err != nil {\\n    return nil, err\\n}\\n\\n\\t// Modify LastValidators stateDB entry.\\n\\terr = stateDB.Set(fmt.Appendf(nil, \\\"validatorsKey:%v\\\", blockStore.Height()-1), buf)\\n    if err != nil {\\n    return nil, err\\n}\\n\\n\\t// Modify NextValidators stateDB entry.\\n\\terr = stateDB.Set(fmt.Appendf(nil, \\\"validatorsKey:%v\\\", blockStore.Height()+1), buf)\\n    if err != nil {\\n    return nil, err\\n}\\n\\n\\t// Since we modified the chainID, we set the new genesisDoc in the stateDB.\\n\\tb, err := cmtjson.Marshal(genDoc)\\n    if err != nil {\\n    return nil, err\\n}\\n    if err := stateDB.SetSync([]byte(\\\"genesisDoc\\\"), b); err != nil {\\n    return nil, err\\n}\\n\\nreturn testnetApp, err\\n}\\n\\n// addStartNodeFlags should be added to any CLI commands that start the network.\\nfunc addStartNodeFlags(cmd *cobra.Command, opts StartCmdOptions) {\\n    cmd.Flags().Bool(flagWithComet, true, \\\"Run abci app embedded in-process with CometBFT\\\")\\n\\ncmd.Flags().String(flagAddress, \\\"tcp://127.0.0.1:26658\\\", \\\"Listen address\\\")\\n\\ncmd.Flags().String(flagTransport, \\\"socket\\\", \\\"Transport protocol: socket, grpc\\\")\\n\\ncmd.Flags().String(flagTraceStore, \\\"\\\", \\\"Enable KVStore tracing to an output file\\\")\\n\\ncmd.Flags().String(FlagMinGasPrices, \\\"\\\", \\\"Minimum gas prices to accept for transactions; Any fee in a tx must meet this minimum (e.g. 0.01photino;0.0001stake)\\\")\\n\\ncmd.Flags().Uint64(FlagQueryGasLimit, 0, \\\"Maximum gas a Rest/Grpc query can consume. Blank and 0 imply unbounded.\\\")\\n\\ncmd.Flags().IntSlice(FlagUnsafeSkipUpgrades, []int{\\n}, \\\"Skip a set of upgrade heights to continue the old binary\\\")\\n\\ncmd.Flags().Uint64(FlagHaltHeight, 0, \\\"Block height at which to gracefully halt the chain and shutdown the node\\\")\\n\\ncmd.Flags().Uint64(FlagHaltTime, 0, \\\"Minimum block time (in Unix seconds)\\n\\nat which to gracefully halt the chain and shutdown the node\\\")\\n\\ncmd.Flags().Bool(FlagInterBlockCache, true, \\\"Enable inter-block caching\\\")\\n\\ncmd.Flags().String(flagCPUProfile, \\\"\\\", \\\"Enable CPU profiling and write to the provided file\\\")\\n\\ncmd.Flags().Bool(FlagTrace, false, \\\"Provide full stack traces for errors in ABCI Log\\\")\\n\\ncmd.Flags().String(FlagPruning, pruningtypes.PruningOptionDefault, \\\"Pruning strategy (default|nothing|everything|custom)\\\")\\n\\ncmd.Flags().Uint64(FlagPruningKeepRecent, 0, \\\"Number of recent heights to keep on disk (ignored if pruning is not 'custom')\\\")\\n\\ncmd.Flags().Uint64(FlagPruningInterval, 0, \\\"Height interval at which pruned heights are removed from disk (ignored if pruning is not 'custom')\\\")\\n\\ncmd.Flags().Uint(FlagInvCheckPeriod, 0, \\\"Assert registered invariants every N blocks\\\")\\n\\ncmd.Flags().Uint64(FlagMinRetainBlocks, 0, \\\"Minimum block height offset during ABCI commit to prune CometBFT blocks\\\")\\n\\ncmd.Flags().Bool(FlagAPIEnable, false, \\\"Define if the API server should be enabled\\\")\\n\\ncmd.Flags().Bool(FlagAPISwagger, false, \\\"Define if swagger documentation should automatically be registered (Note: the API must also be enabled)\\\")\\n\\ncmd.Flags().String(FlagAPIAddress, serverconfig.DefaultAPIAddress, \\\"the API server address to listen on\\\")\\n\\ncmd.Flags().Uint(FlagAPIMaxOpenConnections, 1000, \\\"Define the number of maximum open connections\\\")\\n\\ncmd.Flags().Uint(FlagRPCReadTimeout, 10, \\\"Define the CometBFT RPC read timeout (in seconds)\\\")\\n\\ncmd.Flags().Uint(FlagRPCWriteTimeout, 0, \\\"Define the CometBFT RPC write timeout (in seconds)\\\")\\n\\ncmd.Flags().Uint(FlagRPCMaxBodyBytes, 1000000, \\\"Define the CometBFT maximum request body (in bytes)\\\")\\n\\ncmd.Flags().Bool(FlagAPIEnableUnsafeCORS, false, \\\"Define if CORS should be enabled (unsafe - use it at your own risk)\\\")\\n\\ncmd.Flags().Bool(flagGRPCOnly, false, \\\"Start the node in gRPC query only mode (no CometBFT process is started)\\\")\\n\\ncmd.Flags().Bool(flagGRPCEnable, true, \\\"Define if the gRPC server should be enabled\\\")\\n\\ncmd.Flags().String(flagGRPCAddress, serverconfig.DefaultGRPCAddress, \\\"the gRPC server address to listen on\\\")\\n\\ncmd.Flags().Bool(flagGRPCWebEnable, true, \\\"Define if the gRPC-Web server should be enabled. (Note: gRPC must also be enabled)\\\")\\n\\ncmd.Flags().Uint64(FlagStateSyncSnapshotInterval, 0, \\\"State sync snapshot interval\\\")\\n\\ncmd.Flags().Uint32(FlagStateSyncSnapshotKeepRecent, 2, \\\"State sync snapshot to keep\\\")\\n\\ncmd.Flags().Bool(FlagDisableIAVLFastNode, false, \\\"Disable fast node for IAVL tree\\\")\\n\\ncmd.Flags().Int(FlagMempoolMaxTxs, mempool.DefaultMaxTx, \\\"Sets MaxTx value for the app-side mempool\\\")\\n\\ncmd.Flags().Duration(FlagShutdownGrace, 0*time.Second, \\\"On Shutdown, duration to wait for resource clean up\\\")\\n\\n\\t// support old flags name for backwards compatibility\\n\\tcmd.Flags().SetNormalizeFunc(func(f *pflag.FlagSet, name string)\\n\\npflag.NormalizedName {\\n    if name == \\\"with-tendermint\\\" {\\n    name = flagWithComet\\n}\\n\\nreturn pflag.NormalizedName(name)\\n})\\n\\n\\t// add support for all CometBFT-specific command line options\\n\\tcmtcmd.AddNodeFlags(cmd)\\n    if opts.AddFlags != nil {\\n    opts.AddFlags(cmd)\\n}\\n}\\n\\n\", \"Title: Node Client (Daemon)\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/advanced/node\\nContent: Synopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nNode Client (Daemon)\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nstart command\\nThe start command is defined in the /server folder of the Cosmos SDK. It is added to the root command of the full-node client in the main function and called by the end-user to start their node: As a reminder, the full-node is composed of three conceptual layers: the networking layer, the consensus layer and the application layer. The first two are generally bundled together in an entity called the consensus engine (CometBFT by default), while the third is the state-machine defined with the help of the Cosmos SDK. Currently, the Cosmos SDK uses CometBFT as the default consensus engine, meaning the start command is implemented to boot up a CometBFT node. The flow of the start command is pretty straightforward. First, it retrieves the config from the context in order to open the db (a leveldb instance by default). This db contains the latest known state of the application (empty if the application is started from the first time. With the db , the start command creates a new instance of the application using an appCreator function: Note that an appCreator is a function that fulfills the AppCreator signature: In practice, the constructor of the application is passed as the appCreator. Then, the instance of app is used to instantiate a new CometBFT node: The CometBFT node can be created with app because the latter satisfies the abci.Application interface (given that app extends baseapp ). As part of the node.New method, CometBFT makes sure that the height of the application (i.e. number of blocks since genesis) is equal to the height of the CometBFT node. The difference between these two heights should always be negative or null. If it is strictly negative, node.New will replay blocks until the height of the application reaches the height of the CometBFT node. Finally, if the height of the application is 0 , the CometBFT node will call InitChain on the application to initialize the state from the genesis file. Once the CometBFT node is instantiated and in sync with the application, the node can be started: package server\\n\\nimport (\\n    \\n\\t\\\"context\\\"\\n    \\\"errors\\\"\\n    \\\"fmt\\\"\\n    \\\"io\\\"\\n    \\\"net\\\"\\n    \\\"os\\\"\\n    \\\"runtime/pprof\\\"\\n\\n\\tpruningtypes \\\"cosmossdk.io/store/pruning/types\\\"\\n    \\\"github.com/armon/go-metrics\\\"\\n    \\\"github.com/cometbft/cometbft/abci/server\\\"\\n\\tcmtcmd \\\"github.com/cometbft/cometbft/cmd/cometbft/commands\\\"\\n\\tcmtcfg \\\"github.com/cometbft/cometbft/config\\\"\\n    \\\"github.com/cometbft/cometbft/node\\\"\\n    \\\"github.com/cometbft/cometbft/p2p\\\"\\n\\tpvm \\\"github.com/cometbft/cometbft/privval\\\"\\n    \\\"github.com/cometbft/cometbft/proxy\\\"\\n    \\\"github.com/cometbft/cometbft/rpc/client/local\\\"\\n\\tcmttypes \\\"github.com/cometbft/cometbft/types\\\"\\n\\tdbm \\\"github.com/cosmos/cosmos-db\\\"\\n    \\\"github.com/spf13/cobra\\\"\\n    \\\"github.com/spf13/pflag\\\"\\n    \\\"golang.org/x/sync/errgroup\\\"\\n    \\\"google.golang.org/grpc\\\"\\n    \\\"google.golang.org/grpc/credentials/insecure\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client/flags\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/codec\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/api\\\"\\n\\tserverconfig \\\"github.com/cosmos/cosmos-sdk/server/config\\\"\\n\\tservergrpc \\\"github.com/cosmos/cosmos-sdk/server/grpc\\\"\\n\\tservercmtlog \\\"github.com/cosmos/cosmos-sdk/server/log\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/types\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/telemetry\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/types/mempool\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/version\\\"\\n\\tgenutiltypes \\\"github.com/cosmos/cosmos-sdk/x/genutil/types\\\"\\n)\\n\\nconst (\\n\\t// CometBFT full-node start flags\\n\\tflagWithComet          = \\\"with-comet\\\"\\n\\tflagAddress            = \\\"address\\\"\\n\\tflagTransport          = \\\"transport\\\"\\n\\tflagTraceStore         = \\\"trace-store\\\"\\n\\tflagCPUProfile         = \\\"cpu-profile\\\"\\n\\tFlagMinGasPrices       = \\\"minimum-gas-prices\\\"\\n\\tFlagHaltHeight         = \\\"halt-height\\\"\\n\\tFlagHaltTime           = \\\"halt-time\\\"\\n\\tFlagInterBlockCache    = \\\"inter-block-cache\\\"\\n\\tFlagUnsafeSkipUpgrades = \\\"unsafe-skip-upgrades\\\"\\n\\tFlagTrace              = \\\"trace\\\"\\n\\tFlagInvCheckPeriod     = \\\"inv-check-period\\\"\\n\\n\\tFlagPruning             = \\\"pruning\\\"\\n\\tFlagPruningKeepRecent   = \\\"pruning-keep-recent\\\"\\n\\tFlagPruningInterval     = \\\"pruning-interval\\\"\\n\\tFlagIndexEvents         = \\\"index-events\\\"\\n\\tFlagMinRetainBlocks     = \\\"min-retain-blocks\\\"\\n\\tFlagIAVLCacheSize       = \\\"iavl-cache-size\\\"\\n\\tFlagDisableIAVLFastNode = \\\"iavl-disable-fastnode\\\"\\n\\n\\t// state sync-related flags\\n\\tFlagStateSyncSnapshotInterval   = \\\"state-sync.snapshot-interval\\\"\\n\\tFlagStateSyncSnapshotKeepRecent = \\\"state-sync.snapshot-keep-recent\\\"\\n\\n\\t// api-related flags\\n\\tFlagAPIEnable             = \\\"api.enable\\\"\\n\\tFlagAPISwagger            = \\\"api.swagger\\\"\\n\\tFlagAPIAddress            = \\\"api.address\\\"\\n\\tFlagAPIMaxOpenConnections = \\\"api.max-open-connections\\\"\\n\\tFlagRPCReadTimeout        = \\\"api.rpc-read-timeout\\\"\\n\\tFlagRPCWriteTimeout       = \\\"api.rpc-write-timeout\\\"\\n\\tFlagRPCMaxBodyBytes       = \\\"api.rpc-max-body-bytes\\\"\\n\\tFlagAPIEnableUnsafeCORS   = \\\"api.enabled-unsafe-cors\\\"\\n\\n\\t// gRPC-related flags\\n\\tflagGRPCOnly      = \\\"grpc-only\\\"\\n\\tflagGRPCEnable    = \\\"grpc.enable\\\"\\n\\tflagGRPCAddress   = \\\"grpc.address\\\"\\n\\tflagGRPCWebEnable = \\\"grpc-web.enable\\\"\\n\\n\\t// mempool flags\\n\\tFlagMempoolMaxTxs = \\\"mempool.max-txs\\\"\\n)\\n\\n// StartCmdOptions defines options that can be customized in `StartCmdWithOptions`,\\ntype StartCmdOptions struct {\\n\\t// DBOpener can be used to customize db opening, for example customize db options or support different db backends,\\n\\t// default to the builtin db opener.\\n\\tDBOpener func(rootDir string, backendType dbm.BackendType) (dbm.DB, error)\\n\\t// PostSetup can be used to setup extra services under the same cancellable context,\\n\\t// it's not called in stand-alone mode, only for in-process mode.\\n\\tPostSetup func(svrCtx *Context, clientCtx client.Context, ctx context.Context, g *errgroup.Group)\\n\\nerror\\n\\t// AddFlags add custom flags to start cmd\\n\\tAddFlags func(cmd *cobra.Command)\\n}\\n\\n// StartCmd runs the service passed in, either stand-alone or in-process with\\n// CometBFT.\\nfunc StartCmd(appCreator types.AppCreator, defaultNodeHome string) *cobra.Command {\\n    return StartCmdWithOptions(appCreator, defaultNodeHome, StartCmdOptions{\\n})\\n}\\n\\n// StartCmdWithOptions runs the service passed in, either stand-alone or in-process with\\n// CometBFT.\\nfunc StartCmdWithOptions(appCreator types.AppCreator, defaultNodeHome string, opts StartCmdOptions) *cobra.Command {\\n    if opts.DBOpener == nil {\\n    opts.DBOpener = openDB\\n}\\n    cmd := &cobra.Command{\\n    Use:   \\\"start\\\",\\n    Short: \\\"Run the full node\\\",\\n    Long: `Run the full node application with CometBFT in or out of process. By\\ndefault, the application will run with CometBFT in process.\\n\\nPruning options can be provided via the '--pruning' flag or alternatively with '--pruning-keep-recent', and\\n'pruning-interval' together.\\n\\nFor '--pruning' the options are as follows:\\n\\ndefault: the last 362880 states are kept, pruning at 10 block intervals\\nnothing: all historic states will be saved, nothing will be deleted (i.e. archiving node)\\n\\neverything: 2 latest states will be kept; pruning at 10 block intervals.\\ncustom: allow pruning options to be manually specified through 'pruning-keep-recent', and 'pruning-interval'\\n\\nNode halting configurations exist in the form of two flags: '--halt-height' and '--halt-time'. During\\nthe ABCI Commit phase, the node will check if the current block height is greater than or equal to\\nthe halt-height or if the current block time is greater than or equal to the halt-time. If so, the\\nnode will attempt to gracefully shutdown and the block will not be committed. In addition, the node\\nwill not be able to commit subsequent blocks.\\n\\nFor profiling and benchmarking purposes, CPU profiling can be enabled via the '--cpu-profile' flag\\nwhich accepts a path for the resulting pprof file.\\n\\nThe node may be started in a 'query only' mode where only the gRPC and JSON HTTP\\nAPI services are enabled via the 'grpc-only' flag. In this mode, CometBFT is\\nbypassed and can be used when legacy queries are needed after an on-chain upgrade\\nis performed. Note, when enabled, gRPC will also be automatically enabled.\\n`,\\n    PreRunE: func(cmd *cobra.Command, _ []string)\\n\\nerror {\\n    serverCtx := GetServerContextFromCmd(cmd)\\n\\n\\t\\t\\t// Bind flags to the Context's Viper so the app construction can set\\n\\t\\t\\t// options accordingly.\\n    if err := serverCtx.Viper.BindPFlags(cmd.Flags()); err != nil {\\n    return err\\n}\\n\\n\\t\\t\\t_, err := GetPruningOptionsFromFlags(serverCtx.Viper)\\n\\nreturn err\\n},\\n    RunE: func(cmd *cobra.Command, _ []string)\\n\\nerror {\\n    serverCtx := GetServerContextFromCmd(cmd)\\n\\nclientCtx, err := client.GetClientQueryContext(cmd)\\n    if err != nil {\\n    return err\\n}\\n\\nwithCMT, _ := cmd.Flags().GetBool(flagWithComet)\\n    if !withCMT {\\n    serverCtx.Logger.Info(\\\"starting ABCI without CometBFT\\\")\\n}\\n\\nreturn wrapCPUProfile(serverCtx, func()\\n\\nerror {\\n    return start(serverCtx, clientCtx, appCreator, withCMT, opts)\\n})\\n},\\n}\\n\\ncmd.Flags().String(flags.FlagHome, defaultNodeHome, \\\"The application home directory\\\")\\n\\ncmd.Flags().Bool(flagWithComet, true, \\\"Run abci app embedded in-process with CometBFT\\\")\\n\\ncmd.Flags().String(flagAddress, \\\"tcp://0.0.0.0:26658\\\", \\\"Listen address\\\")\\n\\ncmd.Flags().String(flagTransport, \\\"socket\\\", \\\"Transport protocol: socket, grpc\\\")\\n\\ncmd.Flags().String(flagTraceStore, \\\"\\\", \\\"Enable KVStore tracing to an output file\\\")\\n\\ncmd.Flags().String(FlagMinGasPrices, \\\"\\\", \\\"Minimum gas prices to accept for transactions; Any fee in a tx must meet this minimum (e.g. 0.01photino;0.0001stake)\\\")\\n\\ncmd.Flags().IntSlice(FlagUnsafeSkipUpgrades, []int{\\n}, \\\"Skip a set of upgrade heights to continue the old binary\\\")\\n\\ncmd.Flags().Uint64(FlagHaltHeight, 0, \\\"Block height at which to gracefully halt the chain and shutdown the node\\\")\\n\\ncmd.Flags().Uint64(FlagHaltTime, 0, \\\"Minimum block time (in Unix seconds)\\n\\nat which to gracefully halt the chain and shutdown the node\\\")\\n\\ncmd.Flags().Bool(FlagInterBlockCache, true, \\\"Enable inter-block caching\\\")\\n\\ncmd.Flags().String(flagCPUProfile, \\\"\\\", \\\"Enable CPU profiling and write to the provided file\\\")\\n\\ncmd.Flags().Bool(FlagTrace, false, \\\"Provide full stack traces for errors in ABCI Log\\\")\\n\\ncmd.Flags().String(FlagPruning, pruningtypes.PruningOptionDefault, \\\"Pruning strategy (default|nothing|everything|custom)\\\")\\n\\ncmd.Flags().Uint64(FlagPruningKeepRecent, 0, \\\"Number of recent heights to keep on disk (ignored if pruning is not 'custom')\\\")\\n\\ncmd.Flags().Uint64(FlagPruningInterval, 0, \\\"Height interval at which pruned heights are removed from disk (ignored if pruning is not 'custom')\\\")\\n\\ncmd.Flags().Uint(FlagInvCheckPeriod, 0, \\\"Assert registered invariants every N blocks\\\")\\n\\ncmd.Flags().Uint64(FlagMinRetainBlocks, 0, \\\"Minimum block height offset during ABCI commit to prune CometBFT blocks\\\")\\n\\ncmd.Flags().Bool(FlagAPIEnable, false, \\\"Define if the API server should be enabled\\\")\\n\\ncmd.Flags().Bool(FlagAPISwagger, false, \\\"Define if swagger documentation should automatically be registered (Note: the API must also be enabled)\\\")\\n\\ncmd.Flags().String(FlagAPIAddress, serverconfig.DefaultAPIAddress, \\\"the API server address to listen on\\\")\\n\\ncmd.Flags().Uint(FlagAPIMaxOpenConnections, 1000, \\\"Define the number of maximum open connections\\\")\\n\\ncmd.Flags().Uint(FlagRPCReadTimeout, 10, \\\"Define the CometBFT RPC read timeout (in seconds)\\\")\\n\\ncmd.Flags().Uint(FlagRPCWriteTimeout, 0, \\\"Define the CometBFT RPC write timeout (in seconds)\\\")\\n\\ncmd.Flags().Uint(FlagRPCMaxBodyBytes, 1000000, \\\"Define the CometBFT maximum request body (in bytes)\\\")\\n\\ncmd.Flags().Bool(FlagAPIEnableUnsafeCORS, false, \\\"Define if CORS should be enabled (unsafe - use it at your own risk)\\\")\\n\\ncmd.Flags().Bool(flagGRPCOnly, false, \\\"Start the node in gRPC query only mode (no CometBFT process is started)\\\")\\n\\ncmd.Flags().Bool(flagGRPCEnable, true, \\\"Define if the gRPC server should be enabled\\\")\\n\\ncmd.Flags().String(flagGRPCAddress, serverconfig.DefaultGRPCAddress, \\\"the gRPC server address to listen on\\\")\\n\\ncmd.Flags().Bool(flagGRPCWebEnable, true, \\\"Define if the gRPC-Web server should be enabled. (Note: gRPC must also be enabled)\\\")\\n\\ncmd.Flags().Uint64(FlagStateSyncSnapshotInterval, 0, \\\"State sync snapshot interval\\\")\\n\\ncmd.Flags().Uint32(FlagStateSyncSnapshotKeepRecent, 2, \\\"State sync snapshot to keep\\\")\\n\\ncmd.Flags().Bool(FlagDisableIAVLFastNode, false, \\\"Disable fast node for IAVL tree\\\")\\n\\ncmd.Flags().Int(FlagMempoolMaxTxs, mempool.DefaultMaxTx, \\\"Sets MaxTx value for the app-side mempool\\\")\\n\\n\\t// support old flags name for backwards compatibility\\n\\tcmd.Flags().SetNormalizeFunc(func(f *pflag.FlagSet, name string)\\n\\npflag.NormalizedName {\\n    if name == \\\"with-tendermint\\\" {\\n    name = flagWithComet\\n}\\n\\nreturn pflag.NormalizedName(name)\\n})\\n\\n\\t// add support for all CometBFT-specific command line options\\n\\tcmtcmd.AddNodeFlags(cmd)\\n    if opts.AddFlags != nil {\\n    opts.AddFlags(cmd)\\n}\\n\\nreturn cmd\\n}\\n\\nfunc start(svrCtx *Context, clientCtx client.Context, appCreator types.AppCreator, withCmt bool, opts StartCmdOptions)\\n\\nerror {\\n    svrCfg, err := getAndValidateConfig(svrCtx)\\n    if err != nil {\\n    return err\\n}\\n\\napp, appCleanupFn, err := startApp(svrCtx, appCreator, opts)\\n    if err != nil {\\n    return err\\n}\\n\\ndefer appCleanupFn()\\n\\nmetrics, err := startTelemetry(svrCfg)\\n    if err != nil {\\n    return err\\n}\\n\\nemitServerInfoMetrics()\\n    if !withCmt {\\n    return startStandAlone(svrCtx, app, opts)\\n}\\n\\nreturn startInProcess(svrCtx, svrCfg, clientCtx, app, metrics, opts)\\n}\\n\\nfunc startStandAlone(svrCtx *Context, app types.Application, opts StartCmdOptions)\\n\\nerror {\\n    addr := svrCtx.Viper.GetString(flagAddress)\\n    transport := svrCtx.Viper.GetString(flagTransport)\\n    cmtApp := NewCometABCIWrapper(app)\\n\\nsvr, err := server.NewServer(addr, transport, cmtApp)\\n    if err != nil {\\n    return fmt.Errorf(\\\"error creating listener: %v\\\", err)\\n}\\n\\nsvr.SetLogger(servercmtlog.CometLoggerWrapper{\\n    Logger: svrCtx.Logger.With(\\\"module\\\", \\\"abci-server\\\")\\n})\\n\\ng, ctx := getCtx(svrCtx, false)\\n\\ng.Go(func()\\n\\nerror {\\n    if err := svr.Start(); err != nil {\\n    svrCtx.Logger.Error(\\\"failed to start out-of-process ABCI server\\\", \\\"err\\\", err)\\n\\nreturn err\\n}\\n\\n\\t\\t// Wait for the calling process to be canceled or close the provided context,\\n\\t\\t// so we can gracefully stop the ABCI server.\\n\\t\\t<-ctx.Done()\\n\\nsvrCtx.Logger.Info(\\\"stopping the ABCI server...\\\")\\n\\nreturn errors.Join(svr.Stop(), app.Close())\\n})\\n\\nreturn g.Wait()\\n}\\n\\nfunc startInProcess(svrCtx *Context, svrCfg serverconfig.Config, clientCtx client.Context, app types.Application,\\n\\tmetrics *telemetry.Metrics, opts StartCmdOptions,\\n)\\n\\nerror {\\n    cmtCfg := svrCtx.Config\\n    home := cmtCfg.RootDir\\n    gRPCOnly := svrCtx.Viper.GetBool(flagGRPCOnly)\\n    if gRPCOnly {\\n\\t\\t// TODO: Generalize logic so that gRPC only is really in startStandAlone\\n\\t\\tsvrCtx.Logger.Info(\\\"starting node in gRPC only mode; CometBFT is disabled\\\")\\n\\nsvrCfg.GRPC.Enable = true\\n}\\n\\nelse {\\n    svrCtx.Logger.Info(\\\"starting node with ABCI CometBFT in-process\\\")\\n\\ntmNode, cleanupFn, err := startCmtNode(cmtCfg, app, svrCtx)\\n    if err != nil {\\n    return err\\n}\\n\\ndefer cleanupFn()\\n\\n\\t\\t// Add the tx service to the gRPC router. We only need to register this\\n\\t\\t// service if API or gRPC is enabled, and avoid doing so in the general\\n\\t\\t// case, because it spawns a new local CometBFT RPC client.\\n    if svrCfg.API.Enable || svrCfg.GRPC.Enable {\\n\\t\\t\\t// Re-assign for making the client available below do not use := to avoid\\n\\t\\t\\t// shadowing the clientCtx variable.\\n\\t\\t\\tclientCtx = clientCtx.WithClient(local.New(tmNode))\\n\\napp.RegisterTxService(clientCtx)\\n\\napp.RegisterTendermintService(clientCtx)\\n\\napp.RegisterNodeService(clientCtx, svrCfg)\\n}\\n\\t\\n}\\n\\ng, ctx := getCtx(svrCtx, true)\\n\\ngrpcSrv, clientCtx, err := startGrpcServer(ctx, g, svrCfg.GRPC, clientCtx, svrCtx, app)\\n    if err != nil {\\n    return err\\n}\\n\\nerr = startAPIServer(ctx, g, cmtCfg, svrCfg, clientCtx, svrCtx, app, home, grpcSrv, metrics)\\n    if err != nil {\\n    return err\\n}\\n    if opts.PostSetup != nil {\\n    if err := opts.PostSetup(svrCtx, clientCtx, ctx, g); err != nil {\\n    return err\\n}\\n\\t\\n}\\n\\n\\t// wait for signal capture and gracefully return\\n\\t// we are guaranteed to be waiting for the \\\"ListenForQuitSignals\\\" goroutine.\\n\\treturn g.Wait()\\n}\\n\\n// TODO: Move nodeKey into being created within the function.\\nfunc startCmtNode(\\n\\tcfg *cmtcfg.Config,\\n\\tapp types.Application,\\n\\tsvrCtx *Context,\\n) (tmNode *node.Node, cleanupFn func(), err error) {\\n    nodeKey, err := p2p.LoadOrGenNodeKey(cfg.NodeKeyFile())\\n    if err != nil {\\n    return nil, cleanupFn, err\\n}\\n    cmtApp := NewCometABCIWrapper(app)\\n\\ntmNode, err = node.NewNode(\\n\\t\\tcfg,\\n\\t\\tpvm.LoadOrGenFilePV(cfg.PrivValidatorKeyFile(), cfg.PrivValidatorStateFile()),\\n\\t\\tnodeKey,\\n\\t\\tproxy.NewLocalClientCreator(cmtApp),\\n\\t\\tgetGenDocProvider(cfg),\\n\\t\\tcmtcfg.DefaultDBProvider,\\n\\t\\tnode.DefaultMetricsProvider(cfg.Instrumentation),\\n\\t\\tservercmtlog.CometLoggerWrapper{\\n    Logger: svrCtx.Logger\\n},\\n\\t)\\n    if err != nil {\\n    return tmNode, cleanupFn, err\\n}\\n    if err := tmNode.Start(); err != nil {\\n    return tmNode, cleanupFn, err\\n}\\n\\ncleanupFn = func() {\\n    if tmNode != nil && tmNode.IsRunning() {\\n\\t\\t\\t_ = tmNode.Stop()\\n\\t\\t\\t_ = app.Close()\\n}\\n\\t\\n}\\n\\nreturn tmNode, cleanupFn, nil\\n}\\n\\nfunc getAndValidateConfig(svrCtx *Context) (serverconfig.Config, error) {\\n    config, err := serverconfig.GetConfig(svrCtx.Viper)\\n    if err != nil {\\n    return config, err\\n}\\n    if err := config.ValidateBasic(); err != nil {\\n    return config, err\\n}\\n\\nreturn config, nil\\n}\\n\\n// returns a function which returns the genesis doc from the genesis file.\\nfunc getGenDocProvider(cfg *cmtcfg.Config)\\n\\nfunc() (*cmttypes.GenesisDoc, error) {\\n    return func() (*cmttypes.GenesisDoc, error) {\\n    appGenesis, err := genutiltypes.AppGenesisFromFile(cfg.GenesisFile())\\n    if err != nil {\\n    return nil, err\\n}\\n\\nreturn appGenesis.ToGenesisDoc()\\n}\\n}\\n\\nfunc setupTraceWriter(svrCtx *Context) (traceWriter io.WriteCloser, cleanup func(), err error) {\\n\\t// clean up the traceWriter when the server is shutting down\\n\\tcleanup = func() {\\n}\\n    traceWriterFile := svrCtx.Viper.GetString(flagTraceStore)\\n\\ntraceWriter, err = openTraceWriter(traceWriterFile)\\n    if err != nil {\\n    return traceWriter, cleanup, err\\n}\\n\\n\\t// if flagTraceStore is not used then traceWriter is nil\\n    if traceWriter != nil {\\n    cleanup = func() {\\n    if err = traceWriter.Close(); err != nil {\\n    svrCtx.Logger.Error(\\\"failed to close trace writer\\\", \\\"err\\\", err)\\n}\\n\\t\\n}\\n\\t\\n}\\n\\nreturn traceWriter, cleanup, nil\\n}\\n\\nfunc startGrpcServer(\\n\\tctx context.Context,\\n\\tg *errgroup.Group,\\n\\tconfig serverconfig.GRPCConfig,\\n\\tclientCtx client.Context,\\n\\tsvrCtx *Context,\\n\\tapp types.Application,\\n) (*grpc.Server, client.Context, error) {\\n    if !config.Enable {\\n\\t\\t// return grpcServer as nil if gRPC is disabled\\n\\t\\treturn nil, clientCtx, nil\\n}\\n\\t_, port, err := net.SplitHostPort(config.Address)\\n    if err != nil {\\n    return nil, clientCtx, err\\n}\\n    maxSendMsgSize := config.MaxSendMsgSize\\n    if maxSendMsgSize == 0 {\\n    maxSendMsgSize = serverconfig.DefaultGRPCMaxSendMsgSize\\n}\\n    maxRecvMsgSize := config.MaxRecvMsgSize\\n    if maxRecvMsgSize == 0 {\\n    maxRecvMsgSize = serverconfig.DefaultGRPCMaxRecvMsgSize\\n}\\n    grpcAddress := fmt.Sprintf(\\\"127.0.0.1:%s\\\", port)\\n\\n\\t// if gRPC is enabled, configure gRPC client for gRPC gateway\\n\\tgrpcClient, err := grpc.Dial(\\n\\t\\tgrpcAddress,\\n\\t\\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\\n\\t\\tgrpc.WithDefaultCallOptions(\\n\\t\\t\\tgrpc.ForceCodec(codec.NewProtoCodec(clientCtx.InterfaceRegistry).GRPCCodec()),\\n\\t\\t\\tgrpc.MaxCallRecvMsgSize(maxRecvMsgSize),\\n\\t\\t\\tgrpc.MaxCallSendMsgSize(maxSendMsgSize),\\n\\t\\t),\\n\\t)\\n    if err != nil {\\n    return nil, clientCtx, err\\n}\\n\\nclientCtx = clientCtx.WithGRPCClient(grpcClient)\\n\\nsvrCtx.Logger.Debug(\\\"gRPC client assigned to client context\\\", \\\"target\\\", grpcAddress)\\n\\ngrpcSrv, err := servergrpc.NewGRPCServer(clientCtx, app, config)\\n    if err != nil {\\n    return nil, clientCtx, err\\n}\\n\\n\\t// Start the gRPC server in a goroutine. Note, the provided ctx will ensure\\n\\t// that the server is gracefully shut down.\\n\\tg.Go(func()\\n\\nerror {\\n    return servergrpc.StartGRPCServer(ctx, svrCtx.Logger.With(\\\"module\\\", \\\"grpc-server\\\"), config, grpcSrv)\\n})\\n\\nreturn grpcSrv, clientCtx, nil\\n}\\n\\nfunc startAPIServer(\\n\\tctx context.Context,\\n\\tg *errgroup.Group,\\n\\tcmtCfg *cmtcfg.Config,\\n\\tsvrCfg serverconfig.Config,\\n\\tclientCtx client.Context,\\n\\tsvrCtx *Context,\\n\\tapp types.Application,\\n\\thome string,\\n\\tgrpcSrv *grpc.Server,\\n\\tmetrics *telemetry.Metrics,\\n)\\n\\nerror {\\n    if !svrCfg.API.Enable {\\n    return nil\\n}\\n\\nclientCtx = clientCtx.WithHomeDir(home)\\n    apiSrv := api.New(clientCtx, svrCtx.Logger.With(\\\"module\\\", \\\"api-server\\\"), grpcSrv)\\n\\napp.RegisterAPIRoutes(apiSrv, svrCfg.API)\\n    if svrCfg.Telemetry.Enabled {\\n    apiSrv.SetTelemetry(metrics)\\n}\\n\\ng.Go(func()\\n\\nerror {\\n    return apiSrv.Start(ctx, svrCfg)\\n})\\n\\nreturn nil\\n}\\n\\nfunc startTelemetry(cfg serverconfig.Config) (*telemetry.Metrics, error) {\\n    if !cfg.Telemetry.Enabled {\\n    return nil, nil\\n}\\n\\nreturn telemetry.New(cfg.Telemetry)\\n}\\n\\n// wrapCPUProfile starts CPU profiling, if enabled, and executes the provided\\n// callbackFn in a separate goroutine, then will wait for that callback to\\n// return.\\n//\\n// NOTE: We expect the caller to handle graceful shutdown and signal handling.\\nfunc wrapCPUProfile(svrCtx *Context, callbackFn func()\\n\\nerror)\\n\\nerror {\\n    if cpuProfile := svrCtx.Viper.GetString(flagCPUProfile); cpuProfile != \\\"\\\" {\\n    f, err := os.Create(cpuProfile)\\n    if err != nil {\\n    return err\\n}\\n\\nsvrCtx.Logger.Info(\\\"starting CPU profiler\\\", \\\"profile\\\", cpuProfile)\\n    if err := pprof.StartCPUProfile(f); err != nil {\\n    return err\\n}\\n\\ndefer func() {\\n    svrCtx.Logger.Info(\\\"stopping CPU profiler\\\", \\\"profile\\\", cpuProfile)\\n\\npprof.StopCPUProfile()\\n    if err := f.Close(); err != nil {\\n    svrCtx.Logger.Info(\\\"failed to close cpu-profile file\\\", \\\"profile\\\", cpuProfile, \\\"err\\\", err.Error())\\n}\\n\\t\\n}()\\n}\\n    errCh := make(chan error)\\n\\ngo func() {\\n    errCh <- callbackFn()\\n}()\\n\\nreturn <-errCh\\n}\\n\\n// emitServerInfoMetrics emits server info related metrics using application telemetry.\\nfunc emitServerInfoMetrics() {\\n    var ls []metrics.Label\\n    versionInfo := version.NewInfo()\\n    if len(versionInfo.GoVersion) > 0 {\\n    ls = append(ls, telemetry.NewLabel(\\\"go\\\", versionInfo.GoVersion))\\n}\\n    if len(versionInfo.CosmosSdkVersion) > 0 {\\n    ls = append(ls, telemetry.NewLabel(\\\"version\\\", versionInfo.CosmosSdkVersion))\\n}\\n    if len(ls) == 0 {\\n    return\\n}\\n\\ntelemetry.SetGaugeWithLabels([]string{\\\"server\\\", \\\"info\\\"\\n}, 1, ls)\\n}\\n\\nfunc getCtx(svrCtx *Context, block bool) (*errgroup.Group, context.Context) {\\n    ctx, cancelFn := context.WithCancel(context.Background())\\n\\ng, ctx := errgroup.WithContext(ctx)\\n\\t// listen for quit signals so the calling parent process can gracefully exit\\n\\tListenForQuitSignals(g, block, cancelFn, svrCtx.Logger)\\n\\nreturn g, ctx\\n}\\n\\nfunc startApp(svrCtx *Context, appCreator types.AppCreator, opts StartCmdOptions) (app types.Application, cleanupFn func(), err error) {\\n    traceWriter, traceCleanupFn, err := setupTraceWriter(svrCtx)\\n    if err != nil {\\n    return app, traceCleanupFn, err\\n}\\n    home := svrCtx.Config.RootDir\\n\\tdb, err := opts.DBOpener(home, GetAppDBBackend(svrCtx.Viper))\\n    if err != nil {\\n    return app, traceCleanupFn, err\\n}\\n\\napp = appCreator(svrCtx.Logger, db, traceWriter, svrCtx.Viper)\\n\\ncleanupFn = func() {\\n    traceCleanupFn()\\n    if localErr := app.Close(); localErr != nil {\\n    svrCtx.Logger.Error(localErr.Error())\\n}\\n\\t\\n}\\n\\nreturn app, cleanupFn, nil\\n}\\n\\n\", \"Title: Node Client (Daemon)\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/advanced/node\\nContent: Synopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI.\\n\\nNode Client (Daemon)\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI.\\n\\nstart command\\nThe start command is defined in the /server folder of the Cosmos SDK. It is added to the root command of the full-node client in the main function and called by the end-user to start their node: As a reminder, the full-node is composed of three conceptual layers: the networking layer, the consensus layer and the application layer. The first two are generally bundled together in an entity called the consensus engine (CometBFT by default), while the third is the state-machine defined with the help of the Cosmos SDK. Currently, the Cosmos SDK uses CometBFT as the default consensus engine, meaning the start command is implemented to boot up a CometBFT node. The flow of the start command is pretty straightforward. First, it retrieves the config from the context in order to open the db (a leveldb instance by default). This db contains the latest known state of the application (empty if the application is started from the first time. With the db , the start command creates a new instance of the application using an appCreator function: Note that an appCreator is a function that fulfills the AppCreator signature: In practice, the constructor of the application is passed as the appCreator. Then, the instance of app is used to instantiate a new CometBFT node: The CometBFT node can be created with app because the latter satisfies the abci.Application interface (given that app extends baseapp ). As part of the node.New method, CometBFT makes sure that the height of the application (i.e. number of blocks since genesis) is equal to the height of the CometBFT node. The difference between these two heights should always be negative or null. If it is strictly negative, node.New will replay blocks until the height of the application reaches the height of the CometBFT node. Finally, if the height of the application is 0 , the CometBFT node will call InitChain on the application to initialize the state from the genesis file. Once the CometBFT node is instantiated and in sync with the application, the node can be started: package server\\n\\n// DONTCOVER\\n\\nimport (\\n    \\n\\t\\\"errors\\\"\\n    \\\"fmt\\\"\\n    \\\"net\\\"\\n    \\\"net/http\\\"\\n    \\\"os\\\"\\n    \\\"runtime/pprof\\\"\\n    \\\"time\\\"\\n    \\\"github.com/spf13/cobra\\\"\\n    \\\"github.com/tendermint/tendermint/abci/server\\\"\\n\\ttcmd \\\"github.com/tendermint/tendermint/cmd/tendermint/commands\\\"\\n    \\\"github.com/tendermint/tendermint/node\\\"\\n    \\\"github.com/tendermint/tendermint/p2p\\\"\\n\\tpvm \\\"github.com/tendermint/tendermint/privval\\\"\\n    \\\"github.com/tendermint/tendermint/proxy\\\"\\n    \\\"github.com/tendermint/tendermint/rpc/client/local\\\"\\n    \\\"google.golang.org/grpc\\\"\\n    \\\"google.golang.org/grpc/credentials/insecure\\\"\\n    \\\"cosmossdk.io/tools/rosetta\\\"\\n\\tcrgserver \\\"cosmossdk.io/tools/rosetta/lib/server\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client/flags\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/codec\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/api\\\"\\n\\tserverconfig \\\"github.com/cosmos/cosmos-sdk/server/config\\\"\\n\\tservergrpc \\\"github.com/cosmos/cosmos-sdk/server/grpc\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/types\\\"\\n\\tpruningtypes \\\"github.com/cosmos/cosmos-sdk/store/pruning/types\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/telemetry\\\"\\n\\tsdk \\\"github.com/cosmos/cosmos-sdk/types\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/types/mempool\\\"\\n)\\n\\nconst (\\n\\t// Tendermint full-node start flags\\n\\tflagWithTendermint     = \\\"with-tendermint\\\"\\n\\tflagAddress            = \\\"address\\\"\\n\\tflagTransport          = \\\"transport\\\"\\n\\tflagTraceStore         = \\\"trace-store\\\"\\n\\tflagCPUProfile         = \\\"cpu-profile\\\"\\n\\tFlagMinGasPrices       = \\\"minimum-gas-prices\\\"\\n\\tFlagHaltHeight         = \\\"halt-height\\\"\\n\\tFlagHaltTime           = \\\"halt-time\\\"\\n\\tFlagInterBlockCache    = \\\"inter-block-cache\\\"\\n\\tFlagUnsafeSkipUpgrades = \\\"unsafe-skip-upgrades\\\"\\n\\tFlagTrace              = \\\"trace\\\"\\n\\tFlagInvCheckPeriod     = \\\"inv-check-period\\\"\\n\\n\\tFlagPruning             = \\\"pruning\\\"\\n\\tFlagPruningKeepRecent   = \\\"pruning-keep-recent\\\"\\n\\tFlagPruningInterval     = \\\"pruning-interval\\\"\\n\\tFlagIndexEvents         = \\\"index-events\\\"\\n\\tFlagMinRetainBlocks     = \\\"min-retain-blocks\\\"\\n\\tFlagIAVLCacheSize       = \\\"iavl-cache-size\\\"\\n\\tFlagDisableIAVLFastNode = \\\"iavl-disable-fastnode\\\"\\n\\n\\t// state sync-related flags\\n\\tFlagStateSyncSnapshotInterval   = \\\"state-sync.snapshot-interval\\\"\\n\\tFlagStateSyncSnapshotKeepRecent = \\\"state-sync.snapshot-keep-recent\\\"\\n\\n\\t// api-related flags\\n\\tFlagAPIEnable             = \\\"api.enable\\\"\\n\\tFlagAPISwagger            = \\\"api.swagger\\\"\\n\\tFlagAPIAddress            = \\\"api.address\\\"\\n\\tFlagAPIMaxOpenConnections = \\\"api.max-open-connections\\\"\\n\\tFlagRPCReadTimeout        = \\\"api.rpc-read-timeout\\\"\\n\\tFlagRPCWriteTimeout       = \\\"api.rpc-write-timeout\\\"\\n\\tFlagRPCMaxBodyBytes       = \\\"api.rpc-max-body-bytes\\\"\\n\\tFlagAPIEnableUnsafeCORS   = \\\"api.enabled-unsafe-cors\\\"\\n\\n\\t// gRPC-related flags\\n\\tflagGRPCOnly       = \\\"grpc-only\\\"\\n\\tflagGRPCEnable     = \\\"grpc.enable\\\"\\n\\tflagGRPCAddress    = \\\"grpc.address\\\"\\n\\tflagGRPCWebEnable  = \\\"grpc-web.enable\\\"\\n\\tflagGRPCWebAddress = \\\"grpc-web.address\\\"\\n\\n\\t// mempool flags\\n\\tFlagMempoolMaxTxs = \\\"mempool.max-txs\\\"\\n)\\n\\n// StartCmd runs the service passed in, either stand-alone or in-process with\\n// Tendermint.\\nfunc StartCmd(appCreator types.AppCreator, defaultNodeHome string) *cobra.Command {\\n    cmd := &cobra.Command{\\n    Use:   \\\"start\\\",\\n    Short: \\\"Run the full node\\\",\\n    Long: `Run the full node application with Tendermint in or out of process. By\\ndefault, the application will run with Tendermint in process.\\n\\nPruning options can be provided via the '--pruning' flag or alternatively with '--pruning-keep-recent', and\\n'pruning-interval' together.\\n\\nFor '--pruning' the options are as follows:\\n\\ndefault: the last 362880 states are kept, pruning at 10 block intervals\\nnothing: all historic states will be saved, nothing will be deleted (i.e. archiving node)\\n\\neverything: 2 latest states will be kept; pruning at 10 block intervals.\\ncustom: allow pruning options to be manually specified through 'pruning-keep-recent', and 'pruning-interval'\\n\\nNode halting configurations exist in the form of two flags: '--halt-height' and '--halt-time'. During\\nthe ABCI Commit phase, the node will check if the current block height is greater than or equal to\\nthe halt-height or if the current block time is greater than or equal to the halt-time. If so, the\\nnode will attempt to gracefully shutdown and the block will not be committed. In addition, the node\\nwill not be able to commit subsequent blocks.\\n\\nFor profiling and benchmarking purposes, CPU profiling can be enabled via the '--cpu-profile' flag\\nwhich accepts a path for the resulting pprof file.\\n\\nThe node may be started in a 'query only' mode where only the gRPC and JSON HTTP\\nAPI services are enabled via the 'grpc-only' flag. In this mode, Tendermint is\\nbypassed and can be used when legacy queries are needed after an on-chain upgrade\\nis performed. Note, when enabled, gRPC will also be automatically enabled.\\n`,\\n    PreRunE: func(cmd *cobra.Command, _ []string)\\n\\nerror {\\n    serverCtx := GetServerContextFromCmd(cmd)\\n\\n\\t\\t\\t// Bind flags to the Context's Viper so the app construction can set\\n\\t\\t\\t// options accordingly.\\n    if err := serverCtx.Viper.BindPFlags(cmd.Flags()); err != nil {\\n    return err\\n}\\n\\n\\t\\t\\t_, err := GetPruningOptionsFromFlags(serverCtx.Viper)\\n\\nreturn err\\n},\\n    RunE: func(cmd *cobra.Command, _ []string)\\n\\nerror {\\n    serverCtx := GetServerContextFromCmd(cmd)\\n\\nclientCtx, err := client.GetClientQueryContext(cmd)\\n    if err != nil {\\n    return err\\n}\\n\\nwithTM, _ := cmd.Flags().GetBool(flagWithTendermint)\\n    if !withTM {\\n    serverCtx.Logger.Info(\\\"starting ABCI without Tendermint\\\")\\n\\nreturn startStandAlone(serverCtx, appCreator)\\n}\\n\\n\\t\\t\\t// amino is needed here for backwards compatibility of REST routes\\n\\t\\t\\terr = startInProcess(serverCtx, clientCtx, appCreator)\\n\\nerrCode, ok := err.(ErrorCode)\\n    if !ok {\\n    return err\\n}\\n\\nserverCtx.Logger.Debug(fmt.Sprintf(\\\"received quit signal: %d\\\", errCode.Code))\\n\\nreturn nil\\n},\\n}\\n\\ncmd.Flags().String(flags.FlagHome, defaultNodeHome, \\\"The application home directory\\\")\\n\\ncmd.Flags().Bool(flagWithTendermint, true, \\\"Run abci app embedded in-process with tendermint\\\")\\n\\ncmd.Flags().String(flagAddress, \\\"tcp://0.0.0.0:26658\\\", \\\"Listen address\\\")\\n\\ncmd.Flags().String(flagTransport, \\\"socket\\\", \\\"Transport protocol: socket, grpc\\\")\\n\\ncmd.Flags().String(flagTraceStore, \\\"\\\", \\\"Enable KVStore tracing to an output file\\\")\\n\\ncmd.Flags().String(FlagMinGasPrices, \\\"\\\", \\\"Minimum gas prices to accept for transactions; Any fee in a tx must meet this minimum (e.g. 0.01photino;0.0001stake)\\\")\\n\\ncmd.Flags().IntSlice(FlagUnsafeSkipUpgrades, []int{\\n}, \\\"Skip a set of upgrade heights to continue the old binary\\\")\\n\\ncmd.Flags().Uint64(FlagHaltHeight, 0, \\\"Block height at which to gracefully halt the chain and shutdown the node\\\")\\n\\ncmd.Flags().Uint64(FlagHaltTime, 0, \\\"Minimum block time (in Unix seconds)\\n\\nat which to gracefully halt the chain and shutdown the node\\\")\\n\\ncmd.Flags().Bool(FlagInterBlockCache, true, \\\"Enable inter-block caching\\\")\\n\\ncmd.Flags().String(flagCPUProfile, \\\"\\\", \\\"Enable CPU profiling and write to the provided file\\\")\\n\\ncmd.Flags().Bool(FlagTrace, false, \\\"Provide full stack traces for errors in ABCI Log\\\")\\n\\ncmd.Flags().String(FlagPruning, pruningtypes.PruningOptionDefault, \\\"Pruning strategy (default|nothing|everything|custom)\\\")\\n\\ncmd.Flags().Uint64(FlagPruningKeepRecent, 0, \\\"Number of recent heights to keep on disk (ignored if pruning is not 'custom')\\\")\\n\\ncmd.Flags().Uint64(FlagPruningInterval, 0, \\\"Height interval at which pruned heights are removed from disk (ignored if pruning is not 'custom')\\\")\\n\\ncmd.Flags().Uint(FlagInvCheckPeriod, 0, \\\"Assert registered invariants every N blocks\\\")\\n\\ncmd.Flags().Uint64(FlagMinRetainBlocks, 0, \\\"Minimum block height offset during ABCI commit to prune Tendermint blocks\\\")\\n\\ncmd.Flags().Bool(FlagAPIEnable, false, \\\"Define if the API server should be enabled\\\")\\n\\ncmd.Flags().Bool(FlagAPISwagger, false, \\\"Define if swagger documentation should automatically be registered (Note: the API must also be enabled)\\\")\\n\\ncmd.Flags().String(FlagAPIAddress, serverconfig.DefaultAPIAddress, \\\"the API server address to listen on\\\")\\n\\ncmd.Flags().Uint(FlagAPIMaxOpenConnections, 1000, \\\"Define the number of maximum open connections\\\")\\n\\ncmd.Flags().Uint(FlagRPCReadTimeout, 10, \\\"Define the Tendermint RPC read timeout (in seconds)\\\")\\n\\ncmd.Flags().Uint(FlagRPCWriteTimeout, 0, \\\"Define the Tendermint RPC write timeout (in seconds)\\\")\\n\\ncmd.Flags().Uint(FlagRPCMaxBodyBytes, 1000000, \\\"Define the Tendermint maximum response body (in bytes)\\\")\\n\\ncmd.Flags().Bool(FlagAPIEnableUnsafeCORS, false, \\\"Define if CORS should be enabled (unsafe - use it at your own risk)\\\")\\n\\ncmd.Flags().Bool(flagGRPCOnly, false, \\\"Start the node in gRPC query only mode (no Tendermint process is started)\\\")\\n\\ncmd.Flags().Bool(flagGRPCEnable, true, \\\"Define if the gRPC server should be enabled\\\")\\n\\ncmd.Flags().String(flagGRPCAddress, serverconfig.DefaultGRPCAddress, \\\"the gRPC server address to listen on\\\")\\n\\ncmd.Flags().Bool(flagGRPCWebEnable, true, \\\"Define if the gRPC-Web server should be enabled. (Note: gRPC must also be enabled)\\\")\\n\\ncmd.Flags().String(flagGRPCWebAddress, serverconfig.DefaultGRPCWebAddress, \\\"The gRPC-Web server address to listen on\\\")\\n\\ncmd.Flags().Uint64(FlagStateSyncSnapshotInterval, 0, \\\"State sync snapshot interval\\\")\\n\\ncmd.Flags().Uint32(FlagStateSyncSnapshotKeepRecent, 2, \\\"State sync snapshot to keep\\\")\\n\\ncmd.Flags().Bool(FlagDisableIAVLFastNode, false, \\\"Disable fast node for IAVL tree\\\")\\n\\ncmd.Flags().Int(FlagMempoolMaxTxs, mempool.DefaultMaxTx, \\\"Sets MaxTx value for the app-side mempool\\\")\\n\\n\\t// add support for all Tendermint-specific command line options\\n\\ttcmd.AddNodeFlags(cmd)\\n\\nreturn cmd\\n}\\n\\nfunc startStandAlone(ctx *Context, appCreator types.AppCreator)\\n\\nerror {\\n    addr := ctx.Viper.GetString(flagAddress)\\n    transport := ctx.Viper.GetString(flagTransport)\\n    home := ctx.Viper.GetString(flags.FlagHome)\\n\\ndb, err := openDB(home, GetAppDBBackend(ctx.Viper))\\n    if err != nil {\\n    return err\\n}\\n    traceWriterFile := ctx.Viper.GetString(flagTraceStore)\\n\\ntraceWriter, err := openTraceWriter(traceWriterFile)\\n    if err != nil {\\n    return err\\n}\\n    app := appCreator(ctx.Logger, db, traceWriter, ctx.Viper)\\n\\nconfig, err := serverconfig.GetConfig(ctx.Viper)\\n    if err != nil {\\n    return err\\n}\\n\\n\\t_, err = startTelemetry(config)\\n    if err != nil {\\n    return err\\n}\\n\\nsvr, err := server.NewServer(addr, transport, app)\\n    if err != nil {\\n    return fmt.Errorf(\\\"error creating listener: %v\\\", err)\\n}\\n\\nsvr.SetLogger(ctx.Logger.With(\\\"module\\\", \\\"abci-server\\\"))\\n\\nerr = svr.Start()\\n    if err != nil {\\n    fmt.Println(err.Error())\\n\\nos.Exit(1)\\n}\\n\\ndefer func() {\\n    if err = svr.Stop(); err != nil {\\n    fmt.Println(err.Error())\\n\\nos.Exit(1)\\n}\\n\\t\\n}()\\n\\n\\t// Wait for SIGINT or SIGTERM signal\\n\\treturn WaitForQuitSignals()\\n}\\n\\nfunc startInProcess(ctx *Context, clientCtx client.Context, appCreator types.AppCreator)\\n\\nerror {\\n    cfg := ctx.Config\\n    home := cfg.RootDir\\n\\tvar cpuProfileCleanup func()\\n    if cpuProfile := ctx.Viper.GetString(flagCPUProfile); cpuProfile != \\\"\\\" {\\n    f, err := os.Create(cpuProfile)\\n    if err != nil {\\n    return err\\n}\\n\\nctx.Logger.Info(\\\"starting CPU profiler\\\", \\\"profile\\\", cpuProfile)\\n    if err := pprof.StartCPUProfile(f); err != nil {\\n    return err\\n}\\n\\ncpuProfileCleanup = func() {\\n    ctx.Logger.Info(\\\"stopping CPU profiler\\\", \\\"profile\\\", cpuProfile)\\n\\npprof.StopCPUProfile()\\n    if err := f.Close(); err != nil {\\n    ctx.Logger.Info(\\\"failed to close cpu-profile file\\\", \\\"profile\\\", cpuProfile, \\\"err\\\", err.Error())\\n}\\n\\t\\n}\\n\\t\\n}\\n\\ndb, err := openDB(home, GetAppDBBackend(ctx.Viper))\\n    if err != nil {\\n    return err\\n}\\n    traceWriterFile := ctx.Viper.GetString(flagTraceStore)\\n\\ntraceWriter, err := openTraceWriter(traceWriterFile)\\n    if err != nil {\\n    return err\\n}\\n\\n\\t// Clean up the traceWriter in the cpuProfileCleanup routine that is invoked\\n\\t// when the server is shutting down.\\n    fn := cpuProfileCleanup\\n\\tcpuProfileCleanup = func() {\\n    if fn != nil {\\n    fn()\\n}\\n\\n\\t\\t// if flagTraceStore is not used then traceWriter is nil\\n    if traceWriter != nil {\\n    if err = traceWriter.Close(); err != nil {\\n    ctx.Logger.Error(\\\"failed to close trace writer\\\", \\\"err\\\", err)\\n}\\n\\t\\n}\\n\\t\\n}\\n\\nconfig, err := serverconfig.GetConfig(ctx.Viper)\\n    if err != nil {\\n    return err\\n}\\n    if err := config.ValidateBasic(); err != nil {\\n    return err\\n}\\n    app := appCreator(ctx.Logger, db, traceWriter, ctx.Viper)\\n\\nnodeKey, err := p2p.LoadOrGenNodeKey(cfg.NodeKeyFile())\\n    if err != nil {\\n    return err\\n}\\n    genDocProvider := node.DefaultGenesisDocProviderFunc(cfg)\\n\\nvar (\\n\\t\\ttmNode   *node.Node\\n\\t\\tgRPCOnly = ctx.Viper.GetBool(flagGRPCOnly)\\n\\t)\\n    if gRPCOnly {\\n    ctx.Logger.Info(\\\"starting node in gRPC only mode; Tendermint is disabled\\\")\\n\\nconfig.GRPC.Enable = true\\n}\\n\\nelse {\\n    ctx.Logger.Info(\\\"starting node with ABCI Tendermint in-process\\\")\\n\\ntmNode, err = node.NewNode(\\n\\t\\t\\tcfg,\\n\\t\\t\\tpvm.LoadOrGenFilePV(cfg.PrivValidatorKeyFile(), cfg.PrivValidatorStateFile()),\\n\\t\\t\\tnodeKey,\\n\\t\\t\\tproxy.NewLocalClientCreator(app),\\n\\t\\t\\tgenDocProvider,\\n\\t\\t\\tnode.DefaultDBProvider,\\n\\t\\t\\tnode.DefaultMetricsProvider(cfg.Instrumentation),\\n\\t\\t\\tctx.Logger,\\n\\t\\t)\\n    if err != nil {\\n    return err\\n}\\n    if err := tmNode.Start(); err != nil {\\n    return err\\n}\\n\\t\\n}\\n\\n\\t// Add the tx service to the gRPC router. We only need to register this\\n\\t// service if API or gRPC is enabled, and avoid doing so in the general\\n\\t// case, because it spawns a new local tendermint RPC client.\\n    if (config.API.Enable || config.GRPC.Enable) && tmNode != nil {\\n\\t\\t// re-assign for making the client available below\\n\\t\\t// do not use := to avoid shadowing clientCtx\\n\\t\\tclientCtx = clientCtx.WithClient(local.New(tmNode))\\n\\napp.RegisterTxService(clientCtx)\\n\\napp.RegisterTendermintService(clientCtx)\\n\\napp.RegisterNodeService(clientCtx)\\n}\\n\\nmetrics, err := startTelemetry(config)\\n    if err != nil {\\n    return err\\n}\\n\\nvar apiSrv *api.Server\\n    if config.API.Enable {\\n    genDoc, err := genDocProvider()\\n    if err != nil {\\n    return err\\n}\\n    clientCtx := clientCtx.WithHomeDir(home).WithChainID(genDoc.ChainID)\\n    if config.GRPC.Enable {\\n\\t\\t\\t_, port, err := net.SplitHostPort(config.GRPC.Address)\\n    if err != nil {\\n    return err\\n}\\n    maxSendMsgSize := config.GRPC.MaxSendMsgSize\\n    if maxSendMsgSize == 0 {\\n    maxSendMsgSize = serverconfig.DefaultGRPCMaxSendMsgSize\\n}\\n    maxRecvMsgSize := config.GRPC.MaxRecvMsgSize\\n    if maxRecvMsgSize == 0 {\\n    maxRecvMsgSize = serverconfig.DefaultGRPCMaxRecvMsgSize\\n}\\n    grpcAddress := fmt.Sprintf(\\\"127.0.0.1:%s\\\", port)\\n\\n\\t\\t\\t// If grpc is enabled, configure grpc client for grpc gateway.\\n\\t\\t\\tgrpcClient, err := grpc.Dial(\\n\\t\\t\\t\\tgrpcAddress,\\n\\t\\t\\t\\tgrpc.WithTransportCredentials(insecure.NewCredentials()),\\n\\t\\t\\t\\tgrpc.WithDefaultCallOptions(\\n\\t\\t\\t\\t\\tgrpc.ForceCodec(codec.NewProtoCodec(clientCtx.InterfaceRegistry).GRPCCodec()),\\n\\t\\t\\t\\t\\tgrpc.MaxCallRecvMsgSize(maxRecvMsgSize),\\n\\t\\t\\t\\t\\tgrpc.MaxCallSendMsgSize(maxSendMsgSize),\\n\\t\\t\\t\\t),\\n\\t\\t\\t)\\n    if err != nil {\\n    return err\\n}\\n\\nclientCtx = clientCtx.WithGRPCClient(grpcClient)\\n\\nctx.Logger.Debug(\\\"grpc client assigned to client context\\\", \\\"target\\\", grpcAddress)\\n}\\n\\napiSrv = api.New(clientCtx, ctx.Logger.With(\\\"module\\\", \\\"api-server\\\"))\\n\\napp.RegisterAPIRoutes(apiSrv, config.API)\\n    if config.Telemetry.Enabled {\\n    apiSrv.SetTelemetry(metrics)\\n}\\n    errCh := make(chan error)\\n\\ngo func() {\\n    if err := apiSrv.Start(config); err != nil {\\n    errCh <- err\\n}\\n\\t\\n}()\\n\\nselect {\\n    case err := <-errCh:\\n\\t\\t\\treturn err\\n    case <-time.After(types.ServerStartTime): // assume server started successfully\\n}\\n\\t\\n}\\n\\nvar (\\n\\t\\tgrpcSrv    *grpc.Server\\n\\t\\tgrpcWebSrv *http.Server\\n\\t)\\n    if config.GRPC.Enable {\\n    grpcSrv, err = servergrpc.StartGRPCServer(clientCtx, app, config.GRPC)\\n    if err != nil {\\n    return err\\n}\\n\\ndefer grpcSrv.Stop()\\n    if config.GRPCWeb.Enable {\\n    grpcWebSrv, err = servergrpc.StartGRPCWeb(grpcSrv, config)\\n    if err != nil {\\n    ctx.Logger.Error(\\\"failed to start grpc-web http server: \\\", err)\\n\\nreturn err\\n}\\n\\ndefer func() {\\n    if err := grpcWebSrv.Close(); err != nil {\\n    ctx.Logger.Error(\\\"failed to close grpc-web http server: \\\", err)\\n}\\n\\t\\n}()\\n}\\n\\t\\n}\\n\\n\\t// At this point it is safe to block the process if we're in gRPC only mode as\\n\\t// we do not need to start Rosetta or handle any Tendermint related processes.\\n    if gRPCOnly {\\n\\t\\t// wait for signal capture and gracefully return\\n\\t\\treturn WaitForQuitSignals()\\n}\\n\\nvar rosettaSrv crgserver.Server\\n    if config.Rosetta.Enable {\\n    offlineMode := config.Rosetta.Offline\\n\\n\\t\\t// If GRPC is not enabled rosetta cannot work in online mode, so we throw an error.\\n    if !config.GRPC.Enable && !offlineMode {\\n    return errors.New(\\\"'grpc' must be enable in online mode for Rosetta to work\\\")\\n}\\n\\nminGasPrices, err := sdk.ParseDecCoins(config.MinGasPrices)\\n    if err != nil {\\n    ctx.Logger.Error(\\\"failed to parse minimum-gas-prices: \\\", err)\\n\\nreturn err\\n}\\n    conf := &rosetta.Config{\\n    Blockchain:          config.Rosetta.Blockchain,\\n    Network:             config.Rosetta.Network,\\n    TendermintRPC:       ctx.Config.RPC.ListenAddress,\\n    GRPCEndpoint:        config.GRPC.Address,\\n    Addr:                config.Rosetta.Address,\\n    Retries:             config.Rosetta.Retries,\\n    Offline:             offlineMode,\\n    GasToSuggest:        config.Rosetta.GasToSuggest,\\n    EnableFeeSuggestion: config.Rosetta.EnableFeeSuggestion,\\n    GasPrices:           minGasPrices.Sort(),\\n    Codec:               clientCtx.Codec.(*codec.ProtoCodec),\\n    InterfaceRegistry:   clientCtx.InterfaceRegistry,\\n}\\n\\nrosettaSrv, err = rosetta.ServerFromConfig(conf)\\n    if err != nil {\\n    return err\\n}\\n    errCh := make(chan error)\\n\\ngo func() {\\n    if err := rosettaSrv.Start(); err != nil {\\n    errCh <- err\\n}\\n\\t\\n}()\\n\\nselect {\\n    case err := <-errCh:\\n\\t\\t\\treturn err\\n    case <-time.After(types.ServerStartTime): // assume server started successfully\\n}\\n\\t\\n}\\n\\ndefer func() {\\n    if tmNode != nil && tmNode.IsRunning() {\\n\\t\\t\\t_ = tmNode.Stop()\\n}\\n    if cpuProfileCleanup != nil {\\n    cpuProfileCleanup()\\n}\\n    if apiSrv != nil {\\n\\t\\t\\t_ = apiSrv.Close()\\n}\\n\\nctx.Logger.Info(\\\"exiting...\\\")\\n}()\\n\\n\\t// wait for signal capture and gracefully return\\n\\treturn WaitForQuitSignals()\\n}\\n\\nfunc startTelemetry(cfg serverconfig.Config) (*telemetry.Metrics, error) {\\n    if !cfg.Telemetry.Enabled {\\n    return nil, nil\\n}\\n\\nreturn telemetry.New(cfg.Telemetry)\\n}\\n\\n\", \"Title: Overview of a Cosmos SDK Application\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/beginner/overview-app\\nContent: Synopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\nOverview of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\nNode Client\\nThe Daemon, or Full-Node Client , is the core process of a Cosmos SDK-based blockchain. Participants in the network run this process to initialize their state-machine, connect with other full-nodes, and update their state-machine as new blocks come in. ^  +-------------------------------+  ^\\n                |  |                               |  |\\n                |  |  State-machine = Application  |  |\\n                |  |                               |  |   Built with Cosmos SDK\\n                |  |            ^      +           |  |\\n                |  +----------- | ABCI | ----------+  v\\n                |  |            +      v           |  ^\\n                |  |                               |  |\\nBlockchain Node |  |           Consensus           |  |\\n                |  |                               |  |\\n                |  +-------------------------------+  |   CometBFT\\n                |  |                               |  |\\n                |  |           Networking          |  |\\n                |  |                               |  |\\n                v  +-------------------------------+  v\\n\\n\"]",
            "additional_kwargs": {},
            "response_metadata": {},
            "type": "tool",
            "name": "SearchCosmosDocumentation",
            "id": "2179a6fd-0b3f-46c2-be22-8e6d1174cf27",
            "tool_call_id": "call_cOYl9CuzVBe7rKwdat9qUzgb",
            "artifact": null,
            "status": "success"
        },
        {
            "content": "",
            "additional_kwargs": {
                "tool_calls": [
                    {
                        "id": "call_x3PcTTbPOziFbCe1U3FYrqEp",
                        "function": {
                            "arguments": "{\"query\": \"pprof endpoint Cosmos SDK debug/pprof\"}",
                            "name": "SearchCosmosDocumentation"
                        },
                        "type": "function"
                    }
                ],
                "refusal": null
            },
            "response_metadata": {
                "token_usage": {
                    "completion_tokens": 866,
                    "prompt_tokens": 33819,
                    "total_tokens": 34685,
                    "completion_tokens_details": {
                        "accepted_prediction_tokens": 0,
                        "audio_tokens": 0,
                        "reasoning_tokens": 832,
                        "rejected_prediction_tokens": 0
                    },
                    "prompt_tokens_details": {
                        "audio_tokens": 0,
                        "cached_tokens": 0
                    }
                },
                "model_name": "o3-2025-04-16",
                "system_fingerprint": null,
                "id": "chatcmpl-CPvtT3DA5PONlaDS2qiEcaQph9HyY",
                "service_tier": "default",
                "finish_reason": "tool_calls",
                "logprobs": null
            },
            "type": "ai",
            "name": null,
            "id": "run--f3ad7021-5265-40dc-b7e7-40a5042c9001-0",
            "example": false,
            "tool_calls": [
                {
                    "name": "SearchCosmosDocumentation",
                    "args": {
                        "query": "pprof endpoint Cosmos SDK debug/pprof"
                    },
                    "id": "call_x3PcTTbPOziFbCe1U3FYrqEp",
                    "type": "tool_call"
                }
            ],
            "invalid_tool_calls": [],
            "usage_metadata": {
                "input_tokens": 33819,
                "output_tokens": 866,
                "total_tokens": 34685,
                "input_token_details": {
                    "audio": 0,
                    "cache_read": 0
                },
                "output_token_details": {
                    "audio": 0,
                    "reasoning": 832
                }
            }
        },
        {
            "content": "[\"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: Each module can be seen as a little state-machine. Developers need to define the subset of the state handled by the module, as well as custom message types that modify the state ( Note: messages are extracted from transactions by baseapp ). In general, each module declares its own KVStore in the multistore to persist the subset of the state it defines. Most developers will need to access other 3rd party modules when building their own modules. Given that the Cosmos SDK is an open framework, some of the modules may be malicious, which means there is a need for security principles to reason about inter-module interactions. These principles are based on object-capabilities . In practice, this means that instead of having each module keep an access control list for other modules, each module implements special objects called keepers that can be passed to other modules to grant a pre-defined set of capabilities. Cosmos SDK modules are defined in the x/ folder of the Cosmos SDK. Some core modules include: x/auth : Used to manage accounts and signatures. x/bank : Used to enable tokens and token transfers. x/staking + x/slashing : Used to build Proof-Of-Stake blockchains. In addition to the already existing modules in x/ , that anyone can use in their app, the Cosmos SDK lets you build your own custom modules. You can check an example of that in the tutorial.\\n\\n\", \"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: Each module can be seen as a little state-machine. Developers need to define the subset of the state handled by the module, as well as custom message types that modify the state ( Note: messages are extracted from transactions by baseapp ). In general, each module declares its own KVStore in the multistore to persist the subset of the state it defines. Most developers will need to access other 3rd party modules when building their own modules. Given that the Cosmos SDK is an open framework, some of the modules may be malicious, which means there is a need for security principles to reason about inter-module interactions. These principles are based on object-capabilities . In practice, this means that instead of having each module keep an access control list for other modules, each module implements special objects called keepers that can be passed to other modules to grant a pre-defined set of capabilities. Cosmos SDK modules are defined in the x/ folder of the Cosmos SDK. Some core modules include: x/auth : Used to manage accounts and signatures. x/bank : Used to enable tokens and token transfers. x/staking + x/slashing : Used to build Proof-Of-Stake blockchains. In addition to the already existing modules in x/ , that anyone can use in their app, the Cosmos SDK lets you build your own custom modules. You can check an example of that in the tutorial.\\n\\n\", \"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: Each module can be seen as a little state-machine. Developers need to define the subset of the state handled by the module, as well as custom message types that modify the state ( Note: messages are extracted from transactions by baseapp ). In general, each module declares its own KVStore in the multistore to persist the subset of the state it defines. Most developers will need to access other 3rd party modules when building their own modules. Given that the Cosmos SDK is an open framework, some of the modules may be malicious, which means there is a need for security principles to reason about inter-module interactions. These principles are based on object-capabilities . In practice, this means that instead of having each module keep an access control list for other modules, each module implements special objects called keepers that can be passed to other modules to grant a pre-defined set of capabilities. Cosmos SDK modules are defined in the x/ folder of the Cosmos SDK. Some core modules include: x/auth : Used to manage accounts and signatures. x/bank : Used to enable tokens and token transfers. x/staking + x/slashing : Used to build Proof-of-Stake blockchains. In addition to the already existing modules in x/ , which anyone can use in their app, the Cosmos SDK lets you build your own custom modules. You can check an example of that in the tutorial.\\n\\n\", \"Title: What is the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/intro/overview\\nContent: The Cosmos SDK is an open-source framework for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains. We envision the Cosmos SDK as the npm-like framework to build secure blockchain applications on top of CometBFT . SDK-based blockchains are built out of composable modules , most of which are open-source and readily available for any developers to use. Anyone can create a module for the Cosmos SDK, and integrating already-built modules is as simple as importing them into your blockchain application. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model.\\n\\nWhat is the Cosmos SDK\\nThe Cosmos SDK is an open-source framework for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains. We envision the Cosmos SDK as the npm-like framework to build secure blockchain applications on top of CometBFT . SDK-based blockchains are built out of composable modules , most of which are open-source and readily available for any developers to use. Anyone can create a module for the Cosmos SDK, and integrating already-built modules is as simple as importing them into your blockchain application. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model.\\n\\nWhy the Cosmos SDK\\nThe Cosmos SDK is the most advanced framework for building custom application-specific blockchains today. Here are a few reasons why you might want to consider building your decentralized application with the Cosmos SDK: The default consensus engine available within the Cosmos SDK is CometBFT . CometBFT is the most (and only) mature BFT consensus engine in existence. It is widely used across the industry and is considered the gold standard consensus engine for building Proof-of-Stake systems. The Cosmos SDK is open-source and designed to make it easy to build blockchains out of composable modules . As the ecosystem of open-source Cosmos SDK modules grows, it will become increasingly easier to build complex decentralized platforms with it. The Cosmos SDK is inspired by capabilities-based security, and informed by years of wrestling with blockchain state-machines. This makes the Cosmos SDK a very secure environment to build blockchains. Most importantly, the Cosmos SDK has already been used to build many application-specific blockchains that are already in production. Among others, we can cite Cosmos Hub , IRIS Hub , Binance Chain , Terra or Kava . Many more are building on the Cosmos SDK.\\n\\n\", \"Title: What is the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/intro/overview\\nContent: The Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhat is the Cosmos SDK\\nThe Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhy the Cosmos SDK\\nThe Cosmos SDK is the most advanced framework for building custom modular application-specific blockchains today. Here are a few reasons why you might want to consider building your decentralized application with the Cosmos SDK: It allows you to plug and play and customize your consensus layer. As above you can use Rollkit and Celestia as your consensus and data availability layer. This offers a lot of flexibility and customisation. Previously the default consensus engine available within the Cosmos SDK is CometBFT . CometBFT is the most (and only) mature BFT consensus engine in existence. It is widely used across the industry and is considered the gold standard consensus engine for building Proof-of-Stake systems. The Cosmos SDK is open-source and designed to make it easy to build blockchains out of composable modules . As the ecosystem of open-source Cosmos SDK modules grows, it will become increasingly easier to build complex decentralized platforms with it. The Cosmos SDK is inspired by capabilities-based security, and informed by years of wrestling with blockchain state-machines. This makes the Cosmos SDK a very secure environment to build blockchains. Most importantly, the Cosmos SDK has already been used to build many application-specific blockchains that are already in production. Among others, we can cite Cosmos Hub , IRIS Hub , Binance Chain , Terra or Kava . Many more are building on the Cosmos SDK.\\n\\n\", \"Title: What is the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/intro/overview\\nContent: The Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhat is the Cosmos SDK\\nThe Cosmos SDK is an open-source toolkit for building multi-asset public Proof-of-Stake (PoS) blockchains , like the Cosmos Hub, as well as permissioned Proof-of-Authority (PoA) blockchains. Blockchains built with the Cosmos SDK are generally referred to as application-specific blockchains. The goal of the Cosmos SDK is to allow developers to easily create custom blockchains from scratch that can natively interoperate with other blockchains.\\nWe further this modular approach by allowing developers to plug and play with different consensus engines this can range from the CometBFT or Rollkit. SDK-based blockchains have the choice to use the predefined modules or to build their own modules. What this means is that developers can build a blockchain that is tailored to their specific use case, without having to worry about the low-level details of building a blockchain from scratch. Predefined modules include staking, governance, and token issuance, among others. What's more, the Cosmos SDK is a capabilities-based system that allows developers to better reason about the security of interactions between modules. For a deeper look at capabilities, jump to Object-Capability Model. How you can look at this is if we imagine that the SDK is like a lego kit. You can choose to build the basic house from the instructions or you can choose to modify your house and add more floors, more doors, more windows. The choice is yours.\\n\\nWhy the Cosmos SDK\\nThe Cosmos SDK is the most advanced framework for building custom modular application-specific blockchains today. Here are a few reasons why you might want to consider building your decentralized application with the Cosmos SDK: It allows you to plug and play and customize your consensus layer. As above you can use Rollkit and Celestia as your consensus and data availability layer. This offers a lot of flexibility and customisation. Previously the default consensus engine available within the Cosmos SDK is CometBFT . CometBFT is the most (and only) mature BFT consensus engine in existence. It is widely used across the industry and is considered the gold standard consensus engine for building Proof-of-Stake systems. The Cosmos SDK is open-source and designed to make it easy to build blockchains out of composable modules . As the ecosystem of open-source Cosmos SDK modules grows, it will become increasingly easier to build complex decentralized platforms with it. The Cosmos SDK is inspired by capabilities-based security, and informed by years of wrestling with blockchain state-machines. This makes the Cosmos SDK a very secure environment to build blockchains. Most importantly, the Cosmos SDK has already been used to build many application-specific blockchains that are already in production. Among others, we can cite Cosmos Hub , IRIS Hub , Binance Chain , Terra or Kava . Many more are building on the Cosmos SDK.\\n\\n\", \"Title: gRPC-gateway REST Endpoints\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/beginner/overview-app\\nContent: Some external clients may not wish to use gRPC. In this case, the Cosmos SDK provides a gRPC gateway service, which exposes each gRPC service as a corresponding REST endpoint. Please refer to the grpc-gateway documentation to learn more. The REST endpoints are defined in the Protobuf files, along with the gRPC services, using Protobuf annotations. Modules that want to expose REST queries should add google.api.http annotations to their rpc methods. By default, all REST endpoints defined in the SDK have a URL starting with the /cosmos/ prefix. The Cosmos SDK also provides a development endpoint to generate Swagger definition files for these REST endpoints. This endpoint can be enabled inside the app.toml config file, under the api.swagger key.\\n\\nOverview of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\nOverview of a Cosmos SDK Application\\nOverview of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\n\", \"Title: gRPC-gateway REST Endpoints\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/beginner/app-anatomy\\nContent: Some external clients may not wish to use gRPC. In this case, the Cosmos SDK provides a gRPC gateway service, which exposes each gRPC service as a corresponding REST endpoint. Please refer to the grpc-gateway documentation to learn more. The REST endpoints are defined in the Protobuf files, along with the gRPC services, using Protobuf annotations. Modules that want to expose REST queries should add google.api.http annotations to their rpc methods. By default, all REST endpoints defined in the SDK have a URL starting with the /cosmos/ prefix. The Cosmos SDK also provides a development endpoint to generate Swagger definition files for these REST endpoints. This endpoint can be enabled inside the app.toml config file, under the api.swagger key.\\n\\nAnatomy of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\nAnatomy of a Cosmos SDK Application\\nAnatomy of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\n\", \"Title: gRPC-gateway REST Endpoints\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/beginner/app-anatomy\\nContent: Some external clients may not wish to use gRPC. In this case, the Cosmos SDK provides a gRPC gateway service, which exposes each gRPC service as a corresponding REST endpoint. Please refer to the grpc-gateway documentation to learn more. The REST endpoints are defined in the Protobuf files, along with the gRPC services, using Protobuf annotations. Modules that want to expose REST queries should add google.api.http annotations to their rpc methods. By default, all REST endpoints defined in the SDK have a URL starting with the /cosmos/ prefix. The Cosmos SDK also provides a development endpoint to generate Swagger definition files for these REST endpoints. This endpoint can be enabled inside the app.toml config file, under the api.swagger key.\\n\\nAnatomy of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\nAnatomy of a Cosmos SDK Application\\nAnatomy of a Cosmos SDK Application\\nSynopsis This document describes the core parts of a Cosmos SDK application, represented throughout the document as a placeholder application named app.\\n\\n\", \"Title: Using the REST Endpoints\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/user/run-node/interact-node\\nContent: As described in the gRPC guide , all gRPC services on the Cosmos SDK are made available for more convenient REST-based queries through gRPC-gateway. The format of the URL path is based on the Protobuf service method's full-qualified name, but may contain small customizations so that final URLs look more idiomatic. For example, the REST endpoint for the cosmos.bank.v1beta1.Query/AllBalances method is GET /cosmos/bank/v1beta1/balances/{address} . Request arguments are passed as query parameters. Note that the REST endpoints are not enabled by default. To enable them, edit the api section of your  ~/.simapp/config/app.toml file: # Enable defines if the API server should be enabled.\\nenable = true\\n\\nAs described in the gRPC guide , all gRPC services on the Cosmos SDK are made available for more convenient REST-based queries through gRPC-gateway. The format of the URL path is based on the Protobuf service method's full-qualified name, but may contain small customizations so that final URLs look more idiomatic. For example, the REST endpoint for the cosmos.bank.v1beta1.Query/AllBalances method is GET /cosmos/bank/v1beta1/balances/{address} . Request arguments are passed as query parameters. Note that the REST endpoints are not enabled by default. To enable them, edit the api section of your  ~/.simapp/config/app.toml file: As a concrete example, the curl command to make balances request is: curl \\\\\\n    -X GET \\\\\\n    -H \\\"Content-Type: application/json\\\" \\\\\\n    http://localhost:1317/cosmos/bank/v1beta1/balances/$MY_VALIDATOR_ADDRESS\\n\\nInstall Cosmos SDK\\nYou can replace the query client (here we are using x/bank 's) with one generated from any other Protobuf service. The list of all available gRPC query endpoints is coming soon.\\n\\n\"]",
            "additional_kwargs": {},
            "response_metadata": {},
            "type": "tool",
            "name": "SearchCosmosDocumentation",
            "id": "5ae74b50-e740-4b36-b87c-ae2da0f18a19",
            "tool_call_id": "call_x3PcTTbPOziFbCe1U3FYrqEp",
            "artifact": null,
            "status": "success"
        },
        {
            "content": "[\n  {\n    \"intent\": \"Write a CPU profile to cpu.prof for 30 seconds\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"check_pprof_port\",\n        \"description\": \"Confirm the node was started with the built-in pprof HTTP server enabled (it listens on http://localhost:6060 by default). Restart the node with pprof enabled if necessary.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"send_pprof_request\",\n        \"description\": \"Run: `curl http://localhost:6060/debug/pprof/profile?seconds=30 --output cpu.prof` to start a 30-second CPU capture and write the binary data to cpu.prof.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"verify_profile_file\",\n        \"description\": \"Ensure cpu.prof now exists and its size is greater than 0 bytes.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"cpu.prof is present in the working directory.\",\n      \"File size > 0 bytes and `go tool pprof cpu.prof` opens without error.\"\n    ]\n  },\n  {\n    \"intent\": \"Write a block profile to block.prof for 30 seconds\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"enable_block_profiling\",\n        \"description\": \"Restart the node with the environment variable `GODEBUG=blockprofilerate=1` so the Go runtime records blocking events.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"wait_runtime\",\n        \"description\": \"Allow the node to run under normal load for 30 seconds to accumulate samples.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"fetch_block_profile\",\n        \"description\": \"Execute `curl http://localhost:6060/debug/pprof/block?debug=0 --output block.prof` to download the collected block profile.\"\n      },\n      {\n        \"step\": 4,\n        \"tool\": \"verify_profile_file\",\n        \"description\": \"Confirm block.prof exists and is non-zero in size.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"block.prof exists and its size is > 0 bytes.\",\n      \"`go tool pprof block.prof` loads successfully.\"\n    ]\n  },\n  {\n    \"intent\": \"Start indefinite CPU profiling to cpu.prof\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"update_node_start_flags\",\n        \"description\": \"Edit the node start command or systemd service to add `--cpu-profile cpu.prof`, leveraging the Cosmos SDK\u2019s `wrapCPUProfile` helper.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"restart_node\",\n        \"description\": \"Restart (or start) the node so continuous CPU profiling begins immediately.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"check_profile_growth\",\n        \"description\": \"After a few seconds run `ls -lh cpu.prof` (or similar) to verify the file is being written and growing.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"cpu.prof is present and its size increases over time.\",\n      \"Node logs contain the line `starting CPU profiler`.\"\n    ]\n  },\n  {\n    \"intent\": \"Stop the current CPU profile\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"graceful_shutdown\",\n        \"description\": \"Send SIGINT (Ctrl-C) or `kill -INT <pid>` to the node process\u2014or `systemctl stop <service>` if running as a service\u2014to trigger the deferred `pprof.StopCPUProfile()` call.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"wait_for_exit\",\n        \"description\": \"Wait until the node fully exits and observe the log message `stopping CPU profiler`.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"verify_profile_file\",\n        \"description\": \"Check that cpu.prof has been flushed to disk and is non-empty.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"Node process has terminated cleanly.\",\n      \"cpu.prof timestamp updated and size > 0 bytes.\",\n      \"`go tool pprof cpu.prof` parses without error.\"\n    ]\n  },\n  {\n    \"intent\": \"Set garbage collection percentage to 100\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"update_environment\",\n        \"description\": \"Add `Environment=\\\"GOGC=100\\\"` to the node\u2019s service file or export `GOGC=100` in the shell before starting the daemon. This sets `debug.SetGCPercent(100)` at launch.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"restart_node\",\n        \"description\": \"Restart the Cosmos SDK daemon so the new GC setting takes effect.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"validate_gc_setting\",\n        \"description\": \"Query `http://localhost:6060/debug/vars` and confirm the GCPercent value is 100, or inspect runtime metrics if enabled.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"debug/vars endpoint reports `\\\"GCPercent\\\": 100`.\",\n      \"Node starts normally and exhibits expected memory behavior.\"\n    ]\n  }\n]",
            "additional_kwargs": {
                "refusal": null
            },
            "response_metadata": {
                "token_usage": {
                    "completion_tokens": 5201,
                    "prompt_tokens": 40213,
                    "total_tokens": 45414,
                    "completion_tokens_details": {
                        "accepted_prediction_tokens": 0,
                        "audio_tokens": 0,
                        "reasoning_tokens": 4032,
                        "rejected_prediction_tokens": 0
                    },
                    "prompt_tokens_details": {
                        "audio_tokens": 0,
                        "cached_tokens": 0
                    }
                },
                "model_name": "o3-2025-04-16",
                "system_fingerprint": null,
                "id": "chatcmpl-CPvtny6rODSuweRDNrfZmDD06GvnV",
                "service_tier": "default",
                "finish_reason": "stop",
                "logprobs": null
            },
            "type": "ai",
            "name": null,
            "id": "run--60e4f88e-f673-442b-a26a-f462192a979d-0",
            "example": false,
            "tool_calls": [],
            "invalid_tool_calls": [],
            "usage_metadata": {
                "input_tokens": 40213,
                "output_tokens": 5201,
                "total_tokens": 45414,
                "input_token_details": {
                    "audio": 0,
                    "cache_read": 0
                },
                "output_token_details": {
                    "audio": 0,
                    "reasoning": 4032
                }
            }
        }
    ]
}