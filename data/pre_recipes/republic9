[
    {
        "intent": "List compute jobs that violated time constraints or were slashed",
        "workflow": [
            {
                "step": 1,
                "tool": "parse_list_jobs_filters",
                "description": "Parse and normalize any user-provided filters (e.g., validator address, client address, time range, job type, pagination) for listing compute jobs; if no filters are provided, default to a reasonable recent time window (e.g., last N days or blocks)."
            },
            {
                "step": 2,
                "tool": "fetch_time_violated_jobs",
                "description": "Query the Republic compute job registry or indexer for jobs whose execution exceeded predefined time constraints (e.g., status in {TIMED_OUT, LATE_COMPLETION} or deadline_exceeded=true), using the normalized filters."
            },
            {
                "step": 3,
                "tool": "fetch_compute_slashing_events",
                "description": "Query the Republic slashing or penalty module (via RPC/indexer) for slashing events where the reason is tied to compute misbehavior (e.g., incorrect results, missed deadlines), returning at minimum job_id, validator_id, reason_code, and timestamps."
            },
            {
                "step": 4,
                "tool": "join_jobs_with_slashing_events",
                "description": "Join the time-violated job list with slashing events by job_id and validator_id to produce a unified set of jobs that either violated time constraints, were slashed due to compute issues, or both; annotate each job with flags such as time_violation, slashed, and a combined violation_reason."
            },
            {
                "step": 5,
                "tool": "enrich_job_violation_records",
                "description": "Optionally enrich each job record with additional metadata pulled from the job registry or validator set (e.g., validator moniker, validator reputation score, client identifier, job type, block heights), without altering the violation filters."
            },
            {
                "step": 6,
                "tool": "sort_and_paginate_job_violations",
                "description": "Sort the unified job list by most recent violation timestamp (e.g., max of deadline_time, completion_time, or slash_time) in descending order, then apply pagination (limit/offset or cursor) to prepare the final response payload."
            }
        ],
        "outcome_checks": [
            "Verify that every returned job has either a time_violation flag set or at least one associated compute slashing event (or both).",
            "Confirm that no jobs outside the requested filter range (time window, validator, client, job type) are present in the result.",
            "Ensure that sorting is correct (e.g., violation_time is monotonically non-increasing across the returned list).",
            "If the query span includes known violations (e.g., as cross-checked against a reference indexer), confirm that they appear in the output; otherwise surface a data_consistency warning."
        ]
    },
    {
        "intent": "Show Proof of Model Execution root hash and verification status for a specific compute job",
        "workflow": [
            {
                "step": 1,
                "tool": "validate_and_normalize_job_id",
                "description": "Validate that the provided {job_id} is non-empty and matches the expected Republic job identifier format (e.g., hex string or UUID); normalize it to the canonical representation used by backend queries."
            },
            {
                "step": 2,
                "tool": "fetch_job_metadata",
                "description": "Query the Republic job registry or indexer by job_id to confirm the job exists and retrieve basic metadata (validator_id, client_id, job type, assigned_time, completion_time, current status)."
            },
            {
                "step": 3,
                "tool": "fetch_pome_root_hash",
                "description": "Query the Proof of Model Execution (PoME) or compute-proof storage for this job_id to retrieve the recorded root_hash and associated proof metadata (e.g., checkpoint_count, proof_version, submission_block_height, submission_timestamp)."
            },
            {
                "step": 4,
                "tool": "fetch_verification_results_for_job",
                "description": "Query the verification module/indexer for all verifier re-execution records referencing this job_id, collecting per-verifier fields such as verifier_address, local_root_hash, verification_outcome (match/mismatch), and timestamps."
            },
            {
                "step": 5,
                "tool": "compute_overall_verification_status",
                "description": "Aggregate the per-verifier results to derive an overall verification status for the job (e.g., VERIFIED if all verifiers match the submitted root_hash and threshold is met; DISPUTED if there are mixed match/mismatch results; FAILED if consensus is that the root_hash is incorrect; PENDING if insufficient or no verifications)."
            },
            {
                "step": 6,
                "tool": "construct_pome_status_response",
                "description": "Assemble a response object that includes job_id, root_hash, proof metadata, overall_verification_status, number_of_verifiers, number_of_matches, number_of_mismatches, last_verified_at, and any links to associated slashing or dispute records."
            }
        ],
        "outcome_checks": [
            "Confirm that a root_hash is present for the job and that it conforms to the expected format (e.g., a fixed-length hex string representing the cryptographic hash).",
            "If overall_verification_status is VERIFIED, ensure that the number_of_mismatches is zero and that the number_of_matches satisfies the minimum verification threshold configured by the protocol.",
            "If the job is marked COMPLETED in metadata but no PoME record exists, flag an inconsistency between job_status and proof availability.",
            "If any verifier record reports match=true but its local_root_hash differs from the stored root_hash, flag a data_integrity error and do not report the job as VERIFIED."
        ]
    },
    {
        "intent": "Show cryptographic checkpoint hashes recorded for a specific compute job",
        "workflow": [
            {
                "step": 1,
                "tool": "validate_and_normalize_job_id",
                "description": "Validate and canonicalize the provided {job_id} according to Republic\u2019s job identifier format to ensure it is safe to use in backend queries."
            },
            {
                "step": 2,
                "tool": "fetch_pome_metadata_for_job",
                "description": "Query the PoME or compute-proof registry for this job_id to retrieve metadata such as root_hash, expected checkpoint_count, and information about where checkpoint hashes are stored (on-chain field, indexer table, or off-chain storage URI)."
            },
            {
                "step": 3,
                "tool": "resolve_checkpoint_storage_location",
                "description": "Interpret the storage information from metadata to produce a concrete plan for fetching checkpoint hashes, determining whether to read from an on-chain proof field, an indexer database, or an external object store (e.g., IPFS, S3-style storage) and constructing the appropriate query or URL."
            },
            {
                "step": 4,
                "tool": "fetch_checkpoint_hashes_for_job",
                "description": "Execute the resolved fetch plan to obtain the ordered list of checkpoint hashes for the job, returning an array of records including at least checkpoint_index and hash, and optionally per-checkpoint timestamps or segment identifiers."
            },
            {
                "step": 5,
                "tool": "verify_checkpoint_hash_chain",
                "description": "Using the fetched checkpoint hashes and the HashedModel aggregation scheme, recompute the root hash and compare it to the stored root_hash from PoME metadata; annotate the result with a boolean flags such as checkpoints_match_root and any discrepancy details."
            },
            {
                "step": 6,
                "tool": "construct_checkpoint_hashes_response",
                "description": "Build the final response object containing job_id, root_hash, expected_checkpoint_count, actual_checkpoint_count, ordered list of {index, hash} pairs, and the verification result (e.g., MATCHES_ROOT, MISMATCH_WITH_ROOT, or CHECKPOINTS_NOT_AVAILABLE if none were recorded)."
            }
        ],
        "outcome_checks": [
            "If metadata reports a positive checkpoint_count, verify that the number of fetched checkpoint hashes equals this count; if not, flag a partial_data error.",
            "Validate that each checkpoint hash is in the expected cryptographic format and that indexes are strictly increasing with no gaps or duplicates.",
            "If checkpoints are available, recomputing the root hash from them must either match the stored root_hash (checkpoints_match_root=true) or, on mismatch, cause the response to surface a verification_failure state rather than silently succeeding.",
            "Ensure that only cryptographic hashes and non-sensitive metadata are returned (no raw model parameters or private input data)."
        ]
    },
    {
        "intent": "Show which verifiers re-executed a compute job and whether they matched the submitted root hash",
        "workflow": [
            {
                "step": 1,
                "tool": "validate_and_normalize_job_id",
                "description": "Validate the provided {job_id} and normalize it to the canonical form expected by Republic backend services."
            },
            {
                "step": 2,
                "tool": "fetch_canonical_pome_root_hash",
                "description": "Retrieve the canonical PoME root_hash for this job from the compute-proof registry to use as the reference value for verifier comparison."
            },
            {
                "step": 3,
                "tool": "fetch_verifier_execution_records",
                "description": "Query the verification module or indexer for all verification_result records associated with job_id, retrieving for each: verifier_address, local_root_hash (recomputed by the verifier), reported_match_flag, block_height, and timestamp."
            },
            {
                "step": 4,
                "tool": "enrich_verifier_metadata",
                "description": "For each verifier_address that appears in the records, optionally fetch validator metadata (e.g., moniker, reputation_score, total_stake) from the validator registry to include contextual information in the response."
            },
            {
                "step": 5,
                "tool": "cross_check_verifier_match_flags",
                "description": "For each verification record, recompute whether local_root_hash equals the canonical root_hash and compare this to the stored reported_match_flag; mark each record as consistent or inconsistent and derive a normalized matched boolean field."
            },
            {
                "step": 6,
                "tool": "construct_verifier_match_summary",
                "description": "Build a response that includes a list of verifiers sorted by verification timestamp, each entry containing verifier_address, optional validator metadata, local_root_hash, matched flag (based on canonical comparison), consistency_status, and timestamp, along with aggregate counts of matches, mismatches, and inconsistencies."
            }
        ],
        "outcome_checks": [
            "Ensure that for every record where matched is reported true, local_root_hash exactly equals the canonical root_hash; otherwise, mark the record as inconsistent and do not treat it as a successful match.",
            "If there are mismatching verifiers, check whether corresponding slashing or dispute events exist for the responsible validator addresses and surface this linkage where available.",
            "If the job is marked COMPLETED and has a PoME root_hash but zero verifier records, return an explicit verification_status of UNVERIFIED instead of implying success.",
            "Confirm that the sum of per-verifier match and mismatch counts equals the total number of verification records returned for the job."
        ]
    },
    {
        "intent": "Show latency distribution for compute job completion times across validators",
        "workflow": [
            {
                "step": 1,
                "tool": "parse_latency_distribution_filters",
                "description": "Parse any user-provided filters for the latency analysis (e.g., time range, job_type, subset of validators, only successful jobs vs. all jobs); if no filters are provided, default to a recent time window and include all validators."
            },
            {
                "step": 2,
                "tool": "fetch_job_assignment_and_completion_events",
                "description": "From the Republic job scheduler and compute modules (via indexer or analytics DB), fetch all compute jobs matching the filters, including at least job_id, validator_id, assignment_time, completion_time (if any), and final job status."
            },
            {
                "step": 3,
                "tool": "compute_per_job_latency",
                "description": "For each job that has a valid completion_time and assignment_time, compute latency = completion_time - assignment_time; skip or separately track jobs without completion_time (e.g., timed out or cancelled)."
            },
            {
                "step": 4,
                "tool": "aggregate_latency_statistics",
                "description": "Aggregate the computed per-job latencies into global and per-validator statistics, calculating metrics such as count, min, max, mean, median, and selected percentiles (e.g., p90, p95), and constructing histogram buckets suitable for plotting a latency distribution."
            },
            {
                "step": 5,
                "tool": "clean_and_clip_latency_outliers",
                "description": "Detect and handle anomalous latency values (e.g., negative latencies or outliers beyond a configurable maximum) by excluding them from statistical calculations or placing them into a dedicated 'outliers' bucket, and record any data_quality_issues encountered."
            },
            {
                "step": 6,
                "tool": "construct_latency_distribution_response",
                "description": "Return a structured response containing the filters used, the total number of jobs considered, global latency statistics, per-validator aggregates, histogram data (bucket boundaries and counts), and a summary of excluded or anomalous samples if applicable."
            }
        ],
        "outcome_checks": [
            "Verify that no negative latency values are used in the final statistics; any such records should either be corrected (if due to known clock skew) or excluded and reported as data_quality_issues.",
            "Confirm that the sum of all histogram bucket counts equals the number of latency samples actually included in the analysis.",
            "Ensure that the reported sample_size matches the number of jobs that contributed to the distribution (after filtering and outlier handling).",
            "If the result set is very small (e.g., below a minimum threshold), flag that the latency distribution may not be statistically meaningful."
        ]
    }
]