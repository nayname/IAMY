{
    "messages": [
        {
            "content": "[{\"label\": \"UserGuides\", \"query\": \"Delete a local snapshot\"}, {\"label\": \"UserGuides\", \"query\": \"List available gRPC services on localhost:9090 via grpcurl\"}, {\"label\": \"UserGuides\", \"query\": \"Query bank balances via REST endpoint\"}, {\"label\": \"UserGuides\", \"query\": \"Broadcast a transaction over REST\"}, {\"label\": \"UserGuides\", \"query\": \"Broadcast a transaction over gRPC\"}]",
            "additional_kwargs": {},
            "response_metadata": {},
            "type": "human",
            "name": null,
            "id": "f3e5f2df-57d6-4f60-83b7-4680786b57ba",
            "example": false
        },
        {
            "content": "",
            "additional_kwargs": {
                "tool_calls": [
                    {
                        "id": "call_2SFYKvaMkcHr0buDvYOeWBxh",
                        "function": {
                            "arguments": "{\"query\":\"delete snapshot cosmos node\"}",
                            "name": "SearchCosmosDocumentation"
                        },
                        "type": "function"
                    }
                ],
                "refusal": null
            },
            "response_metadata": {
                "token_usage": {
                    "completion_tokens": 668,
                    "prompt_tokens": 704,
                    "total_tokens": 1372,
                    "completion_tokens_details": {
                        "accepted_prediction_tokens": 0,
                        "audio_tokens": 0,
                        "reasoning_tokens": 640,
                        "rejected_prediction_tokens": 0
                    },
                    "prompt_tokens_details": {
                        "audio_tokens": 0,
                        "cached_tokens": 0
                    }
                },
                "model_name": "o3-2025-04-16",
                "system_fingerprint": null,
                "id": "chatcmpl-CPpU623NDwmpiPsJvLQSc3gvUSVG8",
                "service_tier": "default",
                "finish_reason": "tool_calls",
                "logprobs": null
            },
            "type": "ai",
            "name": null,
            "id": "run--7d5b18e7-74df-4643-8422-6d3cc052e99e-0",
            "example": false,
            "tool_calls": [
                {
                    "name": "SearchCosmosDocumentation",
                    "args": {
                        "query": "delete snapshot cosmos node"
                    },
                    "id": "call_2SFYKvaMkcHr0buDvYOeWBxh",
                    "type": "tool_call"
                }
            ],
            "invalid_tool_calls": [],
            "usage_metadata": {
                "input_tokens": 704,
                "output_tokens": 668,
                "total_tokens": 1372,
                "input_token_details": {
                    "audio": 0,
                    "cache_read": 0
                },
                "output_token_details": {
                    "audio": 0,
                    "reasoning": 640
                }
            }
        },
        {
            "content": "[\"Title: Snapshots Commands\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/user/run-node/run-node\\nContent: The Cosmos SDK provides commands for managing snapshots.\\nThese commands can be added in an app with the following snippet in cmd/<app>/root.go: Then the following commands are available at <appd> snapshots [command]: list : list local snapshots load : Load a snapshot archive file into snapshot store restore : Restore app state from local snapshot export : Export app state to snapshot store dump : Dump the snapshot as portable archive format delete : Delete a local snapshot\\n\\nThe Cosmos SDK provides commands for managing snapshots.\\nThese commands can be added in an app with the following snippet in cmd/<app>/root.go: import (\\n    \\n  \\\"github.com/cosmos/cosmos-sdk/client/snapshot\\\"\\n)\\n\\nfunc initRootCmd(/* ... */) {\\n  // ...\\n  rootCmd.AddCommand(\\n    snapshot.Cmd(appCreator),\\n  )\\n}\\n\\nRunning a Node\\nSynopsis Now that the application is ready and the keyring populated, it's time to see how to run the blockchain node. In this section, the application we are running is called simapp , and its corresponding CLI binary simd. Pre-requisite Readings Anatomy of a Cosmos SDK Application Setting up the keyring\\n\\n\", \"Title: Snapshots Commands\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/user/run-node/run-node\\nContent: The Cosmos SDK provides commands for managing snapshots.\\nThese commands can be added in an app with the following snippet in cmd/<app>/root.go: Then following commands are available at <appd> snapshots [command]: list : list local snapshots load : Load a snapshot archive file into snapshot store restore : Restore app state from local snapshot export :  Export app state to snapshot store dump : Dump the snapshot as portable archive format delete : Delete a local snapshot\\n\\nThe Cosmos SDK provides commands for managing snapshots.\\nThese commands can be added in an app with the following snippet in cmd/<app>/root.go: import (\\n    \\n  \\\"github.com/cosmos/cosmos-sdk/client/snapshot\\\"\\n)\\n\\nfunc initRootCmd(/* ... */) {\\n  // ...\\n  rootCmd.AddCommand(\\n    snapshot.Cmd(appCreator),\\n  )\\n}\\n\\nRunning a Node\\nSynopsis Now that the application is ready and the keyring populated, it's time to see how to run the blockchain node. In this section, the application we are running is called simapp , and its corresponding CLI binary simd. Pre-requisite Readings Anatomy of a Cosmos SDK Application Setting up the keyring\\n\\n\", \"Title: Snapshots Commands\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/user/run-node/run-node\\nContent: The Cosmos SDK provides commands for managing snapshots.\\nThese commands can be added in an app with the following snippet in cmd/<app>/root.go: Then following commands are available at <appd> snapshots [command]: list : list local snapshots load : Load a snapshot archive file into snapshot store restore : Restore app state from local snapshot export :  Export app state to snapshot store dump : Dump the snapshot as portable archive format delete : Delete a local snapshot\\n\\nThe Cosmos SDK provides commands for managing snapshots.\\nThese commands can be added in an app with the following snippet in cmd/<app>/root.go: import (\\n    \\n  \\\"github.com/cosmos/cosmos-sdk/client/snapshot\\\"\\n)\\n\\nfunc initRootCmd(/* ... */) {\\n  // ...\\n  rootCmd.AddCommand(\\n    snapshot.Cmd(appCreator),\\n  )\\n}\\n\\nRunning a Node\\nSynopsis Now that the application is ready and the keyring populated, it's time to see how to run the blockchain node. In this section, the application we are running is called simapp , and its corresponding CLI binary simd. Pre-requisite Readings Anatomy of a Cosmos SDK Application Setting up the keyring\\n\\n\", \"Title: Node Client (Daemon)\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/learn/advanced/node\\nContent: Node Client (Daemon)\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI.\\n\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI.\\n\\nstart command\\nThe start command is defined in the /server folder of the Cosmos SDK. It is added to the root command of the full-node client in the main function and called by the end-user to start their node: # For an example app named \\\"app\\\", the following command starts the full-node.\\nappd start\\n\\n# Using the Cosmos SDK's own simapp, the following commands start the simapp node.\\nsimd start\\n\\n\", \"Title: Node Client (Daemon)\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/advanced/node\\nContent: Node Client (Daemon)\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nstart command\\nThe start command is defined in the /server folder of the Cosmos SDK. It is added to the root command of the full-node client in the main function and called by the end-user to start their node: As a reminder, the full-node is composed of three conceptual layers: the networking layer, the consensus layer and the application layer. The first two are generally bundled together in an entity called the consensus engine (CometBFT by default), while the third is the state-machine defined with the help of the Cosmos SDK. Currently, the Cosmos SDK uses CometBFT as the default consensus engine, meaning the start command is implemented to boot up a CometBFT node. The flow of the start command is pretty straightforward. First, it retrieves the config from the context in order to open the db (a leveldb instance by default). This db contains the latest known state of the application (empty if the application is started from the first time. With the db , the start command creates a new instance of the application using an appCreator function: Note that an appCreator is a function that fulfills the AppCreator signature: package types\\n\\nimport (\\n    \\n\\t\\\"encoding/json\\\"\\n    \\\"io\\\"\\n\\n\\tcmtproto \\\"github.com/cometbft/cometbft/proto/tendermint/types\\\"\\n\\tcmttypes \\\"github.com/cometbft/cometbft/types\\\"\\n\\tdbm \\\"github.com/cosmos/cosmos-db\\\"\\n    \\\"github.com/cosmos/gogoproto/grpc\\\"\\n    \\\"github.com/spf13/cobra\\\"\\n    \\\"cosmossdk.io/log\\\"\\n    \\\"cosmossdk.io/store/snapshots\\\"\\n\\tstoretypes \\\"cosmossdk.io/store/types\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/api\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/config\\\"\\n)\\n\\ntype (\\n\\t// AppOptions defines an interface that is passed into an application\\n\\t// constructor, typically used to set BaseApp options that are either supplied\\n\\t// via config file or through CLI arguments/flags. The underlying implementation\\n\\t// is defined by the server package and is typically implemented via a Viper\\n\\t// literal defined on the server Context. Note, casting Get calls may not yield\\n\\t// the expected types and could result in type assertion errors. It is recommend\\n\\t// to either use the cast package or perform manual conversion for safety.\\n\\tAppOptions interface {\\n    Get(string)\\n\\nany\\n}\\n\\n\\t// Application defines an application interface that wraps abci.Application.\\n\\t// The interface defines the necessary contracts to be implemented in order\\n\\t// to fully bootstrap and start an application.\\n\\tApplication interface {\\n    ABCI\\n\\n\\t\\tRegisterAPIRoutes(*api.Server, config.APIConfig)\\n\\n\\t\\t// RegisterGRPCServerWithSkipCheckHeader registers gRPC services directly with the gRPC\\n\\t\\t// server and bypass check header flag.\\n\\t\\tRegisterGRPCServerWithSkipCheckHeader(grpc.Server, bool)\\n\\n\\t\\t// RegisterTxService registers the gRPC Query service for tx (such as tx\\n\\t\\t// simulation, fetching txs by hash...).\\n\\t\\tRegisterTxService(client.Context)\\n\\n\\t\\t// RegisterTendermintService registers the gRPC Query service for CometBFT queries.\\n\\t\\tRegisterTendermintService(client.Context)\\n\\n\\t\\t// RegisterNodeService registers the node gRPC Query service.\\n\\t\\tRegisterNodeService(client.Context, config.Config)\\n\\n\\t\\t// CommitMultiStore return the multistore instance\\n\\t\\tCommitMultiStore()\\n\\nstoretypes.CommitMultiStore\\n\\n\\t\\t// Return the snapshot manager\\n\\t\\tSnapshotManager() *snapshots.Manager\\n\\n\\t\\t// Close is called in start cmd to gracefully cleanup resources.\\n\\t\\t// Must be safe to be called multiple times.\\n\\t\\tClose()\\n\\nerror\\n}\\n\\n\\t// AppCreator is a function that allows us to lazily initialize an\\n\\t// application using various configurations.\\n\\tAppCreator func(log.Logger, dbm.DB, io.Writer, AppOptions)\\n\\nApplication\\n\\n\\t// ModuleInitFlags takes a start command and adds modules specific init flags.\\n\\tModuleInitFlags func(startCmd *cobra.Command)\\n\\n\\t// ExportedApp represents an exported app state, along with\\n\\t// validators, consensus params and latest app height.\\n\\tExportedApp struct {\\n\\t\\t// AppState is the application state as JSON.\\n\\t\\tAppState json.RawMessage\\n\\t\\t// Validators is the exported validator set.\\n\\t\\tValidators []cmttypes.GenesisValidator\\n\\t\\t// Height is the app's latest block height.\\n\\t\\tHeight int64\\n\\t\\t// ConsensusParams are the exported consensus params for ABCI.\\n\\t\\tConsensusParams cmtproto.ConsensusParams\\n}\\n\\n\\t// AppExporter is a function that dumps all app state to\\n\\t// JSON-serializable structure and returns the current validator set.\\n\\tAppExporter func(\\n\\t\\tlogger log.Logger,\\n\\t\\tdb dbm.DB,\\n\\t\\ttraceWriter io.Writer,\\n\\t\\theight int64,\\n    forZeroHeight bool,\\n\\t\\tjailAllowedAddrs []string,\\n\\t\\topts AppOptions,\\n\\t\\tmodulesToExport []string,\\n\\t) (ExportedApp, error)\\n)\\n\\n\", \"Title: Node Client (Daemon)\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/learn/advanced/node\\nContent: Node Client (Daemon)\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nSynopsis The main endpoint of a Cosmos SDK application is the daemon client, otherwise known as the full-node client. The full-node runs the state-machine, starting from a genesis file. It connects to peers running the same client in order to receive and relay transactions, block proposals and signatures. The full-node is constituted of the application, defined with the Cosmos SDK, and of a consensus engine connected to the application via the ABCI. Pre-requisite Readings Anatomy of an SDK application\\n\\nstart command\\nThe start command is defined in the /server folder of the Cosmos SDK. It is added to the root command of the full-node client in the main function and called by the end-user to start their node: As a reminder, the full-node is composed of three conceptual layers: the networking layer, the consensus layer and the application layer. The first two are generally bundled together in an entity called the consensus engine (CometBFT by default), while the third is the state-machine defined with the help of the Cosmos SDK. Currently, the Cosmos SDK uses CometBFT as the default consensus engine, meaning the start command is implemented to boot up a CometBFT node. The flow of the start command is pretty straightforward. First, it retrieves the config from the context in order to open the db (a leveldb instance by default). This db contains the latest known state of the application (empty if the application is started from the first time. With the db , the start command creates a new instance of the application using an appCreator function: Note that an appCreator is a function that fulfills the AppCreator signature: package types\\n\\nimport (\\n    \\n\\t\\\"encoding/json\\\"\\n    \\\"io\\\"\\n    \\\"cosmossdk.io/log\\\"\\n    \\\"cosmossdk.io/store/snapshots\\\"\\n\\tstoretypes \\\"cosmossdk.io/store/types\\\"\\n\\tcmtproto \\\"github.com/cometbft/cometbft/proto/tendermint/types\\\"\\n\\tcmttypes \\\"github.com/cometbft/cometbft/types\\\"\\n\\tdbm \\\"github.com/cosmos/cosmos-db\\\"\\n    \\\"github.com/cosmos/gogoproto/grpc\\\"\\n    \\\"github.com/spf13/cobra\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/client\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/api\\\"\\n    \\\"github.com/cosmos/cosmos-sdk/server/config\\\"\\n)\\n\\ntype (\\n\\t// AppOptions defines an interface that is passed into an application\\n\\t// constructor, typically used to set BaseApp options that are either supplied\\n\\t// via config file or through CLI arguments/flags. The underlying implementation\\n\\t// is defined by the server package and is typically implemented via a Viper\\n\\t// literal defined on the server Context. Note, casting Get calls may not yield\\n\\t// the expected types and could result in type assertion errors. It is recommend\\n\\t// to either use the cast package or perform manual conversion for safety.\\n\\tAppOptions interface {\\n    Get(string)\\n\\ninterface{\\n}\\n\\t\\n}\\n\\n\\t// Application defines an application interface that wraps abci.Application.\\n\\t// The interface defines the necessary contracts to be implemented in order\\n\\t// to fully bootstrap and start an application.\\n\\tApplication interface {\\n    ABCI\\n\\n\\t\\tRegisterAPIRoutes(*api.Server, config.APIConfig)\\n\\n\\t\\t// RegisterGRPCServer registers gRPC services directly with the gRPC\\n\\t\\t// server.\\n\\t\\tRegisterGRPCServer(grpc.Server)\\n\\n\\t\\t// RegisterTxService registers the gRPC Query service for tx (such as tx\\n\\t\\t// simulation, fetching txs by hash...).\\n\\t\\tRegisterTxService(client.Context)\\n\\n\\t\\t// RegisterTendermintService registers the gRPC Query service for CometBFT queries.\\n\\t\\tRegisterTendermintService(client.Context)\\n\\n\\t\\t// RegisterNodeService registers the node gRPC Query service.\\n\\t\\tRegisterNodeService(client.Context, config.Config)\\n\\n\\t\\t// CommitMultiStore return the multistore instance\\n\\t\\tCommitMultiStore()\\n\\nstoretypes.CommitMultiStore\\n\\n\\t\\t// Return the snapshot manager\\n\\t\\tSnapshotManager() *snapshots.Manager\\n\\n\\t\\t// Close is called in start cmd to gracefully cleanup resources.\\n\\t\\t// Must be safe to be called multiple times.\\n\\t\\tClose()\\n\\nerror\\n}\\n\\n\\t// AppCreator is a function that allows us to lazily initialize an\\n\\t// application using various configurations.\\n\\tAppCreator func(log.Logger, dbm.DB, io.Writer, AppOptions)\\n\\nApplication\\n\\n\\t// ModuleInitFlags takes a start command and adds modules specific init flags.\\n\\tModuleInitFlags func(startCmd *cobra.Command)\\n\\n\\t// ExportedApp represents an exported app state, along with\\n\\t// validators, consensus params and latest app height.\\n\\tExportedApp struct {\\n\\t\\t// AppState is the application state as JSON.\\n\\t\\tAppState json.RawMessage\\n\\t\\t// Validators is the exported validator set.\\n\\t\\tValidators []cmttypes.GenesisValidator\\n\\t\\t// Height is the app's latest block height.\\n\\t\\tHeight int64\\n\\t\\t// ConsensusParams are the exported consensus params for ABCI.\\n\\t\\tConsensusParams cmtproto.ConsensusParams\\n}\\n\\n\\t// AppExporter is a function that dumps all app state to\\n\\t// JSON-serializable structure and returns the current validator set.\\n\\tAppExporter func(\\n\\t\\tlogger log.Logger,\\n\\t\\tdb dbm.DB,\\n\\t\\ttraceWriter io.Writer,\\n\\t\\theight int64,\\n    forZeroHeight bool,\\n\\t\\tjailAllowedAddrs []string,\\n\\t\\topts AppOptions,\\n\\t\\tmodulesToExport []string,\\n\\t) (ExportedApp, error)\\n)\\n\\n\", \"Title: Snapshots for storage sync and state versioning\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/build/architecture/adr-040-storage-and-smt-state-commitments\\nContent: Below, with simple snapshot we refer to a database snapshot mechanism, not to a ABCI snapshot sync . The latter will be referred as snapshot sync (which will directly use DB snapshot as described below). Database snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a copy on write and it allows DB state to be efficiently delivered at a certain stage.\\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs. One of the Stargate core features is a snapshot sync delivered in the /snapshot package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a SS at a certain version together with a header chain. A new database snapshot will be created in every EndBlocker and identified by a block height. The root store keeps track of the available snapshots to offer SS at a certain version. The root store implements the RootStore interface described below. In essence, RootStore encapsulates a Committer interface. Committer has a Commit , SetPruning , GetPruning functions which will be used for creating and removing snapshots. The rootStore.Commit function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the Committer interface.\\nNOTE: Commit must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into Committer and PruningCommitter - only the multiroot should implement PruningCommitter (cache and prefix store don't need pruning). Number of historical versions for abci.RequestQuery and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions. Pruning old snapshots is effectively done by a database. Whenever we update a record in SC , SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions. To manage the active snapshots we will either use a DB max number of snapshots option (if available), or we will remove DB snapshots in the EndBlocker . The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\\n\\nContext\\nCurrently, Cosmos SDK uses IAVL for both state commitments and data storage. IAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems: Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level. Each edge traversal requires a DB query. Creating snapshots is expensive . It takes about 30 seconds to export less than 100 MB of state (as of March 2020). Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck. The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, ref. Moreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\\n\\nAccessing old state versions\\nOne of the functional requirements is to access old state. This is done through abci.RequestQuery structure.  The version is specified by a block height (so we query for an object by a key K at block height H ). The number of old versions supported for abci.RequestQuery is configurable. Accessing an old state is done by using available snapshots.\\nabci.RequestQuery doesn't need old state of SC unless the prove=true parameter is set. The SMT merkle proof must be included in the abci.ResponseQuery only if both SC and SS have a snapshot for requested version. Moreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution. We positively validated a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\\n\\n\", \"Title: Snapshots for storage sync and state versioning\\nLink: https://evm.cosmos.network/docs/sdk/v0.47/build/architecture/adr-040-storage-and-smt-state-commitments\\nContent: Below, with simple snapshot we refer to a database snapshot mechanism, not to a ABCI snapshot sync . The latter will be referred as snapshot sync (which will directly use DB snapshot as described below). Database snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a copy on write and it allows DB state to be efficiently delivered at a certain stage.\\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs. One of the Stargate core features is a snapshot sync delivered in the /snapshot package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a SS at a certain version together with a header chain. A new database snapshot will be created in every EndBlocker and identified by a block height. The root store keeps track of the available snapshots to offer SS at a certain version. The root store implements the RootStore interface described below. In essence, RootStore encapsulates a Committer interface. Committer has a Commit , SetPruning , GetPruning functions which will be used for creating and removing snapshots. The rootStore.Commit function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the Committer interface.\\nNOTE: Commit must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into Committer and PruningCommitter - only the multiroot should implement PruningCommitter (cache and prefix store don't need pruning). Number of historical versions for abci.RequestQuery and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions. Pruning old snapshots is effectively done by a database. Whenever we update a record in SC , SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions. To manage the active snapshots we will either use a DB max number of snapshots option (if available), or we will remove DB snapshots in the EndBlocker . The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\\n\\nContext\\nCurrently, Cosmos SDK uses IAVL for both state commitments and data storage. IAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems: Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level. Each edge traversal requires a DB query. Creating snapshots is expensive . It takes about 30 seconds to export less than 100 MB of state (as of March 2020). Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck. The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, ref. Moreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\\n\\nAccessing old state versions\\nOne of the functional requirements is to access old state. This is done through abci.RequestQuery structure.  The version is specified by a block height (so we query for an object by a key K at block height H ). The number of old versions supported for abci.RequestQuery is configurable. Accessing an old state is done by using available snapshots.\\nabci.RequestQuery doesn't need old state of SC unless the prove=true parameter is set. The SMT merkle proof must be included in the abci.ResponseQuery only if both SC and SS have a snapshot for requested version. Moreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution. We positively validated a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\\n\\n\", \"Title: Snapshots for storage sync and state versioning\\nLink: https://evm.cosmos.network/docs/sdk/v0.50/build/architecture/adr-040-storage-and-smt-state-commitments\\nContent: Below, with simple snapshot we refer to a database snapshot mechanism, not to a ABCI snapshot sync . The latter will be referred as snapshot sync (which will directly use DB snapshot as described below). Database snapshot is a view of DB state at a certain time or transaction. It's not a full copy of a database (it would be too big). Usually a snapshot mechanism is based on a copy on write and it allows DB state to be efficiently delivered at a certain stage.\\nSome DB engines support snapshotting. Hence, we propose to reuse that functionality for the state sync and versioning (described below). We limit the supported DB engines to ones which efficiently implement snapshots. In a final section we discuss the evaluated DBs. One of the Stargate core features is a snapshot sync delivered in the /snapshot package. It provides a way to trustlessly sync a blockchain without repeating all transactions from the genesis. This feature is implemented in Cosmos SDK and requires storage support. Currently IAVL is the only supported backend. It works by streaming to a client a snapshot of a SS at a certain version together with a header chain. A new database snapshot will be created in every EndBlocker and identified by a block height. The root store keeps track of the available snapshots to offer SS at a certain version. The root store implements the RootStore interface described below. In essence, RootStore encapsulates a Committer interface. Committer has a Commit , SetPruning , GetPruning functions which will be used for creating and removing snapshots. The rootStore.Commit function creates a new snapshot and increments the version on each call, and checks if it needs to remove old versions. We will need to update the SMT interface to implement the Committer interface.\\nNOTE: Commit must be called exactly once per block. Otherwise we risk going out of sync for the version number and block height.\\nNOTE: For the Cosmos SDK storage, we may consider splitting that interface into Committer and PruningCommitter - only the multiroot should implement PruningCommitter (cache and prefix store don't need pruning). Number of historical versions for abci.RequestQuery and state sync snapshots is part of a node configuration, not a chain configuration (configuration implied by the blockchain consensus). A configuration should allow to specify number of past blocks and number of past blocks modulo some number (eg: 100 past blocks and one snapshot every 100 blocks for past 2000 blocks). Archival nodes can keep all past versions. Pruning old snapshots is effectively done by a database. Whenever we update a record in SC , SMT won't update nodes - instead it creates new nodes on the update path, without removing the old one. Since we are snapshotting each block, we need to change that mechanism to immediately remove orphaned nodes from the database. This is a safe operation - snapshots will keep track of the records and make it available when accessing past versions. To manage the active snapshots we will either use a DB max number of snapshots option (if available), or we will remove DB snapshots in the EndBlocker . The latter option can be done efficiently by identifying snapshots with block height and calling a store function to remove past versions.\\n\\nContext\\nCurrently, Cosmos SDK uses IAVL for both state commitments and data storage. IAVL has effectively become an orphaned project within the Cosmos ecosystem and it's proven to be an inefficient state commitment data structure.\\nIn the current design, IAVL is used for both data storage and as a Merkle Tree for state commitments. IAVL is meant to be a standalone Merkelized key/value database, however it's using a KV DB engine to store all tree nodes. So, each node is stored in a separate record in the KV DB. This causes many inefficiencies and problems: Each object query requires a tree traversal from the root. Subsequent queries for the same object are cached on the Cosmos SDK level. Each edge traversal requires a DB query. Creating snapshots is expensive . It takes about 30 seconds to export less than 100 MB of state (as of March 2020). Updates in IAVL may trigger tree reorganization and possible O(log(n)) hashes re-computation, which can become a CPU bottleneck. The node structure is pretty expensive - it contains a standard tree node elements (key, value, left and right element) and additional metadata such as height, version (which is not required by the Cosmos SDK). The entire node is hashed, and that hash is used as the key in the underlying database, ref. Moreover, the IAVL project lacks support and a maintainer and we already see better and well-established alternatives. Instead of optimizing the IAVL, we are looking into other solutions for both storage and state commitments.\\n\\nAccessing old state versions\\nOne of the functional requirements is to access old state. This is done through abci.RequestQuery structure.  The version is specified by a block height (so we query for an object by a key K at block height H ). The number of old versions supported for abci.RequestQuery is configurable. Accessing an old state is done by using available snapshots.\\nabci.RequestQuery doesn't need old state of SC unless the prove=true parameter is set. The SMT merkle proof must be included in the abci.ResponseQuery only if both SC and SS have a snapshot for requested version. Moreover, Cosmos SDK could provide a way to directly access a historical state. However, a state machine shouldn't do that - since the number of snapshots is configurable, it would lead to nondeterministic execution. We positively validated a versioning and snapshot mechanism for querying old state with regards to the database we evaluated.\\n\\n\", \"Title: Main Components of the Cosmos SDK\\nLink: https://evm.cosmos.network/docs/sdk/v0.53/learn/intro/sdk-design\\nContent: The Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nMain Components of the Cosmos SDK\\nThe Cosmos SDK is a framework that facilitates the development of secure state-machines on top of CometBFT. At its core, the Cosmos SDK is a boilerplate implementation of the ABCI in Golang. It comes with a multistore to persist data and a router to handle transactions. Here is a simplified view of how transactions are handled by an application built on top of the Cosmos SDK when transferred from CometBFT via DeliverTx: Decode transactions received from the CometBFT consensus engine (remember that CometBFT only deals with []bytes ). Extract messages from transactions and do basic sanity checks. Route each message to the appropriate module so that it can be processed. Commit state changes.\\n\\nModules\\nThe power of the Cosmos SDK lies in its modularity. Cosmos SDK applications are built by aggregating a collection of interoperable modules. Each module defines a subset of the state and contains its own message/transaction processor, while the Cosmos SDK is responsible for routing each message to its respective module. Here is a simplified view of how a transaction is processed by the application of each full-node when it is received in a valid block: flowchart TD\\n    A[Transaction relayed from the full-node's CometBFT engine to the node's application via DeliverTx] --> B[APPLICATION]\\n    B -->|\\\"Using baseapp's methods: Decode the Tx, extract and route the message(s)\\\"| C[Message routed to the correct module to be processed]\\n    C --> D1[AUTH MODULE]\\n    C --> D2[BANK MODULE]\\n    C --> D3[STAKING MODULE]\\n    C --> D4[GOV MODULE]\\n    D1 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n    D2 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n    D3 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n    D4 -->|Handle message, Update state| E[\\\"Return result to CometBFT (0=Ok, 1=Err)\\\"]\\n\\n\"]",
            "additional_kwargs": {},
            "response_metadata": {},
            "type": "tool",
            "name": "SearchCosmosDocumentation",
            "id": "936db85d-315d-491d-837a-37212c22ecd2",
            "tool_call_id": "call_2SFYKvaMkcHr0buDvYOeWBxh",
            "artifact": null,
            "status": "success"
        },
        {
            "content": "[\n  {\n    \"intent\": \"Delete a local snapshot stored by the node\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"snapshots_list\",\n        \"description\": \"Run \\\"<appd> snapshots list\\\" to enumerate all local snapshots and note the snapshot height or identifier you wish to remove.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"snapshots_delete\",\n        \"description\": \"Execute \\\"<appd> snapshots delete <snapshot_identifier>\\\" to remove the selected snapshot from the snapshot store.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"snapshots_list\",\n        \"description\": \"Re-run \\\"<appd> snapshots list\\\" to confirm the snapshot no longer appears in the inventory.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"The chosen snapshot identifier is absent from the list after deletion.\",\n      \"Command exits with status code 0 and prints a confirmation message such as \\\"snapshot deleted\\\".\"\n    ]\n  },\n  {\n    \"intent\": \"List all gRPC services exposed on localhost:9090 using grpcurl\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"verify_grpcurl_installation\",\n        \"description\": \"Ensure grpcurl is installed and in PATH by running \\\"grpcurl -help\\\".\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"grpcurl_list_services\",\n        \"description\": \"Run \\\"grpcurl -plaintext localhost:9090 list\\\" (add -insecure if TLS is enabled without trusted certs) to query the server\u2019s gRPC reflection service.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"Command returns a newline-delimited list of fully-qualified service names (e.g., cosmos.tx.v1beta1.Service, cosmos.bank.v1beta1.Query).\",\n      \"Exit status is 0 indicating the server responded successfully.\"\n    ]\n  },\n  {\n    \"intent\": \"Query a wallet\u2019s bank balances via the REST API\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"collect_target_address\",\n        \"description\": \"Obtain the bech32 address whose balances you want to query.\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"rest_get_request\",\n        \"description\": \"Send an HTTP GET to \\\"http://localhost:1317/cosmos/bank/v1beta1/balances/<address>\\\" using curl or any HTTP client.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"parse_json_response\",\n        \"description\": \"Decode the JSON payload to view the \\\"balances\\\" array containing denomination/amount pairs.\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"HTTP response code is 200.\",\n      \"Returned JSON contains a \\\"balances\\\" field with at least one entry or an empty array if the account is empty.\"\n    ]\n  },\n  {\n    \"intent\": \"Broadcast a pre-signed transaction via the REST endpoint\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"construct_unsigned_tx_cli\",\n        \"description\": \"Use \\\"<appd> tx <module> <msg> --generate-only --chain-id=<id> --fees=<amount><denom> -o json > unsigned.json\\\" to build an unsigned transaction (replace placeholders appropriately).\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"sign_tx_cli\",\n        \"description\": \"Sign the transaction with \\\"<appd> tx sign unsigned.json --from=<key_name> --chain-id=<id> -o json > signed.json\\\" to obtain a base64-encoded tx.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"extract_tx_bytes\",\n        \"description\": \"Read \\\"signed.json\\\" and copy the value of \\\"tx_bytes\\\" (base64 string).\"\n      },\n      {\n        \"step\": 4,\n        \"tool\": \"broadcast_tx_rest\",\n        \"description\": \"POST the signed bytes: \\n  curl -X POST http://localhost:1317/cosmos/tx/v1beta1/txs \\\\\\n       -H \\\"Content-Type: application/json\\\" \\\\\\n       -d '{\\\"tx_bytes\\\":\\\"<base64>\\\",\\\"mode\\\":\\\"BROADCAST_MODE_SYNC\\\"}'\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"HTTP 200 response contains a \\\"tx_response\\\" object.\",\n      \"\\\"tx_response.code\\\" equals 0 indicating success or provides an error code if it failed.\",\n      \"\\\"tx_response.txhash\\\" can be searched in a block to confirm inclusion.\"\n    ]\n  },\n  {\n    \"intent\": \"Broadcast a pre-signed transaction over gRPC\",\n    \"workflow\": [\n      {\n        \"step\": 1,\n        \"tool\": \"construct_unsigned_tx_cli\",\n        \"description\": \"Generate an unsigned tx JSON with \\\"<appd> tx <module> <msg> --generate-only\\\".\"\n      },\n      {\n        \"step\": 2,\n        \"tool\": \"sign_tx_cli\",\n        \"description\": \"Sign the tx with \\\"<appd> tx sign\\\" to obtain a base64 \\\"tx_bytes\\\" field.\"\n      },\n      {\n        \"step\": 3,\n        \"tool\": \"grpcurl_broadcast_tx\",\n        \"description\": \"Invoke the gRPC method with: \\n  grpcurl -plaintext -d '{\\\"tx_bytes\\\":\\\"<base64>\\\",\\\"mode\\\":\\\"BROADCAST_MODE_SYNC\\\"}' \\\\\\n          localhost:9090 cosmos.tx.v1beta1.Service/BroadcastTx\"\n      }\n    ],\n    \"outcome_checks\": [\n      \"grpcurl returns JSON containing \\\"tx_response\\\" with a non-empty \\\"txhash\\\".\",\n      \"\\\"tx_response.code\\\" is 0 for success; non-zero indicates an error.\",\n      \"Searching the hash via REST or gRPC shows the tx included in a block.\"\n    ]\n  }\n]",
            "additional_kwargs": {
                "refusal": null
            },
            "response_metadata": {
                "token_usage": {
                    "completion_tokens": 2496,
                    "prompt_tokens": 9924,
                    "total_tokens": 12420,
                    "completion_tokens_details": {
                        "accepted_prediction_tokens": 0,
                        "audio_tokens": 0,
                        "reasoning_tokens": 1216,
                        "rejected_prediction_tokens": 0
                    },
                    "prompt_tokens_details": {
                        "audio_tokens": 0,
                        "cached_tokens": 0
                    }
                },
                "model_name": "o3-2025-04-16",
                "system_fingerprint": null,
                "id": "chatcmpl-CPpUKnVJKPwFCxW9SDGosG0MCfTnv",
                "service_tier": "default",
                "finish_reason": "stop",
                "logprobs": null
            },
            "type": "ai",
            "name": null,
            "id": "run--3de0196d-c38e-43eb-8f53-bb35b0143d5d-0",
            "example": false,
            "tool_calls": [],
            "invalid_tool_calls": [],
            "usage_metadata": {
                "input_tokens": 9924,
                "output_tokens": 2496,
                "total_tokens": 12420,
                "input_token_details": {
                    "audio": 0,
                    "cache_read": 0
                },
                "output_token_details": {
                    "audio": 0,
                    "reasoning": 1216
                }
            }
        }
    ]
}